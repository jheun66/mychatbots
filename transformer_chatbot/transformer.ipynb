{
 "cells": [
  {
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Chatbot\n",
    "\n",
    "https://colab.research.google.com/github/tensorflow/examples/blob/master/community/en/transformer_chatbot.ipynb#scrollTo=YcK7-_3RxRvK\n",
    "\n",
    "위 튜토리얼 번역하면서 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 튜토리얼은 <a href=\"https://arxiv.org/abs/1706.03762\" class=\"external\">Transformer model</a>을 사용하여 챗봇을 학습시킵니다.\n",
    "이 고급 예제는 [text generation](https://tensorflow.org/alpha/tutorials/text/text_generation), [attention](https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention) 과 [transformer](https://www.tensorflow.org/alpha/tutorials/text/transformer)에 대한 지식을 전제로 진행됩니다.\n",
    "\n",
    "Transformer 모델의 핵심 아이디어는 *self-attention*—, 입력 시퀀스의 (representation)을 계산하기 위하여 입력 시퀀스의 다른 위치를 참조하는 기술입니다. Transformer는 self-attention 레이어 스택을 생성하며 이는 아래에서 *Scaled dot product attention*  과 *Multi-head attention* 섹션에 설명되어 있습니다.\n",
    "\n",
    "Note: 모델 구조는  [Transformer model for language understanding](https://www.tensorflow.org/alpha/tutorials/text/transformer)와 동일합니다. 그리고 우리는 Subclassing 대신에 Fuctional approach로 같은 모델을 어떻게 구현하는지 설명합니다."
   ]
  },
  {
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # The %tensorflow_version magic only works in colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "!pip install tensorflow-datasets==1.2.0\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the conversations in movies and TV shows provided by [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), which contains more than 220 thousands conversational exchanges between more than 10k pairs of movie characters, as our dataset.\n",
    "\n",
    "`movie_conversations.txt` contains list of the conversation IDs and `movie_lines.text` contains the text of assoicated with each conversation ID. For further  information regarding the dataset, please check the README file in the zip file.\n"
   ]
  },
  {
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'cornell_movie_dialogs.zip',\n",
    "    origin=\n",
    "    'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_dataset = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n",
    "\n",
    "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
    "path_to_movie_conversations = os.path.join(path_to_dataset,\n",
    "                                           'movie_conversations.txt')"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data\n",
    "\n",
    "이 예제를 단순하고 빠르게 하기위해 최대 학습 샘플의 수를 `MAX_SAMPLES=25000`로 최대 문장의 길이를 `MAX_LENGTH=40`로 제한합니다.\n",
    "\n",
    "다음과 같은 순서로 데이터셋의 전처리를 수행합니다:\n",
    "* `MAX_SAMPLES` 대화 쌍을 `questions` 과 `answers` 리스트로 추출합니다.\n",
    "* 각각의 문장에서 특수 문자를 제거하며 전처리를 진행합니다.\n",
    "* [TensorFlow Datasets SubwordTextEncoder](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder)를 사용하여 tokenizer (map text to ID and ID to text)를 빌드합니다.\n",
    "* 각 문장에 문장의 시작과 끝을 알려주는 `START_TOKEN` 와 `END_TOKEN`을 추가하고 토큰화합니다.\n",
    "* `MAX_LENGTH` 토큰보다 많은 토큰을 가진 문장을 제외합니다.\n",
    "* 문장을 `MAX_LENGTH` 크기로 패딩을 추가합니다. "
   ]
  },
  {
   "source": [
    "# Maximum number of samples to preprocess\n",
    "MAX_SAMPLES = 25000\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = sentence.lower().strip()\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  # adding a start and an end token to the sentence\n",
    "  return sentence\n",
    "\n",
    "\n",
    "def load_conversations():\n",
    "  # dictionary of line id to text\n",
    "  id2line = {}\n",
    "  with open(path_to_movie_lines, errors='ignore') as file:\n",
    "    lines = file.readlines()\n",
    "  for line in lines:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    id2line[parts[0]] = parts[4]\n",
    "\n",
    "  inputs, outputs = [], []\n",
    "  with open(path_to_movie_conversations, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "  for line in lines:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    # get conversation in a list of line ID\n",
    "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "    for i in range(len(conversation) - 1):\n",
    "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "      outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
    "      if len(inputs) >= MAX_SAMPLES:\n",
    "        return inputs, outputs\n",
    "  return inputs, outputs\n",
    "\n",
    "\n",
    "questions, answers = load_conversations()"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "print('Sample question: {}'.format(questions[20]))\n",
    "print('Sample answer: {}'.format(answers[20]))"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "# Build tokenizer using tfds for both questions and answers\n",
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)\n",
    "\n",
    "# Define start and end token to indicate the start and end of a sentence\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# Vocabulary size plus start and end token\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "print('Tokenized sample question: {}'.format(tokenizer.encode(questions[20])))"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "# Maximum sentence length\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "\n",
    "# Tokenize, filter and pad sentences\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # tokenize sentence\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "    # check tokenized sentence max length\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # pad tokenized sentences\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "print('Vocab size: {}'.format(VOCAB_SIZE))\n",
    "print('Number of samples: {}'.format(len(questions)))"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `tf.data.Dataset`\n",
    "\n",
    "caching과 prefetching 같은 학습 프로세스를 빠르게 하는 특징들을 활용하기 위한 input pipeline을 구성하기 위해  [tf.data.Dataset API](https://www.tensorflow.org/api_docs/python/tf/data)를 사용합니다.\n",
    "\n",
    "transformer는 auto-regressive model 입니다: 한번에 한 파트씩 예측하고 그 결과를 사용하여 다음에 나올 값을 결정합니다. \n",
    "\n",
    "이 예제에서는 학습하는 동안 teacher-forcing을 사용합니다. teacher forcing은 현재 스텝에서 모델이 예측하는지에 상관없이 실제 출력을 다음 스텝으로 전달합니다.  \n",
    "\n",
    "transformer가 각 단어를 예측할 때, self-attention은 입력 시퀀스에서 이전 단어들을 보고 다음 단어를 더 잘 예측하도록 합니다.\n",
    "\n",
    "모델이 예상 출력에 peaking되는 것을 막기 위해 look-ahead mask를 사용합니다.\n",
    "\n",
    "타겟은 decoder의 input에 패딩을 한 `decoder_inputs`과 loss와 accuracy를 계산하기 위한 `cropped_targets`으로 나뉩니다. "
   ]
  },
  {
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# decoder inputs use the previous target as input\n",
    "# remove START_TOKEN from targets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "print(dataset)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot product Attention\n",
    "\n",
    "이 transformer에 사용되는 scaled dot-product attention은 Q (query), K (key), V (value) 세가지 input을 가집니다. attention weights를 계산하기 위새 사용되는 방정식은 다음과 같습니다.\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "softmax 정규화가 `key`에 의해 수행될 때, 이 값들은 `query`에 주어진 중요도를 결정합니다.\n",
    "\n",
    "출력물은 attention weights와 `value` 벡터의 곱으로 나타냅니다. 이를 통해 우리가 집중하길 원하는 단어는 그대로 두고 관련없는 단어는 지웁니다.\n",
    "\n",
    "dot-product attention을 key depth의 제곱근으로 스케일링합니다. 이것은 depth가 클수록, dot product의 크기가 더욱 커지기 때문에 기울기가 작은 softmax function을 사용할 때 softmax가 매우 힘들기 때문입니다.\n",
    "\n",
    "예를 들어 `query`와 `key`가 평균이 0이고 분산이 1이라 가정하면 이들의 행렬곱은 평균이 0이고 분산이 `dk`일 것입니다. 그러므로 (다른 숫자가 아닌) *square root of `dk`* 로 스케일링합니다. 왜냐하면 소프트맥스를 더 부드럽게 하기위해서는 `query`와 `key`의 행렬곱이 평균 0과 분산 1을 가져야 하기 때문입니다.\n",
    "\n",
    "mask는 *-1e9 (close to negative infinity)* 로 곱합니다. 이는 마스크가 `query`와 `key`의 스케일된 행렬 곱에 더해지고 softmax 직전에 적용되기 때문입니다. 목표는 이 셀들(마스크한 부분)을 0으로 만드는 것입니다. softmax에 큰 음의 입력을 하면 출력으로 거의 0에 가깝게 나옵니다."
   ]
  },
  {
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  \"\"\"Calculate the attention weights. \"\"\"\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # add the mask to zero out padding tokens\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k)\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "\n",
    "  return output"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
    "\n",
    "\n",
    "Multi-head attention은 네 파트로 구성됩니다:\n",
    "* Linear layers and split into heads.\n",
    "* Scaled dot-product attention.\n",
    "* Concatenation of heads.\n",
    "* Final linear layer.\n",
    "\n",
    "각각의 multi-head attention block은 세개의 입력을 가집니다; Q (query), K (key), V (value). 이들은 linear (Dense) layers를 통해 여러 헤드로 분할됩니다.\n",
    "\n",
    "위에서 정의한 `scaled_dot_product_attention` 각 헤드에 적용됩니다 (broadcasted for efficiency). attention 단계에서 적절한 mask가 사용되어야합니다. 그런 다음 ( `tf.transpose`, and `tf.reshape`를 사용하여) 각 헤드의 attention output이 연결되고 최종 `Dense`를 통해 출력됩니다.\n",
    "\n",
    "모델이 다른 표현 공간으로 부터 다른 위치의 정보들을 함께 참여하기 위해서 single attenion head 대신 `query`, `key`, and `value` 를 multiple heads로 분리합니다. 분할 후 각 헤드의 차원수가 감소하므로 전체 계산 비용은 전체 차원의 single head attention과 동일합니다."
   ]
  },
  {
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # linear layers\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # split heads\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # scaled dot-product attention\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # concatenation of heads\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # final linear layer\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_padding_mask` and `create_look_ahead` are helper functions to creating masks to mask out padded tokens, we are going to use these helper functions as `tf.keras.layers.Lambda` layers.\n",
    "\n",
    "Mask all the pad tokens (value `0`) in the batch to ensure the model does not treat padding as input."
   ]
  },
  {
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look-ahead mask to mask the future tokens in a sequence.\n",
    "We also mask out pad tokens.\n",
    "\n",
    "i.e. To predict the third word, only the first and second word will be used\n",
    "\n",
    "* 해당 예제에서는 look ahead mask에 바로 padding mask와 결합하여 결과를 리턴함, [Transformer model for language understanding](https://www.tensorflow.org/alpha/tutorials/text/transformer) 에서의 combined mask와 동일"
   ]
  },
  {
   "source": [
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "print(create_look_ahead_mask(tf.constant([[1, 2, 0, 4, 5]])))"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding\n",
    "\n",
    "이 모델이 어떠한 recurrence 나 convolution 같은 성질을 포함하지 않기 때문에, 이 모델에 문장에서 단어의 상대적 위치에 대한 정보를 주기 위해 positional encoding을 추가하였습니다. \n",
    "\n",
    "embedding vector에 positional encoding vector를 추가합니다. Embeddings은 비슷한 의미를 가진 토큰들끼리 가까이 있는 d-차원 공간의 토큰을 의미합니다. 그러나 embedding은 문장에서 단어의 상대적 위치를 encode 하지 않습니다. 따라서 positional encoding을 추가한하면, 단어들은 *단어들의 의미의 유사성과 그들의 문장내의 위치*를 기반으로 하여 d-차원 공간에서 가까워질 것입니다.\n",
    "\n",
    "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
   ]
  },
  {
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "    # apply sin to even index in the array\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd index in the array\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "sample_pos_encoding = PositionalEncoding(50, 512)\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "\n",
    "Each encoder layer consists of sublayers:\n",
    "\n",
    "1. Multi-head attention (with padding mask) \n",
    "2. 2 dense layers followed by dropout\n",
    "\n",
    "각각의 sublayer들은 residual connection을 하고 layer normalization을 합니다. Residual connections는 deep networks에서 vanishing gradient problem을 방지하는데 도움을 줍니다.\n",
    "\n",
    "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis."
   ]
  },
  {
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "sample_encoder_layer = encoder_layer(\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_encoder_layer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_encoder_layer, to_file='encoder_layer.png', show_shapes=True)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The Encoder consists of:\n",
    "1.   Input Embedding\n",
    "2.   Positional Encoding\n",
    "3.   `num_layers` encoder layers\n",
    "\n",
    "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
   ]
  },
  {
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "sample_encoder = encoder(\n",
    "    vocab_size=8192,\n",
    "    num_layers=2,\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_encoder\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "   sample_encoder, to_file='encoder.png', show_shapes=True)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer\n",
    "\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
    "2.   Multi-head attention (with padding mask). `value` and `key` receive the *encoder output* as inputs. `query` receives the *output from the masked multi-head attention sublayer.*\n",
    "3.   2 dense layers followed by dropout\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
    "\n",
    "As `query` receives the output from decoder's first attention block, and `key` receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
   ]
  },
  {
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "sample_decoder_layer = decoder_layer(\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_decoder_layer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_decoder_layer, to_file='decoder_layer.png', show_shapes=True)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "The Decoder consists of:\n",
    "1.   Output Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N decoder layers\n",
    "\n",
    "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
   ]
  },
  {
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "sample_decoder = decoder(\n",
    "    vocab_size=8192,\n",
    "    num_layers=2,\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_decoder\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_decoder, to_file='decoder.png', show_shapes=True)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "  # mask the future tokens for decoder inputs at the 1st attention block\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "  # mask the encoder outputs for the 2nd attention block\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "sample_transformer = transformer(\n",
    "    vocab_size=8192,\n",
    "    num_layers=4,\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_transformer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_transformer, to_file='transformer.png', show_shapes=True)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model\n",
    "\n",
    "To keep this example small and relatively fast, the values for *num_layers, d_model, and units* have been reduced. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer."
   ]
  },
  {
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hyper-parameters\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom learning rate\n",
    "\n",
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$"
   ]
  },
  {
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model\n",
    "\n",
    "Train our transformer by simply calling `model.fit()`"
   ]
  },
  {
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "model.fit(dataset, epochs=EPOCHS)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and predict\n",
    "\n",
    "The following steps are used for evaluation:\n",
    "\n",
    "* Apply the same preprocessing method we used to create our dataset for the input sentence.\n",
    "* Tokenize the input sentence and add `START_TOKEN` and `END_TOKEN`. \n",
    "* Calculate the padding masks and the look ahead masks.\n",
    "* The decoder then outputs the predictions by looking at the encoder output and its own output.\n",
    "* Select the last word and calculate the argmax of that.\n",
    "* Concatentate the predicted word to the decoder input as pass it to the decoder.\n",
    "* In this approach, the decoder predicts the next word based on the previous words it predicted.\n",
    "\n",
    "Note: The model used here has less capacity and trained on a subset of the full dataset, hence its performance can be further improved."
   ]
  },
  {
   "source": [
    "def evaluate(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  for i in range(MAX_LENGTH):\n",
    "    predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # concatenated the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0)\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "  prediction = evaluate(sentence)\n",
    "\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model!"
   ]
  },
  {
   "source": [
    "output = predict('Where have you been?')"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "output = predict(\"It's a trap\")"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "source": [
    "# feed the model with its previous output\n",
    "sentence = 'I am not crazy, my mother had me tested.'\n",
    "for _ in range(5):\n",
    "  sentence = predict(sentence)\n",
    "  print('')"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here we are, we have implemented a Transformer in TensorFlow 2.0 in around 500 lines of code.\n",
    "\n",
    "In this tutorial, we focus on the two different approaches to implement complex models with Functional API and Model subclassing, and how to incorporate them.\n",
    "\n",
    "Try using a different dataset or hyper-parameters to train the Transformer! Thanks for reading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}