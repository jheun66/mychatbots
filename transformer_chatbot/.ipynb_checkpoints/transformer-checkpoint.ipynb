{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Chatbot\n",
    "\n",
    "https://colab.research.google.com/github/tensorflow/examples/blob/master/community/en/transformer_chatbot.ipynb#scrollTo=YcK7-_3RxRvK\n",
    "\n",
    "위 튜토리얼 번역하면서 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 튜토리얼은 <a href=\"https://arxiv.org/abs/1706.03762\" class=\"external\">Transformer model</a>을 사용하여 챗봇을 학습시킵니다.\n",
    "이 고급 예제는 [text generation](https://tensorflow.org/alpha/tutorials/text/text_generation), [attention](https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention) 과 [transformer](https://www.tensorflow.org/alpha/tutorials/text/transformer)에 대한 지식을 전제로 진행됩니다.\n",
    "\n",
    "Transformer 모델의 핵심 아이디어는 *self-attention*—, 입력 시퀀스의 (representation)을 계산하기 위하여 입력 시퀀스의 다른 위치를 참조하는 기술입니다. Transformer는 self-attention 레이어 스택을 생성하며 이는 아래에서 *Scaled dot product attention*  과 *Multi-head attention* 섹션에 설명되어 있습니다.\n",
    "\n",
    "Note: 모델 구조는  [Transformer model for language understanding](https://www.tensorflow.org/alpha/tutorials/text/transformer)와 동일합니다. 그리고 우리는 Subclassing 대신에 Fuctional approach로 같은 모델을 어떻게 구현하는지 설명합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets==1.2.0 in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-datasets==1.2.0) (2.22.0)\n",
      "Requirement already satisfied: future in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-datasets==1.2.0) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-datasets==1.2.0) (1.16.5)\n",
      "Requirement already satisfied: wrapt in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-datasets==1.2.0) (1.11.2)\n",
      "Requirement already satisfied: dill in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-datasets==1.2.0) (0.3.1.1)\n",
      "Requirement already satisfied: promise in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-datasets==1.2.0) (2.2.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-datasets==1.2.0) (3.10.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-datasets==1.2.0) (0.8.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-datasets==1.2.0) (5.6.3)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-datasets==1.2.0) (0.15.0)\n",
      "Requirement already satisfied: six in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-datasets==1.2.0) (1.12.0)\n",
      "Requirement already satisfied: attrs in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-datasets==1.2.0) (19.2.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-datasets==1.2.0) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-datasets==1.2.0) (4.36.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (3.0.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from protobuf>=3.6.1->tensorflow-datasets==1.2.0) (41.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos in c:\\users\\eun\\anaconda3\\envs\\chatbot\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets==1.2.0) (1.6.0)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # The %tensorflow_version magic only works in colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "!pip install tensorflow-datasets==1.2.0\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the conversations in movies and TV shows provided by [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), which contains more than 220 thousands conversational exchanges between more than 10k pairs of movie characters, as our dataset.\n",
    "\n",
    "`movie_conversations.txt` contains list of the conversation IDs and `movie_lines.text` contains the text of assoicated with each conversation ID. For further  information regarding the dataset, please check the README file in the zip file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'cornell_movie_dialogs.zip',\n",
    "    origin=\n",
    "    'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_dataset = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n",
    "\n",
    "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
    "path_to_movie_conversations = os.path.join(path_to_dataset,\n",
    "                                           'movie_conversations.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data\n",
    "\n",
    "이 예제를 단순하고 빠르게 하기위해 최대 학습 샘플의 수를 `MAX_SAMPLES=25000`로 최대 문장의 길이를 `MAX_LENGTH=40`로 제한합니다.\n",
    "\n",
    "다음과 같은 순서로 데이터셋의 전처리를 수행합니다:\n",
    "* `MAX_SAMPLES` 대화 쌍을 `questions` 과 `answers` 리스트로 추출합니다.\n",
    "* 각각의 문장에서 특수 문자를 제거하며 전처리를 진행합니다.\n",
    "* [TensorFlow Datasets SubwordTextEncoder](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder)를 사용하여 tokenizer (map text to ID and ID to text)를 빌드합니다.\n",
    "* 각 문장에 문장의 시작과 끝을 알려주는 `START_TOKEN` 와 `END_TOKEN`을 추가하고 토큰화합니다.\n",
    "* `MAX_LENGTH` 토큰보다 많은 토큰을 가진 문장을 제외합니다.\n",
    "* 문장을 `MAX_LENGTH` 크기로 패딩을 추가합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of samples to preprocess\n",
    "MAX_SAMPLES = 25000\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = sentence.lower().strip()\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  # adding a start and an end token to the sentence\n",
    "  return sentence\n",
    "\n",
    "\n",
    "def load_conversations():\n",
    "  # dictionary of line id to text\n",
    "  id2line = {}\n",
    "  with open(path_to_movie_lines, errors='ignore') as file:\n",
    "    lines = file.readlines()\n",
    "  for line in lines:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    id2line[parts[0]] = parts[4]\n",
    "\n",
    "  inputs, outputs = [], []\n",
    "  with open(path_to_movie_conversations, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "  for line in lines:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    # get conversation in a list of line ID\n",
    "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "    for i in range(len(conversation) - 1):\n",
    "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "      outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
    "      if len(inputs) >= MAX_SAMPLES:\n",
    "        return inputs, outputs\n",
    "  return inputs, outputs\n",
    "\n",
    "\n",
    "questions, answers = load_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample question: i really , really , really wanna go , but i can t . not unless my sister goes .\n",
      "Sample answer: i m workin on it . but she doesn t seem to be goin for him .\n"
     ]
    }
   ],
   "source": [
    "print('Sample question: {}'.format(questions[20]))\n",
    "print('Sample answer: {}'.format(answers[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tokenizer using tfds for both questions and answers\n",
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)\n",
    "\n",
    "# Define start and end token to indicate the start and end of a sentence\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# Vocabulary size plus start and end token\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sample question: [4, 253, 3, 253, 3, 150, 430, 168, 3, 41, 4, 39, 8007, 2, 33, 821, 27, 1652, 2843, 1]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sample question: {}'.format(tokenizer.encode(questions[20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sentence length\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "\n",
    "# Tokenize, filter and pad sentences\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # tokenize sentence\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "    # check tokenized sentence max length\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # pad tokenized sentences\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8149\n",
      "Number of samples: 22073\n"
     ]
    }
   ],
   "source": [
    "print('Vocab size: {}'.format(VOCAB_SIZE))\n",
    "print('Number of samples: {}'.format(len(questions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `tf.data.Dataset`\n",
    "\n",
    "caching과 prefetching 같은 학습 프로세스를 빠르게 하는 특징들을 활용하기 위한 input pipeline을 구성하기 위해  [tf.data.Dataset API](https://www.tensorflow.org/api_docs/python/tf/data)를 사용합니다.\n",
    "\n",
    "transformer는 auto-regressive model 입니다: 한번에 한 파트씩 예측하고 그 결과를 사용하여 다음에 나올 값을 결정합니다. \n",
    "\n",
    "이 예제에서는 학습하는 동안 teacher-forcing을 사용합니다. teacher forcing은 현재 스텝에서 모델이 예측하는지에 상관없이 실제 출력을 다음 스텝으로 전달합니다.  \n",
    "\n",
    "transformer가 각 단어를 예측할 때, self-attention은 입력 시퀀스에서 이전 단어들을 보고 다음 단어를 더 잘 예측하도록 합니다.\n",
    "\n",
    "모델이 예상 출력에 peaking되는 것을 막기 위해 look-ahead mask를 사용합니다.\n",
    "\n",
    "타겟은 decoder의 input에 패딩을 한 `decoder_inputs`과 loss와 accuracy를 계산하기 위한 `cropped_targets`으로 나뉩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# decoder inputs use the previous target as input\n",
    "# remove START_TOKEN from targets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ({inputs: (None, 40), dec_inputs: (None, 39)}, {outputs: (None, 39)}), types: ({inputs: tf.int32, dec_inputs: tf.int32}, {outputs: tf.int32})>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot product Attention\n",
    "\n",
    "이 transformer에 사용되는 scaled dot-product attention은 Q (query), K (key), V (value) 세가지 input을 가집니다. attention weights를 계산하기 위새 사용되는 방정식은 다음과 같습니다.\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "softmax 정규화가 `key`에 의해 수행될 때, 이 값들은 `query`에 주어진 중요도를 결정합니다.\n",
    "\n",
    "출력물은 attention weights와 `value` 벡터의 곱으로 나타냅니다. 이를 통해 우리가 집중하길 원하는 단어는 그대로 두고 관련없는 단어는 지웁니다.\n",
    "\n",
    "dot-product attention을 key depth의 제곱근으로 스케일링합니다. 이것은 depth가 클수록, dot product의 크기가 더욱 커지기 때문에 기울기가 작은 softmax function을 사용할 때 softmax가 매우 힘들기 때문입니다.\n",
    "\n",
    "예를 들어 `query`와 `key`가 평균이 0이고 분산이 1이라 가정하면 이들의 행렬곱은 평균이 0이고 분산이 `dk`일 것입니다. 그러므로 (다른 숫자가 아닌) *square root of `dk`* 로 스케일링합니다. 왜냐하면 소프트맥스를 더 부드럽게 하기위해서는 `query`와 `key`의 행렬곱이 평균 0과 분산 1을 가져야 하기 때문입니다.\n",
    "\n",
    "mask는 *-1e9 (close to negative infinity)* 로 곱합니다. 이는 마스크가 `query`와 `key`의 스케일된 행렬 곱에 더해지고 softmax 직전에 적용되기 때문입니다. 목표는 이 셀들(마스크한 부분)을 0으로 만드는 것입니다. softmax에 큰 음의 입력을 하면 출력으로 거의 0에 가깝게 나옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  \"\"\"Calculate the attention weights. \"\"\"\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # add the mask to zero out padding tokens\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k)\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
    "\n",
    "\n",
    "Multi-head attention은 네 파트로 구성됩니다:\n",
    "* Linear layers and split into heads.\n",
    "* Scaled dot-product attention.\n",
    "* Concatenation of heads.\n",
    "* Final linear layer.\n",
    "\n",
    "각각의 multi-head attention block은 세개의 입력을 가집니다; Q (query), K (key), V (value). 이들은 linear (Dense) layers를 통해 여러 헤드로 분할됩니다.\n",
    "\n",
    "위에서 정의한 `scaled_dot_product_attention` 각 헤드에 적용됩니다 (broadcasted for efficiency). attention 단계에서 적절한 mask가 사용되어야합니다. 그런 다음 ( `tf.transpose`, and `tf.reshape`를 사용하여) 각 헤드의 attention output이 연결되고 최종 `Dense`를 통해 출력됩니다.\n",
    "\n",
    "모델이 다른 표현 공간으로 부터 다른 위치의 정보들을 함께 참여하기 위해서 single attenion head 대신 `query`, `key`, and `value` 를 multiple heads로 분리합니다. 분할 후 각 헤드의 차원수가 감소하므로 전체 계산 비용은 전체 차원의 single head attention과 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # linear layers\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # split heads\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # scaled dot-product attention\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # concatenation of heads\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # final linear layer\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_padding_mask` and `create_look_ahead` are helper functions to creating masks to mask out padded tokens, we are going to use these helper functions as `tf.keras.layers.Lambda` layers.\n",
    "\n",
    "Mask all the pad tokens (value `0`) in the batch to ensure the model does not treat padding as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 1. 0. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 0. 0.]]]], shape=(2, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look-ahead mask to mask the future tokens in a sequence.\n",
    "We also mask out pad tokens.\n",
    "\n",
    "i.e. To predict the third word, only the first and second word will be used\n",
    "\n",
    "* 해당 예제에서는 look ahead mask에 바로 padding mask와 결합하여 결과를 리턴함, [Transformer model for language understanding](https://www.tensorflow.org/alpha/tutorials/text/transformer) 에서의 combined mask와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 1. 0. 1.]\n",
      "   [0. 0. 1. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_look_ahead_mask(tf.constant([[1, 2, 0, 4, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding\n",
    "\n",
    "이 모델이 어떠한 recurrence 나 convolution 같은 성질을 포함하지 않기 때문에, 이 모델에 문장에서 단어의 상대적 위치에 대한 정보를 주기 위해 positional encoding을 추가하였습니다. \n",
    "\n",
    "embedding vector에 positional encoding vector를 추가합니다. Embeddings은 비슷한 의미를 가진 토큰들끼리 가까이 있는 d-차원 공간의 토큰을 의미합니다. 그러나 embedding은 문장에서 단어의 상대적 위치를 encode 하지 않습니다. 따라서 positional encoding을 추가한하면, 단어들은 *단어들의 의미의 유사성과 그들의 문장내의 위치*를 기반으로 하여 d-차원 공간에서 가까워질 것입니다.\n",
    "\n",
    "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "    # apply sin to even index in the array\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd index in the array\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_pos_encoding = PositionalEncoding(50, 512)\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "\n",
    "Each encoder layer consists of sublayers:\n",
    "\n",
    "1. Multi-head attention (with padding mask) \n",
    "2. 2 dense layers followed by dropout\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
    "\n",
    "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder_layer = encoder_layer(\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_encoder_layer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_encoder_layer, to_file='encoder_layer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The Encoder consists of:\n",
    "1.   Input Embedding\n",
    "2.   Positional Encoding\n",
    "3.   `num_layers` encoder layers\n",
    "\n",
    "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = encoder(\n",
    "    vocab_size=8192,\n",
    "    num_layers=2,\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_encoder\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "   sample_encoder, to_file='encoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer\n",
    "\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
    "2.   Multi-head attention (with padding mask). `value` and `key` receive the *encoder output* as inputs. `query` receives the *output from the masked multi-head attention sublayer.*\n",
    "3.   2 dense layers followed by dropout\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
    "\n",
    "As `query` receives the output from decoder's first attention block, and `key` receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "sample_decoder_layer = decoder_layer(\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_decoder_layer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_decoder_layer, to_file='decoder_layer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "The Decoder consists of:\n",
    "1.   Output Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N decoder layers\n",
    "\n",
    "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "sample_decoder = decoder(\n",
    "    vocab_size=8192,\n",
    "    num_layers=2,\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_decoder\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_decoder, to_file='decoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "  # mask the future tokens for decoder inputs at the 1st attention block\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "  # mask the encoder outputs for the 2nd attention block\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "sample_transformer = transformer(\n",
    "    vocab_size=8192,\n",
    "    num_layers=4,\n",
    "    units=512,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dropout=0.3,\n",
    "    name=\"sample_transformer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    sample_transformer, to_file='transformer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model\n",
    "\n",
    "To keep this example small and relatively fast, the values for *num_layers, d_model, and units* have been reduced. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hyper-parameters\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom learning rate\n",
    "\n",
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model\n",
    "\n",
    "Train our transformer by simply calling `model.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "    170/Unknown - 8s 8s/step - loss: 2.8750 - accuracy: 0.0000e+ - 10s 5s/step - loss: 2.6906 - accuracy: 0.0000e+0 - 11s 4s/step - loss: 2.7842 - accuracy: 0.0000e+0 - 11s 3s/step - loss: 2.7966 - accuracy: 0.0000e+0 - 12s 2s/step - loss: 2.8136 - accuracy: 0.0000e+0 - 13s 2s/step - loss: 2.8211 - accuracy: 0.0000e+0 - 14s 2s/step - loss: 2.8468 - accuracy: 0.0000e+0 - 15s 2s/step - loss: 2.8504 - accuracy: 0.0000e+0 - 16s 2s/step - loss: 2.8568 - accuracy: 0.0000e+0 - 17s 2s/step - loss: 2.8216 - accuracy: 0.0000e+0 - 18s 2s/step - loss: 2.8486 - accuracy: 0.0000e+0 - 19s 2s/step - loss: 2.8967 - accuracy: 0.0000e+0 - 20s 2s/step - loss: 2.9139 - accuracy: 0.0000e+0 - 21s 2s/step - loss: 2.9219 - accuracy: 0.0000e+0 - 22s 1s/step - loss: 2.9436 - accuracy: 0.0000e+0 - 23s 1s/step - loss: 2.9206 - accuracy: 0.0000e+0 - 24s 1s/step - loss: 2.9063 - accuracy: 0.0000e+0 - 26s 1s/step - loss: 2.8982 - accuracy: 0.0000e+0 - 27s 1s/step - loss: 2.8905 - accuracy: 0.0000e+0 - 28s 1s/step - loss: 2.8919 - accuracy: 0.0000e+0 - 29s 1s/step - loss: 2.8823 - accuracy: 0.0000e+0 - 30s 1s/step - loss: 2.8623 - accuracy: 0.0000e+0 - 31s 1s/step - loss: 2.8523 - accuracy: 0.0000e+0 - 32s 1s/step - loss: 2.8742 - accuracy: 0.0000e+0 - 33s 1s/step - loss: 2.8741 - accuracy: 0.0000e+0 - 34s 1s/step - loss: 2.8680 - accuracy: 0.0000e+0 - 35s 1s/step - loss: 2.8646 - accuracy: 0.0000e+0 - 36s 1s/step - loss: 2.8758 - accuracy: 0.0000e+0 - 37s 1s/step - loss: 2.8809 - accuracy: 0.0000e+0 - 38s 1s/step - loss: 2.8638 - accuracy: 0.0000e+0 - 39s 1s/step - loss: 2.8694 - accuracy: 0.0000e+0 - 40s 1s/step - loss: 2.8685 - accuracy: 0.0000e+0 - 41s 1s/step - loss: 2.8764 - accuracy: 0.0000e+0 - 41s 1s/step - loss: 2.8689 - accuracy: 1.1784e-0 - 42s 1s/step - loss: 2.8711 - accuracy: 1.1447e-0 - 43s 1s/step - loss: 2.8686 - accuracy: 1.1129e-0 - 44s 1s/step - loss: 2.8693 - accuracy: 2.1656e-0 - 45s 1s/step - loss: 2.8630 - accuracy: 2.1086e-0 - 46s 1s/step - loss: 2.8685 - accuracy: 2.0546e-0 - 47s 1s/step - loss: 2.8738 - accuracy: 2.0032e-0 - 48s 1s/step - loss: 2.8793 - accuracy: 7.8174e-0 - 49s 1s/step - loss: 2.8780 - accuracy: 1.2401e-0 - 50s 1s/step - loss: 2.8689 - accuracy: 1.9566e-0 - 51s 1s/step - loss: 2.8679 - accuracy: 3.1869e-0 - 52s 1s/step - loss: 2.8691 - accuracy: 4.8967e-0 - 53s 1s/step - loss: 2.8632 - accuracy: 6.7064e-0 - 54s 1s/step - loss: 2.8572 - accuracy: 9.2915e-0 - 55s 1s/step - loss: 2.8601 - accuracy: 0.0013    - 56s 1s/step - loss: 2.8639 - accuracy: 0.001 - 57s 1s/step - loss: 2.8684 - accuracy: 0.002 - 58s 1s/step - loss: 2.8734 - accuracy: 0.002 - 59s 1s/step - loss: 2.8699 - accuracy: 0.002 - 60s 1s/step - loss: 2.8705 - accuracy: 0.003 - 61s 1s/step - loss: 2.8758 - accuracy: 0.003 - 62s 1s/step - loss: 2.8730 - accuracy: 0.004 - 63s 1s/step - loss: 2.8742 - accuracy: 0.004 - 64s 1s/step - loss: 2.8747 - accuracy: 0.004 - 65s 1s/step - loss: 2.8681 - accuracy: 0.005 - 66s 1s/step - loss: 2.8701 - accuracy: 0.005 - 67s 1s/step - loss: 2.8687 - accuracy: 0.005 - 68s 1s/step - loss: 2.8632 - accuracy: 0.006 - 69s 1s/step - loss: 2.8659 - accuracy: 0.006 - 70s 1s/step - loss: 2.8632 - accuracy: 0.006 - 71s 1s/step - loss: 2.8673 - accuracy: 0.007 - 72s 1s/step - loss: 2.8702 - accuracy: 0.007 - 73s 1s/step - loss: 2.8707 - accuracy: 0.007 - 74s 1s/step - loss: 2.8707 - accuracy: 0.008 - 75s 1s/step - loss: 2.8714 - accuracy: 0.008 - 76s 1s/step - loss: 2.8684 - accuracy: 0.008 - 77s 1s/step - loss: 2.8693 - accuracy: 0.008 - 78s 1s/step - loss: 2.8685 - accuracy: 0.009 - 79s 1s/step - loss: 2.8671 - accuracy: 0.009 - 80s 1s/step - loss: 2.8725 - accuracy: 0.009 - 81s 1s/step - loss: 2.8707 - accuracy: 0.009 - 82s 1s/step - loss: 2.8673 - accuracy: 0.009 - 83s 1s/step - loss: 2.8655 - accuracy: 0.010 - 84s 1s/step - loss: 2.8619 - accuracy: 0.010 - 85s 1s/step - loss: 2.8577 - accuracy: 0.010 - 86s 1s/step - loss: 2.8604 - accuracy: 0.010 - 87s 1s/step - loss: 2.8570 - accuracy: 0.010 - 88s 1s/step - loss: 2.8530 - accuracy: 0.011 - 89s 1s/step - loss: 2.8553 - accuracy: 0.011 - 90s 1s/step - loss: 2.8564 - accuracy: 0.011 - 91s 1s/step - loss: 2.8587 - accuracy: 0.011 - 92s 1s/step - loss: 2.8582 - accuracy: 0.011 - 93s 1s/step - loss: 2.8553 - accuracy: 0.011 - 94s 1s/step - loss: 2.8525 - accuracy: 0.012 - 95s 1s/step - loss: 2.8541 - accuracy: 0.012 - 96s 1s/step - loss: 2.8516 - accuracy: 0.012 - 97s 1s/step - loss: 2.8520 - accuracy: 0.012 - 98s 1s/step - loss: 2.8468 - accuracy: 0.012 - 99s 1s/step - loss: 2.8433 - accuracy: 0.012 - 100s 1s/step - loss: 2.8431 - accuracy: 0.01 - 101s 1s/step - loss: 2.8404 - accuracy: 0.01 - 102s 1s/step - loss: 2.8447 - accuracy: 0.01 - 103s 1s/step - loss: 2.8440 - accuracy: 0.01 - 104s 1s/step - loss: 2.8404 - accuracy: 0.01 - 105s 1s/step - loss: 2.8408 - accuracy: 0.01 - 106s 1s/step - loss: 2.8366 - accuracy: 0.01 - 107s 1s/step - loss: 2.8335 - accuracy: 0.01 - 108s 1s/step - loss: 2.8293 - accuracy: 0.01 - 109s 1s/step - loss: 2.8291 - accuracy: 0.01 - 110s 1s/step - loss: 2.8291 - accuracy: 0.01 - 111s 1s/step - loss: 2.8265 - accuracy: 0.01 - 112s 1s/step - loss: 2.8248 - accuracy: 0.01 - 113s 1s/step - loss: 2.8262 - accuracy: 0.01 - 114s 1s/step - loss: 2.8253 - accuracy: 0.01 - 115s 1s/step - loss: 2.8216 - accuracy: 0.01 - 116s 1s/step - loss: 2.8183 - accuracy: 0.01 - 117s 1s/step - loss: 2.8156 - accuracy: 0.01 - 118s 1s/step - loss: 2.8148 - accuracy: 0.01 - 119s 1s/step - loss: 2.8189 - accuracy: 0.01 - 120s 1s/step - loss: 2.8163 - accuracy: 0.01 - 121s 1s/step - loss: 2.8163 - accuracy: 0.01 - 122s 1s/step - loss: 2.8183 - accuracy: 0.01 - 123s 1s/step - loss: 2.8180 - accuracy: 0.01 - 124s 1s/step - loss: 2.8187 - accuracy: 0.01 - 125s 1s/step - loss: 2.8165 - accuracy: 0.01 - 126s 1s/step - loss: 2.8133 - accuracy: 0.01 - 127s 1s/step - loss: 2.8143 - accuracy: 0.01 - 128s 1s/step - loss: 2.8108 - accuracy: 0.01 - 129s 1s/step - loss: 2.8079 - accuracy: 0.01 - 130s 1s/step - loss: 2.8037 - accuracy: 0.01 - 131s 1s/step - loss: 2.8034 - accuracy: 0.01 - 132s 1s/step - loss: 2.8022 - accuracy: 0.01 - 133s 1s/step - loss: 2.8010 - accuracy: 0.01 - 134s 1s/step - loss: 2.8007 - accuracy: 0.01 - 135s 1s/step - loss: 2.8013 - accuracy: 0.01 - 136s 1s/step - loss: 2.8016 - accuracy: 0.01 - 137s 1s/step - loss: 2.7996 - accuracy: 0.01 - 138s 1s/step - loss: 2.7979 - accuracy: 0.01 - 139s 1s/step - loss: 2.7981 - accuracy: 0.01 - 140s 1s/step - loss: 2.8002 - accuracy: 0.01 - 141s 1s/step - loss: 2.8008 - accuracy: 0.01 - 142s 1s/step - loss: 2.7992 - accuracy: 0.01 - 143s 1s/step - loss: 2.7962 - accuracy: 0.01 - 144s 1s/step - loss: 2.7932 - accuracy: 0.01 - 145s 1s/step - loss: 2.7908 - accuracy: 0.01 - 146s 1s/step - loss: 2.7887 - accuracy: 0.01 - 147s 1s/step - loss: 2.7879 - accuracy: 0.01 - 148s 1s/step - loss: 2.7862 - accuracy: 0.01 - 149s 1s/step - loss: 2.7837 - accuracy: 0.01 - 150s 1s/step - loss: 2.7850 - accuracy: 0.01 - 151s 1s/step - loss: 2.7821 - accuracy: 0.01 - 152s 1s/step - loss: 2.7809 - accuracy: 0.01 - 153s 1s/step - loss: 2.7792 - accuracy: 0.01 - 154s 1s/step - loss: 2.7778 - accuracy: 0.01 - 155s 1s/step - loss: 2.7767 - accuracy: 0.01 - 156s 1s/step - loss: 2.7708 - accuracy: 0.01 - 157s 1s/step - loss: 2.7685 - accuracy: 0.01 - 158s 1s/step - loss: 2.7666 - accuracy: 0.01 - 159s 1s/step - loss: 2.7662 - accuracy: 0.01 - 160s 1s/step - loss: 2.7669 - accuracy: 0.01 - 161s 1s/step - loss: 2.7631 - accuracy: 0.01 - 162s 1s/step - loss: 2.7603 - accuracy: 0.01 - 163s 1s/step - loss: 2.7583 - accuracy: 0.01 - 164s 1s/step - loss: 2.7576 - accuracy: 0.01 - 165s 1s/step - loss: 2.7550 - accuracy: 0.01 - 166s 1s/step - loss: 2.7557 - accuracy: 0.01 - 167s 1s/step - loss: 2.7540 - accuracy: 0.01 - 168s 1s/step - loss: 2.7527 - accuracy: 0.01 - 169s 1s/step - loss: 2.7519 - accuracy: 0.01 - 170s 1s/step - loss: 2.7507 - accuracy: 0.01 - 171s 1s/step - loss: 2.7500 - accuracy: 0.01 - 172s 1s/step - loss: 2.7474 - accuracy: 0.01 - 173s 1s/step - loss: 2.7469 - accuracy: 0.01 - 174s 1s/step - loss: 2.7455 - accuracy: 0.01 - 175s 1s/step - loss: 2.7483 - accuracy: 0.01 - 176s 1s/step - loss: 2.7438 - accuracy: 0.01 - 177s 1s/step - loss: 2.7434 - accuracy: 0.01    342/Unknown - 178s 1s/step - loss: 2.7444 - accuracy: 0.01 - 179s 1s/step - loss: 2.7428 - accuracy: 0.01 - 180s 1s/step - loss: 2.7391 - accuracy: 0.01 - 181s 1s/step - loss: 2.7377 - accuracy: 0.01 - 182s 1s/step - loss: 2.7387 - accuracy: 0.01 - 182s 1s/step - loss: 2.7382 - accuracy: 0.01 - 183s 1s/step - loss: 2.7356 - accuracy: 0.01 - 184s 1s/step - loss: 2.7311 - accuracy: 0.01 - 185s 1s/step - loss: 2.7293 - accuracy: 0.01 - 186s 1s/step - loss: 2.7292 - accuracy: 0.01 - 187s 1s/step - loss: 2.7263 - accuracy: 0.01 - 188s 1s/step - loss: 2.7271 - accuracy: 0.01 - 189s 1s/step - loss: 2.7238 - accuracy: 0.01 - 190s 1s/step - loss: 2.7218 - accuracy: 0.01 - 191s 1s/step - loss: 2.7205 - accuracy: 0.01 - 192s 1s/step - loss: 2.7207 - accuracy: 0.01 - 193s 1s/step - loss: 2.7203 - accuracy: 0.01 - 193s 1s/step - loss: 2.7184 - accuracy: 0.01 - 194s 1s/step - loss: 2.7157 - accuracy: 0.01 - 195s 1s/step - loss: 2.7149 - accuracy: 0.01 - 196s 1s/step - loss: 2.7129 - accuracy: 0.01 - 197s 1s/step - loss: 2.7107 - accuracy: 0.01 - 198s 1s/step - loss: 2.7117 - accuracy: 0.01 - 199s 1s/step - loss: 2.7086 - accuracy: 0.01 - 200s 1s/step - loss: 2.7064 - accuracy: 0.01 - 201s 1s/step - loss: 2.7043 - accuracy: 0.01 - 202s 1s/step - loss: 2.7035 - accuracy: 0.01 - 202s 1s/step - loss: 2.7050 - accuracy: 0.01 - 203s 1s/step - loss: 2.7046 - accuracy: 0.01 - 204s 1s/step - loss: 2.7022 - accuracy: 0.01 - 205s 1s/step - loss: 2.7014 - accuracy: 0.01 - 206s 1s/step - loss: 2.7011 - accuracy: 0.01 - 207s 1s/step - loss: 2.7003 - accuracy: 0.01 - 208s 1s/step - loss: 2.6987 - accuracy: 0.01 - 209s 1s/step - loss: 2.6988 - accuracy: 0.01 - 210s 1s/step - loss: 2.6959 - accuracy: 0.01 - 211s 1s/step - loss: 2.6938 - accuracy: 0.01 - 212s 1s/step - loss: 2.6929 - accuracy: 0.01 - 213s 1s/step - loss: 2.6926 - accuracy: 0.02 - 214s 1s/step - loss: 2.6921 - accuracy: 0.02 - 215s 1s/step - loss: 2.6903 - accuracy: 0.02 - 216s 1s/step - loss: 2.6891 - accuracy: 0.02 - 217s 1s/step - loss: 2.6867 - accuracy: 0.02 - 218s 1s/step - loss: 2.6835 - accuracy: 0.02 - 219s 1s/step - loss: 2.6810 - accuracy: 0.02 - 220s 1s/step - loss: 2.6800 - accuracy: 0.02 - 221s 1s/step - loss: 2.6795 - accuracy: 0.02 - 221s 1s/step - loss: 2.6766 - accuracy: 0.02 - 222s 1s/step - loss: 2.6767 - accuracy: 0.02 - 223s 1s/step - loss: 2.6754 - accuracy: 0.02 - 224s 1s/step - loss: 2.6737 - accuracy: 0.02 - 225s 1s/step - loss: 2.6745 - accuracy: 0.02 - 226s 1s/step - loss: 2.6721 - accuracy: 0.02 - 227s 1s/step - loss: 2.6713 - accuracy: 0.02 - 228s 1s/step - loss: 2.6694 - accuracy: 0.02 - 229s 1s/step - loss: 2.6672 - accuracy: 0.02 - 230s 1s/step - loss: 2.6651 - accuracy: 0.02 - 230s 1s/step - loss: 2.6627 - accuracy: 0.02 - 231s 1s/step - loss: 2.6613 - accuracy: 0.02 - 232s 1s/step - loss: 2.6586 - accuracy: 0.02 - 233s 1s/step - loss: 2.6584 - accuracy: 0.02 - 234s 1s/step - loss: 2.6565 - accuracy: 0.02 - 235s 1s/step - loss: 2.6545 - accuracy: 0.02 - 236s 1s/step - loss: 2.6530 - accuracy: 0.02 - 237s 1s/step - loss: 2.6521 - accuracy: 0.02 - 238s 1s/step - loss: 2.6504 - accuracy: 0.02 - 239s 1s/step - loss: 2.6482 - accuracy: 0.02 - 240s 1s/step - loss: 2.6452 - accuracy: 0.02 - 241s 1s/step - loss: 2.6445 - accuracy: 0.02 - 242s 1s/step - loss: 2.6427 - accuracy: 0.02 - 243s 1s/step - loss: 2.6406 - accuracy: 0.02 - 244s 1s/step - loss: 2.6380 - accuracy: 0.02 - 245s 1s/step - loss: 2.6357 - accuracy: 0.02 - 246s 1s/step - loss: 2.6342 - accuracy: 0.02 - 247s 1s/step - loss: 2.6339 - accuracy: 0.02 - 247s 1s/step - loss: 2.6316 - accuracy: 0.02 - 249s 1s/step - loss: 2.6310 - accuracy: 0.02 - 250s 1s/step - loss: 2.6295 - accuracy: 0.02 - 251s 1s/step - loss: 2.6271 - accuracy: 0.02 - 252s 1s/step - loss: 2.6255 - accuracy: 0.02 - 253s 1s/step - loss: 2.6238 - accuracy: 0.02 - 254s 1s/step - loss: 2.6219 - accuracy: 0.02 - 255s 1s/step - loss: 2.6199 - accuracy: 0.02 - 255s 1s/step - loss: 2.6195 - accuracy: 0.02 - 256s 1s/step - loss: 2.6169 - accuracy: 0.02 - 257s 1s/step - loss: 2.6160 - accuracy: 0.02 - 258s 1s/step - loss: 2.6139 - accuracy: 0.02 - 259s 1s/step - loss: 2.6118 - accuracy: 0.02 - 260s 1s/step - loss: 2.6102 - accuracy: 0.02 - 261s 1s/step - loss: 2.6086 - accuracy: 0.02 - 262s 1s/step - loss: 2.6071 - accuracy: 0.02 - 263s 1s/step - loss: 2.6049 - accuracy: 0.02 - 264s 1s/step - loss: 2.6032 - accuracy: 0.02 - 265s 1s/step - loss: 2.6019 - accuracy: 0.02 - 266s 1s/step - loss: 2.5992 - accuracy: 0.02 - 267s 1s/step - loss: 2.5966 - accuracy: 0.02 - 268s 1s/step - loss: 2.5940 - accuracy: 0.02 - 269s 1s/step - loss: 2.5911 - accuracy: 0.02 - 270s 1s/step - loss: 2.5894 - accuracy: 0.02 - 271s 1s/step - loss: 2.5884 - accuracy: 0.02 - 272s 1s/step - loss: 2.5876 - accuracy: 0.02 - 273s 1s/step - loss: 2.5857 - accuracy: 0.02 - 274s 1s/step - loss: 2.5849 - accuracy: 0.02 - 275s 1s/step - loss: 2.5846 - accuracy: 0.02 - 276s 1s/step - loss: 2.5820 - accuracy: 0.02 - 277s 1s/step - loss: 2.5808 - accuracy: 0.02 - 278s 1s/step - loss: 2.5793 - accuracy: 0.02 - 279s 1s/step - loss: 2.5788 - accuracy: 0.02 - 280s 1s/step - loss: 2.5781 - accuracy: 0.02 - 281s 1s/step - loss: 2.5766 - accuracy: 0.02 - 282s 1s/step - loss: 2.5744 - accuracy: 0.02 - 283s 1s/step - loss: 2.5737 - accuracy: 0.02 - 284s 1s/step - loss: 2.5722 - accuracy: 0.02 - 285s 1s/step - loss: 2.5704 - accuracy: 0.02 - 286s 1s/step - loss: 2.5694 - accuracy: 0.02 - 287s 1s/step - loss: 2.5671 - accuracy: 0.02 - 288s 1s/step - loss: 2.5653 - accuracy: 0.02 - 289s 1s/step - loss: 2.5640 - accuracy: 0.02 - 290s 1s/step - loss: 2.5633 - accuracy: 0.02 - 291s 1s/step - loss: 2.5623 - accuracy: 0.02 - 292s 1s/step - loss: 2.5602 - accuracy: 0.02 - 293s 1s/step - loss: 2.5588 - accuracy: 0.02 - 293s 1s/step - loss: 2.5577 - accuracy: 0.02 - 294s 1s/step - loss: 2.5567 - accuracy: 0.02 - 295s 1s/step - loss: 2.5556 - accuracy: 0.02 - 296s 1s/step - loss: 2.5553 - accuracy: 0.02 - 297s 1s/step - loss: 2.5537 - accuracy: 0.02 - 298s 1s/step - loss: 2.5508 - accuracy: 0.02 - 299s 1s/step - loss: 2.5493 - accuracy: 0.02 - 300s 1s/step - loss: 2.5480 - accuracy: 0.02 - 301s 1s/step - loss: 2.5457 - accuracy: 0.02 - 302s 1s/step - loss: 2.5438 - accuracy: 0.02 - 303s 1s/step - loss: 2.5425 - accuracy: 0.02 - 304s 1s/step - loss: 2.5400 - accuracy: 0.02 - 305s 1s/step - loss: 2.5381 - accuracy: 0.02 - 306s 1s/step - loss: 2.5377 - accuracy: 0.02 - 307s 1s/step - loss: 2.5365 - accuracy: 0.02 - 308s 1s/step - loss: 2.5345 - accuracy: 0.02 - 309s 1s/step - loss: 2.5326 - accuracy: 0.02 - 310s 1s/step - loss: 2.5317 - accuracy: 0.02 - 311s 1s/step - loss: 2.5302 - accuracy: 0.02 - 312s 1s/step - loss: 2.5286 - accuracy: 0.02 - 313s 1s/step - loss: 2.5267 - accuracy: 0.02 - 314s 1000ms/step - loss: 2.5257 - accuracy: 0.02 - 315s 1000ms/step - loss: 2.5234 - accuracy: 0.02 - 316s 1000ms/step - loss: 2.5222 - accuracy: 0.02 - 317s 1000ms/step - loss: 2.5190 - accuracy: 0.02 - 318s 1000ms/step - loss: 2.5185 - accuracy: 0.02 - 319s 1000ms/step - loss: 2.5175 - accuracy: 0.02 - 320s 999ms/step - loss: 2.5147 - accuracy: 0.0228 - 321s 1000ms/step - loss: 2.5129 - accuracy: 0.02 - 322s 1000ms/step - loss: 2.5117 - accuracy: 0.02 - 323s 999ms/step - loss: 2.5098 - accuracy: 0.0229 - 324s 999ms/step - loss: 2.5082 - accuracy: 0.023 - 325s 999ms/step - loss: 2.5076 - accuracy: 0.023 - 326s 999ms/step - loss: 2.5063 - accuracy: 0.023 - 327s 999ms/step - loss: 2.5051 - accuracy: 0.023 - 328s 999ms/step - loss: 2.5037 - accuracy: 0.023 - 329s 999ms/step - loss: 2.5023 - accuracy: 0.023 - 330s 999ms/step - loss: 2.5016 - accuracy: 0.023 - 331s 999ms/step - loss: 2.4998 - accuracy: 0.023 - 332s 999ms/step - loss: 2.4977 - accuracy: 0.023 - 332s 998ms/step - loss: 2.4956 - accuracy: 0.023 - 334s 999ms/step - loss: 2.4938 - accuracy: 0.023 - 335s 999ms/step - loss: 2.4933 - accuracy: 0.023 - 336s 999ms/step - loss: 2.4917 - accuracy: 0.023 - 337s 999ms/step - loss: 2.4899 - accuracy: 0.023 - 338s 999ms/step - loss: 2.4876 - accuracy: 0.023 - 338s 999ms/step - loss: 2.4863 - accuracy: 0.023 - 340s 999ms/step - loss: 2.4847 - accuracy: 0.023 - 340s 999ms/step - loss: 2.4833 - accuracy: 0.023 - 341s 998ms/step - loss: 2.4814 - accuracy: 0.0240345/345 [==============================]: 2.4791 - accuracy: 0.024 - 343s 998ms/step - loss: 2.4765 - accuracy: 0.024 - 344s 998ms/step - loss: 2.4754 - accuracy: 0.024 - 344s 998ms/step - loss: 2.4754 - accuracy: 0.0241\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/345 [===============>..............] - ETA: 5:35 - loss: 1.9808 - accuracy: 0.04 - ETA: 5:35 - loss: 1.8036 - accuracy: 0.04 - ETA: 5:34 - loss: 1.9615 - accuracy: 0.04 - ETA: 5:32 - loss: 1.9597 - accuracy: 0.04 - ETA: 5:33 - loss: 2.0431 - accuracy: 0.04 - ETA: 5:31 - loss: 2.0073 - accuracy: 0.04 - ETA: 5:29 - loss: 1.9823 - accuracy: 0.04 - ETA: 5:26 - loss: 2.0021 - accuracy: 0.04 - ETA: 5:27 - loss: 2.0054 - accuracy: 0.04 - ETA: 5:25 - loss: 1.9903 - accuracy: 0.04 - ETA: 5:25 - loss: 2.0005 - accuracy: 0.04 - ETA: 5:25 - loss: 2.0015 - accuracy: 0.04 - ETA: 5:22 - loss: 1.9874 - accuracy: 0.04 - ETA: 5:20 - loss: 1.9886 - accuracy: 0.04 - ETA: 5:18 - loss: 1.9635 - accuracy: 0.04 - ETA: 5:19 - loss: 1.9389 - accuracy: 0.04 - ETA: 5:18 - loss: 1.9402 - accuracy: 0.04 - ETA: 5:17 - loss: 1.9491 - accuracy: 0.04 - ETA: 5:16 - loss: 1.9547 - accuracy: 0.04 - ETA: 5:15 - loss: 1.9610 - accuracy: 0.04 - ETA: 5:14 - loss: 1.9508 - accuracy: 0.04 - ETA: 5:13 - loss: 1.9503 - accuracy: 0.04 - ETA: 5:13 - loss: 1.9410 - accuracy: 0.04 - ETA: 5:11 - loss: 1.9395 - accuracy: 0.04 - ETA: 5:11 - loss: 1.9366 - accuracy: 0.04 - ETA: 5:09 - loss: 1.9346 - accuracy: 0.04 - ETA: 5:08 - loss: 1.9364 - accuracy: 0.04 - ETA: 5:07 - loss: 1.9370 - accuracy: 0.04 - ETA: 5:07 - loss: 1.9309 - accuracy: 0.04 - ETA: 5:06 - loss: 1.9296 - accuracy: 0.04 - ETA: 5:05 - loss: 1.9319 - accuracy: 0.04 - ETA: 5:04 - loss: 1.9334 - accuracy: 0.04 - ETA: 5:02 - loss: 1.9303 - accuracy: 0.04 - ETA: 5:01 - loss: 1.9288 - accuracy: 0.04 - ETA: 5:00 - loss: 1.9369 - accuracy: 0.04 - ETA: 5:00 - loss: 1.9332 - accuracy: 0.04 - ETA: 4:59 - loss: 1.9234 - accuracy: 0.04 - ETA: 4:58 - loss: 1.9260 - accuracy: 0.04 - ETA: 4:57 - loss: 1.9304 - accuracy: 0.04 - ETA: 4:56 - loss: 1.9342 - accuracy: 0.04 - ETA: 4:55 - loss: 1.9287 - accuracy: 0.04 - ETA: 4:54 - loss: 1.9232 - accuracy: 0.04 - ETA: 4:53 - loss: 1.9203 - accuracy: 0.04 - ETA: 4:52 - loss: 1.9158 - accuracy: 0.04 - ETA: 4:51 - loss: 1.9146 - accuracy: 0.04 - ETA: 4:50 - loss: 1.9156 - accuracy: 0.04 - ETA: 4:48 - loss: 1.9137 - accuracy: 0.04 - ETA: 4:48 - loss: 1.9157 - accuracy: 0.04 - ETA: 4:47 - loss: 1.9137 - accuracy: 0.04 - ETA: 4:46 - loss: 1.9148 - accuracy: 0.04 - ETA: 4:45 - loss: 1.9156 - accuracy: 0.04 - ETA: 4:44 - loss: 1.9057 - accuracy: 0.04 - ETA: 4:42 - loss: 1.8997 - accuracy: 0.04 - ETA: 4:41 - loss: 1.8995 - accuracy: 0.04 - ETA: 4:41 - loss: 1.9035 - accuracy: 0.04 - ETA: 4:40 - loss: 1.9076 - accuracy: 0.04 - ETA: 4:39 - loss: 1.9102 - accuracy: 0.04 - ETA: 4:38 - loss: 1.9117 - accuracy: 0.04 - ETA: 4:37 - loss: 1.9146 - accuracy: 0.04 - ETA: 4:36 - loss: 1.9101 - accuracy: 0.04 - ETA: 4:35 - loss: 1.9075 - accuracy: 0.04 - ETA: 4:34 - loss: 1.9060 - accuracy: 0.04 - ETA: 4:33 - loss: 1.9103 - accuracy: 0.04 - ETA: 4:32 - loss: 1.9122 - accuracy: 0.04 - ETA: 4:31 - loss: 1.9082 - accuracy: 0.04 - ETA: 4:30 - loss: 1.9067 - accuracy: 0.04 - ETA: 4:29 - loss: 1.9053 - accuracy: 0.04 - ETA: 4:28 - loss: 1.9028 - accuracy: 0.04 - ETA: 4:28 - loss: 1.9105 - accuracy: 0.04 - ETA: 4:26 - loss: 1.9076 - accuracy: 0.04 - ETA: 4:26 - loss: 1.9088 - accuracy: 0.04 - ETA: 4:25 - loss: 1.9075 - accuracy: 0.04 - ETA: 4:24 - loss: 1.9089 - accuracy: 0.04 - ETA: 4:23 - loss: 1.9086 - accuracy: 0.04 - ETA: 4:22 - loss: 1.9069 - accuracy: 0.04 - ETA: 4:21 - loss: 1.9102 - accuracy: 0.04 - ETA: 4:20 - loss: 1.9101 - accuracy: 0.04 - ETA: 4:19 - loss: 1.9101 - accuracy: 0.04 - ETA: 4:18 - loss: 1.9070 - accuracy: 0.04 - ETA: 4:17 - loss: 1.9048 - accuracy: 0.04 - ETA: 4:16 - loss: 1.9022 - accuracy: 0.04 - ETA: 4:15 - loss: 1.9057 - accuracy: 0.04 - ETA: 4:14 - loss: 1.9067 - accuracy: 0.04 - ETA: 4:14 - loss: 1.9079 - accuracy: 0.04 - ETA: 4:12 - loss: 1.9066 - accuracy: 0.04 - ETA: 4:11 - loss: 1.9051 - accuracy: 0.04 - ETA: 4:10 - loss: 1.9065 - accuracy: 0.04 - ETA: 4:09 - loss: 1.9078 - accuracy: 0.04 - ETA: 4:08 - loss: 1.9074 - accuracy: 0.04 - ETA: 4:07 - loss: 1.9015 - accuracy: 0.04 - ETA: 4:07 - loss: 1.8966 - accuracy: 0.04 - ETA: 4:06 - loss: 1.8935 - accuracy: 0.04 - ETA: 4:05 - loss: 1.8897 - accuracy: 0.04 - ETA: 4:04 - loss: 1.8873 - accuracy: 0.04 - ETA: 4:03 - loss: 1.8907 - accuracy: 0.04 - ETA: 4:02 - loss: 1.8976 - accuracy: 0.04 - ETA: 4:01 - loss: 1.8954 - accuracy: 0.04 - ETA: 4:00 - loss: 1.8956 - accuracy: 0.04 - ETA: 3:59 - loss: 1.8956 - accuracy: 0.04 - ETA: 3:58 - loss: 1.8965 - accuracy: 0.04 - ETA: 3:57 - loss: 1.8950 - accuracy: 0.04 - ETA: 3:56 - loss: 1.8947 - accuracy: 0.04 - ETA: 3:55 - loss: 1.8922 - accuracy: 0.04 - ETA: 3:54 - loss: 1.8928 - accuracy: 0.04 - ETA: 3:53 - loss: 1.8910 - accuracy: 0.04 - ETA: 3:52 - loss: 1.8910 - accuracy: 0.04 - ETA: 3:51 - loss: 1.8902 - accuracy: 0.04 - ETA: 3:50 - loss: 1.8889 - accuracy: 0.04 - ETA: 3:49 - loss: 1.8891 - accuracy: 0.04 - ETA: 3:48 - loss: 1.8867 - accuracy: 0.04 - ETA: 3:47 - loss: 1.8837 - accuracy: 0.04 - ETA: 3:46 - loss: 1.8831 - accuracy: 0.04 - ETA: 3:45 - loss: 1.8809 - accuracy: 0.04 - ETA: 3:44 - loss: 1.8793 - accuracy: 0.04 - ETA: 3:43 - loss: 1.8790 - accuracy: 0.04 - ETA: 3:42 - loss: 1.8783 - accuracy: 0.04 - ETA: 3:41 - loss: 1.8776 - accuracy: 0.04 - ETA: 3:40 - loss: 1.8782 - accuracy: 0.04 - ETA: 3:39 - loss: 1.8774 - accuracy: 0.04 - ETA: 3:38 - loss: 1.8763 - accuracy: 0.04 - ETA: 3:37 - loss: 1.8754 - accuracy: 0.04 - ETA: 3:36 - loss: 1.8735 - accuracy: 0.04 - ETA: 3:35 - loss: 1.8707 - accuracy: 0.04 - ETA: 3:34 - loss: 1.8728 - accuracy: 0.04 - ETA: 3:33 - loss: 1.8724 - accuracy: 0.04 - ETA: 3:32 - loss: 1.8726 - accuracy: 0.04 - ETA: 3:31 - loss: 1.8708 - accuracy: 0.04 - ETA: 3:30 - loss: 1.8720 - accuracy: 0.04 - ETA: 3:29 - loss: 1.8727 - accuracy: 0.04 - ETA: 3:28 - loss: 1.8706 - accuracy: 0.04 - ETA: 3:27 - loss: 1.8710 - accuracy: 0.04 - ETA: 3:26 - loss: 1.8708 - accuracy: 0.04 - ETA: 3:25 - loss: 1.8708 - accuracy: 0.04 - ETA: 3:24 - loss: 1.8702 - accuracy: 0.05 - ETA: 3:23 - loss: 1.8683 - accuracy: 0.05 - ETA: 3:23 - loss: 1.8681 - accuracy: 0.05 - ETA: 3:22 - loss: 1.8686 - accuracy: 0.05 - ETA: 3:21 - loss: 1.8691 - accuracy: 0.05 - ETA: 3:20 - loss: 1.8671 - accuracy: 0.05 - ETA: 3:19 - loss: 1.8665 - accuracy: 0.05 - ETA: 3:18 - loss: 1.8640 - accuracy: 0.05 - ETA: 3:17 - loss: 1.8643 - accuracy: 0.05 - ETA: 3:16 - loss: 1.8650 - accuracy: 0.05 - ETA: 3:15 - loss: 1.8635 - accuracy: 0.05 - ETA: 3:14 - loss: 1.8635 - accuracy: 0.05 - ETA: 3:13 - loss: 1.8638 - accuracy: 0.05 - ETA: 3:12 - loss: 1.8640 - accuracy: 0.05 - ETA: 3:11 - loss: 1.8633 - accuracy: 0.05 - ETA: 3:10 - loss: 1.8636 - accuracy: 0.05 - ETA: 3:09 - loss: 1.8647 - accuracy: 0.05 - ETA: 3:08 - loss: 1.8632 - accuracy: 0.05 - ETA: 3:07 - loss: 1.8652 - accuracy: 0.05 - ETA: 3:06 - loss: 1.8637 - accuracy: 0.05 - ETA: 3:05 - loss: 1.8636 - accuracy: 0.05 - ETA: 3:04 - loss: 1.8629 - accuracy: 0.05 - ETA: 3:03 - loss: 1.8613 - accuracy: 0.05 - ETA: 3:02 - loss: 1.8607 - accuracy: 0.05 - ETA: 3:01 - loss: 1.8621 - accuracy: 0.05 - ETA: 3:00 - loss: 1.8600 - accuracy: 0.05 - ETA: 2:59 - loss: 1.8587 - accuracy: 0.05 - ETA: 2:58 - loss: 1.8592 - accuracy: 0.05 - ETA: 2:57 - loss: 1.8587 - accuracy: 0.05 - ETA: 2:56 - loss: 1.8585 - accuracy: 0.05 - ETA: 2:55 - loss: 1.8579 - accuracy: 0.05 - ETA: 2:54 - loss: 1.8564 - accuracy: 0.05 - ETA: 2:53 - loss: 1.8564 - accuracy: 0.05 - ETA: 2:52 - loss: 1.8537 - accuracy: 0.05 - ETA: 2:51 - loss: 1.8531 - accuracy: 0.05 - ETA: 2:50 - loss: 1.8535 - accuracy: 0.05 - ETA: 2:49 - loss: 1.8529 - accuracy: 0.05 - ETA: 2:48 - loss: 1.8509 - accuracy: 0.05 - ETA: 2:47 - loss: 1.8512 - accuracy: 0.05 - ETA: 2:46 - loss: 1.8510 - accuracy: 0.05 - ETA: 2:45 - loss: 1.8500 - accuracy: 0.05 - ETA: 2:45 - loss: 1.8506 - accuracy: 0.05 - ETA: 2:44 - loss: 1.8501 - accuracy: 0.05 - ETA: 2:43 - loss: 1.8492 - accuracy: 0.05 - ETA: 2:42 - loss: 1.8488 - accuracy: 0.05 - ETA: 2:41 - loss: 1.8467 - accuracy: 0.05 - ETA: 2:40 - loss: 1.8466 - accuracy: 0.05 - ETA: 2:39 - loss: 1.8457 - accuracy: 0.05 - ETA: 2:38 - loss: 1.8440 - accuracy: 0.05 - ETA: 2:37 - loss: 1.8438 - accuracy: 0.05 - ETA: 2:36 - loss: 1.8438 - accuracy: 0.05 - ETA: 2:35 - loss: 1.8426 - accuracy: 0.05 - ETA: 2:34 - loss: 1.8409 - accuracy: 0.0538345/345 [==============================] - ETA: 2:33 - loss: 1.8389 - accuracy: 0.05 - ETA: 2:32 - loss: 1.8369 - accuracy: 0.05 - ETA: 2:31 - loss: 1.8352 - accuracy: 0.05 - ETA: 2:30 - loss: 1.8338 - accuracy: 0.05 - ETA: 2:29 - loss: 1.8305 - accuracy: 0.05 - ETA: 2:28 - loss: 1.8297 - accuracy: 0.05 - ETA: 2:27 - loss: 1.8282 - accuracy: 0.05 - ETA: 2:26 - loss: 1.8266 - accuracy: 0.05 - ETA: 2:25 - loss: 1.8249 - accuracy: 0.05 - ETA: 2:24 - loss: 1.8251 - accuracy: 0.05 - ETA: 2:23 - loss: 1.8233 - accuracy: 0.05 - ETA: 2:22 - loss: 1.8233 - accuracy: 0.05 - ETA: 2:21 - loss: 1.8240 - accuracy: 0.05 - ETA: 2:20 - loss: 1.8225 - accuracy: 0.05 - ETA: 2:19 - loss: 1.8211 - accuracy: 0.05 - ETA: 2:18 - loss: 1.8207 - accuracy: 0.05 - ETA: 2:17 - loss: 1.8194 - accuracy: 0.05 - ETA: 2:17 - loss: 1.8188 - accuracy: 0.05 - ETA: 2:16 - loss: 1.8179 - accuracy: 0.05 - ETA: 2:15 - loss: 1.8181 - accuracy: 0.05 - ETA: 2:14 - loss: 1.8179 - accuracy: 0.05 - ETA: 2:13 - loss: 1.8167 - accuracy: 0.05 - ETA: 2:12 - loss: 1.8165 - accuracy: 0.05 - ETA: 2:11 - loss: 1.8148 - accuracy: 0.05 - ETA: 2:10 - loss: 1.8148 - accuracy: 0.05 - ETA: 2:09 - loss: 1.8129 - accuracy: 0.05 - ETA: 2:08 - loss: 1.8114 - accuracy: 0.05 - ETA: 2:07 - loss: 1.8112 - accuracy: 0.05 - ETA: 2:06 - loss: 1.8097 - accuracy: 0.05 - ETA: 2:05 - loss: 1.8087 - accuracy: 0.05 - ETA: 2:04 - loss: 1.8071 - accuracy: 0.05 - ETA: 2:03 - loss: 1.8083 - accuracy: 0.05 - ETA: 2:02 - loss: 1.8081 - accuracy: 0.05 - ETA: 2:01 - loss: 1.8071 - accuracy: 0.05 - ETA: 2:00 - loss: 1.8067 - accuracy: 0.05 - ETA: 1:59 - loss: 1.8060 - accuracy: 0.05 - ETA: 1:58 - loss: 1.8052 - accuracy: 0.05 - ETA: 1:57 - loss: 1.8045 - accuracy: 0.05 - ETA: 1:56 - loss: 1.8053 - accuracy: 0.05 - ETA: 1:55 - loss: 1.8046 - accuracy: 0.05 - ETA: 1:54 - loss: 1.8037 - accuracy: 0.05 - ETA: 1:53 - loss: 1.8035 - accuracy: 0.05 - ETA: 1:52 - loss: 1.8043 - accuracy: 0.05 - ETA: 1:51 - loss: 1.8056 - accuracy: 0.05 - ETA: 1:50 - loss: 1.8048 - accuracy: 0.05 - ETA: 1:49 - loss: 1.8035 - accuracy: 0.05 - ETA: 1:48 - loss: 1.8036 - accuracy: 0.05 - ETA: 1:47 - loss: 1.8029 - accuracy: 0.05 - ETA: 1:46 - loss: 1.8021 - accuracy: 0.05 - ETA: 1:45 - loss: 1.8023 - accuracy: 0.05 - ETA: 1:44 - loss: 1.8025 - accuracy: 0.05 - ETA: 1:43 - loss: 1.8028 - accuracy: 0.05 - ETA: 1:42 - loss: 1.8033 - accuracy: 0.05 - ETA: 1:41 - loss: 1.8032 - accuracy: 0.05 - ETA: 1:40 - loss: 1.8030 - accuracy: 0.05 - ETA: 1:39 - loss: 1.8027 - accuracy: 0.05 - ETA: 1:38 - loss: 1.8032 - accuracy: 0.05 - ETA: 1:37 - loss: 1.8030 - accuracy: 0.05 - ETA: 1:36 - loss: 1.8031 - accuracy: 0.05 - ETA: 1:35 - loss: 1.8032 - accuracy: 0.05 - ETA: 1:34 - loss: 1.8036 - accuracy: 0.05 - ETA: 1:33 - loss: 1.8022 - accuracy: 0.05 - ETA: 1:32 - loss: 1.8013 - accuracy: 0.05 - ETA: 1:31 - loss: 1.8006 - accuracy: 0.05 - ETA: 1:30 - loss: 1.7997 - accuracy: 0.05 - ETA: 1:29 - loss: 1.7993 - accuracy: 0.05 - ETA: 1:28 - loss: 1.7993 - accuracy: 0.05 - ETA: 1:27 - loss: 1.7992 - accuracy: 0.05 - ETA: 1:26 - loss: 1.7987 - accuracy: 0.05 - ETA: 1:25 - loss: 1.7981 - accuracy: 0.05 - ETA: 1:24 - loss: 1.7977 - accuracy: 0.05 - ETA: 1:23 - loss: 1.7980 - accuracy: 0.05 - ETA: 1:22 - loss: 1.7984 - accuracy: 0.05 - ETA: 1:21 - loss: 1.7984 - accuracy: 0.05 - ETA: 1:20 - loss: 1.7987 - accuracy: 0.05 - ETA: 1:19 - loss: 1.7979 - accuracy: 0.05 - ETA: 1:18 - loss: 1.7983 - accuracy: 0.05 - ETA: 1:17 - loss: 1.7987 - accuracy: 0.05 - ETA: 1:16 - loss: 1.7977 - accuracy: 0.05 - ETA: 1:15 - loss: 1.7970 - accuracy: 0.05 - ETA: 1:14 - loss: 1.7971 - accuracy: 0.05 - ETA: 1:13 - loss: 1.7959 - accuracy: 0.05 - ETA: 1:12 - loss: 1.7955 - accuracy: 0.05 - ETA: 1:11 - loss: 1.7948 - accuracy: 0.05 - ETA: 1:11 - loss: 1.7937 - accuracy: 0.05 - ETA: 1:10 - loss: 1.7931 - accuracy: 0.05 - ETA: 1:09 - loss: 1.7923 - accuracy: 0.05 - ETA: 1:08 - loss: 1.7913 - accuracy: 0.05 - ETA: 1:07 - loss: 1.7912 - accuracy: 0.05 - ETA: 1:06 - loss: 1.7912 - accuracy: 0.05 - ETA: 1:05 - loss: 1.7903 - accuracy: 0.05 - ETA: 1:04 - loss: 1.7903 - accuracy: 0.05 - ETA: 1:03 - loss: 1.7901 - accuracy: 0.05 - ETA: 1:02 - loss: 1.7887 - accuracy: 0.05 - ETA: 1:01 - loss: 1.7882 - accuracy: 0.05 - ETA: 1:00 - loss: 1.7875 - accuracy: 0.05 - ETA: 59s - loss: 1.7865 - accuracy: 0.0592 - ETA: 58s - loss: 1.7855 - accuracy: 0.059 - ETA: 57s - loss: 1.7851 - accuracy: 0.059 - ETA: 56s - loss: 1.7844 - accuracy: 0.059 - ETA: 55s - loss: 1.7835 - accuracy: 0.059 - ETA: 54s - loss: 1.7840 - accuracy: 0.059 - ETA: 53s - loss: 1.7831 - accuracy: 0.059 - ETA: 52s - loss: 1.7831 - accuracy: 0.059 - ETA: 52s - loss: 1.7829 - accuracy: 0.059 - ETA: 51s - loss: 1.7830 - accuracy: 0.059 - ETA: 50s - loss: 1.7829 - accuracy: 0.059 - ETA: 49s - loss: 1.7826 - accuracy: 0.059 - ETA: 48s - loss: 1.7827 - accuracy: 0.059 - ETA: 47s - loss: 1.7823 - accuracy: 0.059 - ETA: 46s - loss: 1.7817 - accuracy: 0.059 - ETA: 45s - loss: 1.7819 - accuracy: 0.059 - ETA: 44s - loss: 1.7806 - accuracy: 0.059 - ETA: 43s - loss: 1.7796 - accuracy: 0.059 - ETA: 42s - loss: 1.7786 - accuracy: 0.059 - ETA: 41s - loss: 1.7780 - accuracy: 0.059 - ETA: 40s - loss: 1.7781 - accuracy: 0.059 - ETA: 39s - loss: 1.7765 - accuracy: 0.059 - ETA: 38s - loss: 1.7754 - accuracy: 0.059 - ETA: 37s - loss: 1.7751 - accuracy: 0.060 - ETA: 36s - loss: 1.7755 - accuracy: 0.060 - ETA: 35s - loss: 1.7750 - accuracy: 0.060 - ETA: 34s - loss: 1.7746 - accuracy: 0.060 - ETA: 33s - loss: 1.7732 - accuracy: 0.060 - ETA: 32s - loss: 1.7723 - accuracy: 0.060 - ETA: 31s - loss: 1.7716 - accuracy: 0.060 - ETA: 30s - loss: 1.7707 - accuracy: 0.060 - ETA: 29s - loss: 1.7708 - accuracy: 0.060 - ETA: 29s - loss: 1.7708 - accuracy: 0.060 - ETA: 28s - loss: 1.7695 - accuracy: 0.060 - ETA: 27s - loss: 1.7692 - accuracy: 0.060 - ETA: 26s - loss: 1.7694 - accuracy: 0.060 - ETA: 25s - loss: 1.7684 - accuracy: 0.060 - ETA: 24s - loss: 1.7692 - accuracy: 0.060 - ETA: 23s - loss: 1.7685 - accuracy: 0.060 - ETA: 22s - loss: 1.7674 - accuracy: 0.060 - ETA: 21s - loss: 1.7671 - accuracy: 0.060 - ETA: 20s - loss: 1.7669 - accuracy: 0.060 - ETA: 19s - loss: 1.7668 - accuracy: 0.060 - ETA: 18s - loss: 1.7662 - accuracy: 0.060 - ETA: 17s - loss: 1.7657 - accuracy: 0.060 - ETA: 16s - loss: 1.7648 - accuracy: 0.060 - ETA: 15s - loss: 1.7648 - accuracy: 0.060 - ETA: 14s - loss: 1.7642 - accuracy: 0.060 - ETA: 13s - loss: 1.7645 - accuracy: 0.060 - ETA: 12s - loss: 1.7640 - accuracy: 0.060 - ETA: 11s - loss: 1.7643 - accuracy: 0.060 - ETA: 10s - loss: 1.7642 - accuracy: 0.061 - ETA: 9s - loss: 1.7639 - accuracy: 0.061 - ETA: 8s - loss: 1.7637 - accuracy: 0.06 - ETA: 7s - loss: 1.7642 - accuracy: 0.06 - ETA: 6s - loss: 1.7647 - accuracy: 0.06 - ETA: 5s - loss: 1.7651 - accuracy: 0.06 - ETA: 4s - loss: 1.7648 - accuracy: 0.06 - ETA: 3s - loss: 1.7656 - accuracy: 0.06 - ETA: 2s - loss: 1.7642 - accuracy: 0.06 - ETA: 1s - loss: 1.7645 - accuracy: 0.06 - ETA: 0s - loss: 1.7638 - accuracy: 0.06 - 334s 969ms/step - loss: 1.7638 - accuracy: 0.0614\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/345 [===============>..............] - ETA: 5:41 - loss: 1.8109 - accuracy: 0.07 - ETA: 5:44 - loss: 1.7009 - accuracy: 0.07 - ETA: 5:38 - loss: 1.6280 - accuracy: 0.07 - ETA: 5:37 - loss: 1.5886 - accuracy: 0.07 - ETA: 5:37 - loss: 1.5937 - accuracy: 0.07 - ETA: 5:40 - loss: 1.5968 - accuracy: 0.07 - ETA: 5:39 - loss: 1.5662 - accuracy: 0.07 - ETA: 5:36 - loss: 1.5586 - accuracy: 0.07 - ETA: 5:35 - loss: 1.5634 - accuracy: 0.07 - ETA: 5:35 - loss: 1.5906 - accuracy: 0.07 - ETA: 5:35 - loss: 1.6024 - accuracy: 0.07 - ETA: 5:48 - loss: 1.6040 - accuracy: 0.07 - ETA: 6:02 - loss: 1.5915 - accuracy: 0.07 - ETA: 6:07 - loss: 1.5907 - accuracy: 0.07 - ETA: 6:00 - loss: 1.5898 - accuracy: 0.07 - ETA: 5:55 - loss: 1.6093 - accuracy: 0.07 - ETA: 5:50 - loss: 1.6012 - accuracy: 0.07 - ETA: 5:45 - loss: 1.6020 - accuracy: 0.07 - ETA: 5:41 - loss: 1.6002 - accuracy: 0.07 - ETA: 5:37 - loss: 1.6065 - accuracy: 0.07 - ETA: 5:35 - loss: 1.6021 - accuracy: 0.07 - ETA: 5:33 - loss: 1.6106 - accuracy: 0.07 - ETA: 5:36 - loss: 1.6116 - accuracy: 0.07 - ETA: 5:33 - loss: 1.6212 - accuracy: 0.07 - ETA: 5:31 - loss: 1.6240 - accuracy: 0.07 - ETA: 5:28 - loss: 1.6244 - accuracy: 0.07 - ETA: 5:25 - loss: 1.6222 - accuracy: 0.07 - ETA: 5:29 - loss: 1.6209 - accuracy: 0.07 - ETA: 5:29 - loss: 1.6273 - accuracy: 0.07 - ETA: 5:26 - loss: 1.6261 - accuracy: 0.07 - ETA: 5:23 - loss: 1.6280 - accuracy: 0.07 - ETA: 5:22 - loss: 1.6279 - accuracy: 0.07 - ETA: 5:19 - loss: 1.6140 - accuracy: 0.07 - ETA: 5:16 - loss: 1.6192 - accuracy: 0.07 - ETA: 5:15 - loss: 1.6193 - accuracy: 0.07 - ETA: 5:13 - loss: 1.6147 - accuracy: 0.07 - ETA: 5:14 - loss: 1.6215 - accuracy: 0.07 - ETA: 5:15 - loss: 1.6159 - accuracy: 0.07 - ETA: 5:14 - loss: 1.6109 - accuracy: 0.07 - ETA: 5:20 - loss: 1.6070 - accuracy: 0.07 - ETA: 5:22 - loss: 1.6049 - accuracy: 0.07 - ETA: 5:21 - loss: 1.6049 - accuracy: 0.07 - ETA: 5:19 - loss: 1.6063 - accuracy: 0.07 - ETA: 5:18 - loss: 1.6048 - accuracy: 0.07 - ETA: 5:16 - loss: 1.6018 - accuracy: 0.07 - ETA: 5:15 - loss: 1.6085 - accuracy: 0.07 - ETA: 5:13 - loss: 1.6092 - accuracy: 0.07 - ETA: 5:12 - loss: 1.6142 - accuracy: 0.07 - ETA: 5:11 - loss: 1.6131 - accuracy: 0.07 - ETA: 5:10 - loss: 1.6140 - accuracy: 0.07 - ETA: 5:09 - loss: 1.6141 - accuracy: 0.07 - ETA: 5:07 - loss: 1.6158 - accuracy: 0.07 - ETA: 5:06 - loss: 1.6161 - accuracy: 0.07 - ETA: 5:04 - loss: 1.6161 - accuracy: 0.07 - ETA: 5:02 - loss: 1.6143 - accuracy: 0.07 - ETA: 5:00 - loss: 1.6139 - accuracy: 0.07 - ETA: 4:58 - loss: 1.6155 - accuracy: 0.07 - ETA: 4:57 - loss: 1.6163 - accuracy: 0.07 - ETA: 4:55 - loss: 1.6190 - accuracy: 0.07 - ETA: 4:53 - loss: 1.6200 - accuracy: 0.07 - ETA: 4:52 - loss: 1.6164 - accuracy: 0.07 - ETA: 4:50 - loss: 1.6132 - accuracy: 0.07 - ETA: 4:49 - loss: 1.6176 - accuracy: 0.07 - ETA: 4:47 - loss: 1.6170 - accuracy: 0.07 - ETA: 4:46 - loss: 1.6158 - accuracy: 0.07 - ETA: 4:45 - loss: 1.6144 - accuracy: 0.07 - ETA: 4:43 - loss: 1.6146 - accuracy: 0.07 - ETA: 4:42 - loss: 1.6155 - accuracy: 0.07 - ETA: 4:40 - loss: 1.6167 - accuracy: 0.07 - ETA: 4:40 - loss: 1.6131 - accuracy: 0.07 - ETA: 4:38 - loss: 1.6085 - accuracy: 0.07 - ETA: 4:37 - loss: 1.6087 - accuracy: 0.07 - ETA: 4:36 - loss: 1.6101 - accuracy: 0.07 - ETA: 4:35 - loss: 1.6120 - accuracy: 0.07 - ETA: 4:34 - loss: 1.6103 - accuracy: 0.07 - ETA: 4:32 - loss: 1.6103 - accuracy: 0.07 - ETA: 4:31 - loss: 1.6111 - accuracy: 0.07 - ETA: 4:30 - loss: 1.6109 - accuracy: 0.07 - ETA: 4:29 - loss: 1.6095 - accuracy: 0.07 - ETA: 4:28 - loss: 1.6065 - accuracy: 0.07 - ETA: 4:26 - loss: 1.6094 - accuracy: 0.07 - ETA: 4:25 - loss: 1.6089 - accuracy: 0.07 - ETA: 4:24 - loss: 1.6094 - accuracy: 0.07 - ETA: 4:23 - loss: 1.6064 - accuracy: 0.07 - ETA: 4:21 - loss: 1.6054 - accuracy: 0.07 - ETA: 4:20 - loss: 1.6060 - accuracy: 0.07 - ETA: 4:19 - loss: 1.6082 - accuracy: 0.07 - ETA: 4:18 - loss: 1.6079 - accuracy: 0.07 - ETA: 4:17 - loss: 1.6083 - accuracy: 0.07 - ETA: 4:15 - loss: 1.6090 - accuracy: 0.07 - ETA: 4:14 - loss: 1.6086 - accuracy: 0.07 - ETA: 4:13 - loss: 1.6069 - accuracy: 0.07 - ETA: 4:11 - loss: 1.6071 - accuracy: 0.07 - ETA: 4:10 - loss: 1.6040 - accuracy: 0.07 - ETA: 4:09 - loss: 1.6030 - accuracy: 0.07 - ETA: 4:08 - loss: 1.6026 - accuracy: 0.07 - ETA: 4:07 - loss: 1.6007 - accuracy: 0.07 - ETA: 4:05 - loss: 1.6007 - accuracy: 0.07 - ETA: 4:04 - loss: 1.6015 - accuracy: 0.07 - ETA: 4:03 - loss: 1.5987 - accuracy: 0.07 - ETA: 4:02 - loss: 1.5970 - accuracy: 0.07 - ETA: 4:01 - loss: 1.5978 - accuracy: 0.07 - ETA: 4:00 - loss: 1.5966 - accuracy: 0.07 - ETA: 3:58 - loss: 1.5957 - accuracy: 0.07 - ETA: 3:57 - loss: 1.5971 - accuracy: 0.07 - ETA: 3:56 - loss: 1.5956 - accuracy: 0.07 - ETA: 3:55 - loss: 1.5961 - accuracy: 0.07 - ETA: 3:54 - loss: 1.5977 - accuracy: 0.07 - ETA: 3:52 - loss: 1.5959 - accuracy: 0.07 - ETA: 3:51 - loss: 1.5943 - accuracy: 0.07 - ETA: 3:50 - loss: 1.5953 - accuracy: 0.07 - ETA: 3:49 - loss: 1.5933 - accuracy: 0.07 - ETA: 3:48 - loss: 1.5930 - accuracy: 0.07 - ETA: 3:47 - loss: 1.5945 - accuracy: 0.07 - ETA: 3:45 - loss: 1.5929 - accuracy: 0.07 - ETA: 3:44 - loss: 1.5937 - accuracy: 0.07 - ETA: 3:43 - loss: 1.5914 - accuracy: 0.07 - ETA: 3:42 - loss: 1.5905 - accuracy: 0.07 - ETA: 3:41 - loss: 1.5900 - accuracy: 0.07 - ETA: 3:40 - loss: 1.5870 - accuracy: 0.07 - ETA: 3:39 - loss: 1.5850 - accuracy: 0.07 - ETA: 3:38 - loss: 1.5830 - accuracy: 0.07 - ETA: 3:37 - loss: 1.5836 - accuracy: 0.07 - ETA: 3:36 - loss: 1.5818 - accuracy: 0.07 - ETA: 3:35 - loss: 1.5793 - accuracy: 0.07 - ETA: 3:34 - loss: 1.5791 - accuracy: 0.07 - ETA: 3:32 - loss: 1.5802 - accuracy: 0.07 - ETA: 3:31 - loss: 1.5808 - accuracy: 0.07 - ETA: 3:30 - loss: 1.5798 - accuracy: 0.07 - ETA: 3:29 - loss: 1.5779 - accuracy: 0.07 - ETA: 3:28 - loss: 1.5786 - accuracy: 0.07 - ETA: 3:27 - loss: 1.5769 - accuracy: 0.07 - ETA: 3:26 - loss: 1.5773 - accuracy: 0.07 - ETA: 3:25 - loss: 1.5782 - accuracy: 0.07 - ETA: 3:24 - loss: 1.5781 - accuracy: 0.07 - ETA: 3:23 - loss: 1.5771 - accuracy: 0.07 - ETA: 3:22 - loss: 1.5781 - accuracy: 0.07 - ETA: 3:21 - loss: 1.5794 - accuracy: 0.07 - ETA: 3:20 - loss: 1.5794 - accuracy: 0.07 - ETA: 3:19 - loss: 1.5780 - accuracy: 0.07 - ETA: 3:18 - loss: 1.5771 - accuracy: 0.07 - ETA: 3:17 - loss: 1.5760 - accuracy: 0.07 - ETA: 3:16 - loss: 1.5757 - accuracy: 0.07 - ETA: 3:15 - loss: 1.5754 - accuracy: 0.07 - ETA: 3:14 - loss: 1.5735 - accuracy: 0.07 - ETA: 3:13 - loss: 1.5740 - accuracy: 0.07 - ETA: 3:12 - loss: 1.5730 - accuracy: 0.07 - ETA: 3:11 - loss: 1.5743 - accuracy: 0.07 - ETA: 3:10 - loss: 1.5740 - accuracy: 0.07 - ETA: 3:09 - loss: 1.5719 - accuracy: 0.07 - ETA: 3:08 - loss: 1.5703 - accuracy: 0.07 - ETA: 3:06 - loss: 1.5714 - accuracy: 0.07 - ETA: 3:05 - loss: 1.5723 - accuracy: 0.07 - ETA: 3:04 - loss: 1.5721 - accuracy: 0.07 - ETA: 3:03 - loss: 1.5730 - accuracy: 0.07 - ETA: 3:02 - loss: 1.5745 - accuracy: 0.07 - ETA: 3:01 - loss: 1.5729 - accuracy: 0.07 - ETA: 3:00 - loss: 1.5713 - accuracy: 0.07 - ETA: 2:59 - loss: 1.5710 - accuracy: 0.07 - ETA: 2:58 - loss: 1.5709 - accuracy: 0.07 - ETA: 2:57 - loss: 1.5705 - accuracy: 0.07 - ETA: 2:56 - loss: 1.5703 - accuracy: 0.07 - ETA: 2:55 - loss: 1.5700 - accuracy: 0.07 - ETA: 2:54 - loss: 1.5696 - accuracy: 0.07 - ETA: 2:53 - loss: 1.5682 - accuracy: 0.07 - ETA: 2:52 - loss: 1.5673 - accuracy: 0.07 - ETA: 2:51 - loss: 1.5670 - accuracy: 0.07 - ETA: 2:50 - loss: 1.5673 - accuracy: 0.07 - ETA: 2:49 - loss: 1.5651 - accuracy: 0.07 - ETA: 2:48 - loss: 1.5656 - accuracy: 0.07 - ETA: 2:47 - loss: 1.5654 - accuracy: 0.07 - ETA: 2:46 - loss: 1.5664 - accuracy: 0.07 - ETA: 2:45 - loss: 1.5676 - accuracy: 0.07 - ETA: 2:44 - loss: 1.5676 - accuracy: 0.07 - ETA: 2:43 - loss: 1.5677 - accuracy: 0.07 - ETA: 2:42 - loss: 1.5669 - accuracy: 0.07 - ETA: 2:41 - loss: 1.5666 - accuracy: 0.07 - ETA: 2:40 - loss: 1.5661 - accuracy: 0.07 - ETA: 2:39 - loss: 1.5661 - accuracy: 0.07 - ETA: 2:38 - loss: 1.5658 - accuracy: 0.07 - ETA: 2:37 - loss: 1.5647 - accuracy: 0.07 - ETA: 2:36 - loss: 1.5656 - accuracy: 0.07 - ETA: 2:35 - loss: 1.5651 - accuracy: 0.07 - ETA: 2:34 - loss: 1.5652 - accuracy: 0.07 - ETA: 2:33 - loss: 1.5657 - accuracy: 0.07 - ETA: 2:32 - loss: 1.5652 - accuracy: 0.0750"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - ETA: 2:31 - loss: 1.5649 - accuracy: 0.07 - ETA: 2:30 - loss: 1.5637 - accuracy: 0.07 - ETA: 2:29 - loss: 1.5642 - accuracy: 0.07 - ETA: 2:28 - loss: 1.5637 - accuracy: 0.07 - ETA: 2:27 - loss: 1.5654 - accuracy: 0.07 - ETA: 2:26 - loss: 1.5649 - accuracy: 0.07 - ETA: 2:25 - loss: 1.5655 - accuracy: 0.07 - ETA: 2:24 - loss: 1.5656 - accuracy: 0.07 - ETA: 2:23 - loss: 1.5648 - accuracy: 0.07 - ETA: 2:22 - loss: 1.5646 - accuracy: 0.07 - ETA: 2:21 - loss: 1.5627 - accuracy: 0.07 - ETA: 2:20 - loss: 1.5622 - accuracy: 0.07 - ETA: 2:19 - loss: 1.5627 - accuracy: 0.07 - ETA: 2:18 - loss: 1.5634 - accuracy: 0.07 - ETA: 2:17 - loss: 1.5633 - accuracy: 0.07 - ETA: 2:16 - loss: 1.5632 - accuracy: 0.07 - ETA: 2:15 - loss: 1.5634 - accuracy: 0.07 - ETA: 2:14 - loss: 1.5624 - accuracy: 0.07 - ETA: 2:13 - loss: 1.5635 - accuracy: 0.07 - ETA: 2:12 - loss: 1.5648 - accuracy: 0.07 - ETA: 2:11 - loss: 1.5654 - accuracy: 0.07 - ETA: 2:10 - loss: 1.5656 - accuracy: 0.07 - ETA: 2:09 - loss: 1.5648 - accuracy: 0.07 - ETA: 2:08 - loss: 1.5653 - accuracy: 0.07 - ETA: 2:07 - loss: 1.5650 - accuracy: 0.07 - ETA: 2:06 - loss: 1.5651 - accuracy: 0.07 - ETA: 2:05 - loss: 1.5656 - accuracy: 0.07 - ETA: 2:04 - loss: 1.5657 - accuracy: 0.07 - ETA: 2:03 - loss: 1.5668 - accuracy: 0.07 - ETA: 2:02 - loss: 1.5664 - accuracy: 0.07 - ETA: 2:01 - loss: 1.5659 - accuracy: 0.07 - ETA: 2:00 - loss: 1.5652 - accuracy: 0.07 - ETA: 1:59 - loss: 1.5656 - accuracy: 0.07 - ETA: 1:58 - loss: 1.5652 - accuracy: 0.07 - ETA: 1:57 - loss: 1.5648 - accuracy: 0.07 - ETA: 1:56 - loss: 1.5648 - accuracy: 0.07 - ETA: 1:55 - loss: 1.5639 - accuracy: 0.07 - ETA: 1:54 - loss: 1.5636 - accuracy: 0.07 - ETA: 1:53 - loss: 1.5623 - accuracy: 0.07 - ETA: 1:52 - loss: 1.5617 - accuracy: 0.07 - ETA: 1:51 - loss: 1.5606 - accuracy: 0.07 - ETA: 1:50 - loss: 1.5596 - accuracy: 0.07 - ETA: 1:49 - loss: 1.5584 - accuracy: 0.07 - ETA: 1:48 - loss: 1.5578 - accuracy: 0.07 - ETA: 1:47 - loss: 1.5572 - accuracy: 0.07 - ETA: 1:46 - loss: 1.5561 - accuracy: 0.07 - ETA: 1:45 - loss: 1.5562 - accuracy: 0.07 - ETA: 1:44 - loss: 1.5560 - accuracy: 0.07 - ETA: 1:43 - loss: 1.5567 - accuracy: 0.07 - ETA: 1:42 - loss: 1.5573 - accuracy: 0.07 - ETA: 1:41 - loss: 1.5560 - accuracy: 0.07 - ETA: 1:41 - loss: 1.5552 - accuracy: 0.07 - ETA: 1:40 - loss: 1.5544 - accuracy: 0.07 - ETA: 1:39 - loss: 1.5541 - accuracy: 0.07 - ETA: 1:38 - loss: 1.5544 - accuracy: 0.07 - ETA: 1:37 - loss: 1.5543 - accuracy: 0.07 - ETA: 1:36 - loss: 1.5541 - accuracy: 0.07 - ETA: 1:35 - loss: 1.5529 - accuracy: 0.07 - ETA: 1:34 - loss: 1.5536 - accuracy: 0.07 - ETA: 1:33 - loss: 1.5527 - accuracy: 0.07 - ETA: 1:32 - loss: 1.5527 - accuracy: 0.07 - ETA: 1:31 - loss: 1.5518 - accuracy: 0.07 - ETA: 1:30 - loss: 1.5511 - accuracy: 0.07 - ETA: 1:29 - loss: 1.5502 - accuracy: 0.07 - ETA: 1:28 - loss: 1.5509 - accuracy: 0.07 - ETA: 1:27 - loss: 1.5501 - accuracy: 0.07 - ETA: 1:26 - loss: 1.5502 - accuracy: 0.07 - ETA: 1:25 - loss: 1.5506 - accuracy: 0.07 - ETA: 1:24 - loss: 1.5512 - accuracy: 0.07 - ETA: 1:23 - loss: 1.5511 - accuracy: 0.07 - ETA: 1:22 - loss: 1.5518 - accuracy: 0.07 - ETA: 1:21 - loss: 1.5515 - accuracy: 0.07 - ETA: 1:20 - loss: 1.5517 - accuracy: 0.07 - ETA: 1:19 - loss: 1.5514 - accuracy: 0.07 - ETA: 1:18 - loss: 1.5516 - accuracy: 0.07 - ETA: 1:18 - loss: 1.5516 - accuracy: 0.07 - ETA: 1:17 - loss: 1.5510 - accuracy: 0.07 - ETA: 1:16 - loss: 1.5508 - accuracy: 0.07 - ETA: 1:15 - loss: 1.5515 - accuracy: 0.07 - ETA: 1:14 - loss: 1.5503 - accuracy: 0.07 - ETA: 1:13 - loss: 1.5490 - accuracy: 0.07 - ETA: 1:12 - loss: 1.5480 - accuracy: 0.07 - ETA: 1:11 - loss: 1.5488 - accuracy: 0.07 - ETA: 1:10 - loss: 1.5477 - accuracy: 0.07 - ETA: 1:09 - loss: 1.5480 - accuracy: 0.07 - ETA: 1:08 - loss: 1.5484 - accuracy: 0.07 - ETA: 1:07 - loss: 1.5472 - accuracy: 0.07 - ETA: 1:06 - loss: 1.5470 - accuracy: 0.07 - ETA: 1:05 - loss: 1.5474 - accuracy: 0.07 - ETA: 1:04 - loss: 1.5483 - accuracy: 0.07 - ETA: 1:03 - loss: 1.5484 - accuracy: 0.07 - ETA: 1:02 - loss: 1.5488 - accuracy: 0.07 - ETA: 1:01 - loss: 1.5497 - accuracy: 0.07 - ETA: 1:00 - loss: 1.5487 - accuracy: 0.07 - ETA: 59s - loss: 1.5491 - accuracy: 0.0763 - ETA: 59s - loss: 1.5485 - accuracy: 0.076 - ETA: 58s - loss: 1.5490 - accuracy: 0.076 - ETA: 57s - loss: 1.5489 - accuracy: 0.076 - ETA: 56s - loss: 1.5490 - accuracy: 0.076 - ETA: 55s - loss: 1.5490 - accuracy: 0.076 - ETA: 54s - loss: 1.5489 - accuracy: 0.076 - ETA: 53s - loss: 1.5487 - accuracy: 0.076 - ETA: 52s - loss: 1.5491 - accuracy: 0.076 - ETA: 51s - loss: 1.5484 - accuracy: 0.076 - ETA: 50s - loss: 1.5487 - accuracy: 0.076 - ETA: 49s - loss: 1.5489 - accuracy: 0.076 - ETA: 48s - loss: 1.5508 - accuracy: 0.076 - ETA: 47s - loss: 1.5515 - accuracy: 0.076 - ETA: 46s - loss: 1.5503 - accuracy: 0.076 - ETA: 45s - loss: 1.5503 - accuracy: 0.076 - ETA: 44s - loss: 1.5493 - accuracy: 0.076 - ETA: 43s - loss: 1.5493 - accuracy: 0.076 - ETA: 43s - loss: 1.5491 - accuracy: 0.076 - ETA: 42s - loss: 1.5490 - accuracy: 0.076 - ETA: 41s - loss: 1.5492 - accuracy: 0.076 - ETA: 40s - loss: 1.5497 - accuracy: 0.076 - ETA: 39s - loss: 1.5486 - accuracy: 0.076 - ETA: 38s - loss: 1.5484 - accuracy: 0.076 - ETA: 37s - loss: 1.5473 - accuracy: 0.076 - ETA: 36s - loss: 1.5468 - accuracy: 0.076 - ETA: 35s - loss: 1.5465 - accuracy: 0.076 - ETA: 34s - loss: 1.5472 - accuracy: 0.076 - ETA: 33s - loss: 1.5475 - accuracy: 0.076 - ETA: 32s - loss: 1.5470 - accuracy: 0.076 - ETA: 31s - loss: 1.5473 - accuracy: 0.076 - ETA: 30s - loss: 1.5480 - accuracy: 0.076 - ETA: 29s - loss: 1.5483 - accuracy: 0.076 - ETA: 28s - loss: 1.5483 - accuracy: 0.076 - ETA: 27s - loss: 1.5489 - accuracy: 0.076 - ETA: 27s - loss: 1.5494 - accuracy: 0.076 - ETA: 26s - loss: 1.5487 - accuracy: 0.076 - ETA: 25s - loss: 1.5491 - accuracy: 0.076 - ETA: 24s - loss: 1.5487 - accuracy: 0.076 - ETA: 23s - loss: 1.5485 - accuracy: 0.076 - ETA: 22s - loss: 1.5478 - accuracy: 0.076 - ETA: 21s - loss: 1.5469 - accuracy: 0.076 - ETA: 20s - loss: 1.5469 - accuracy: 0.076 - ETA: 19s - loss: 1.5466 - accuracy: 0.076 - ETA: 18s - loss: 1.5466 - accuracy: 0.076 - ETA: 17s - loss: 1.5466 - accuracy: 0.076 - ETA: 16s - loss: 1.5459 - accuracy: 0.076 - ETA: 15s - loss: 1.5452 - accuracy: 0.076 - ETA: 14s - loss: 1.5443 - accuracy: 0.076 - ETA: 13s - loss: 1.5436 - accuracy: 0.076 - ETA: 13s - loss: 1.5436 - accuracy: 0.076 - ETA: 12s - loss: 1.5432 - accuracy: 0.076 - ETA: 11s - loss: 1.5432 - accuracy: 0.076 - ETA: 10s - loss: 1.5440 - accuracy: 0.076 - ETA: 9s - loss: 1.5434 - accuracy: 0.076 - ETA: 8s - loss: 1.5429 - accuracy: 0.07 - ETA: 7s - loss: 1.5436 - accuracy: 0.07 - ETA: 6s - loss: 1.5431 - accuracy: 0.07 - ETA: 5s - loss: 1.5431 - accuracy: 0.07 - ETA: 4s - loss: 1.5431 - accuracy: 0.07 - ETA: 3s - loss: 1.5439 - accuracy: 0.07 - ETA: 2s - loss: 1.5440 - accuracy: 0.07 - ETA: 1s - loss: 1.5438 - accuracy: 0.07 - ETA: 0s - loss: 1.5439 - accuracy: 0.07 - 320s 929ms/step - loss: 1.5439 - accuracy: 0.0768\n",
      "Epoch 4/20\n",
      "186/345 [===============>..............] - ETA: 4:57 - loss: 1.6482 - accuracy: 0.09 - ETA: 4:58 - loss: 1.4579 - accuracy: 0.08 - ETA: 4:58 - loss: 1.4197 - accuracy: 0.08 - ETA: 5:03 - loss: 1.4145 - accuracy: 0.08 - ETA: 5:04 - loss: 1.4742 - accuracy: 0.08 - ETA: 5:02 - loss: 1.5049 - accuracy: 0.08 - ETA: 5:00 - loss: 1.5157 - accuracy: 0.08 - ETA: 4:58 - loss: 1.4997 - accuracy: 0.08 - ETA: 4:56 - loss: 1.4842 - accuracy: 0.08 - ETA: 4:57 - loss: 1.4746 - accuracy: 0.08 - ETA: 4:57 - loss: 1.4601 - accuracy: 0.08 - ETA: 4:55 - loss: 1.4640 - accuracy: 0.08 - ETA: 4:54 - loss: 1.4758 - accuracy: 0.08 - ETA: 4:54 - loss: 1.4790 - accuracy: 0.08 - ETA: 4:54 - loss: 1.4783 - accuracy: 0.08 - ETA: 4:53 - loss: 1.4828 - accuracy: 0.08 - ETA: 4:52 - loss: 1.4774 - accuracy: 0.08 - ETA: 4:51 - loss: 1.4686 - accuracy: 0.08 - ETA: 4:49 - loss: 1.4734 - accuracy: 0.08 - ETA: 4:48 - loss: 1.4610 - accuracy: 0.08 - ETA: 4:48 - loss: 1.4642 - accuracy: 0.08 - ETA: 4:48 - loss: 1.4618 - accuracy: 0.08 - ETA: 4:46 - loss: 1.4710 - accuracy: 0.08 - ETA: 4:46 - loss: 1.4660 - accuracy: 0.08 - ETA: 4:45 - loss: 1.4743 - accuracy: 0.08 - ETA: 4:44 - loss: 1.4762 - accuracy: 0.08 - ETA: 4:43 - loss: 1.4870 - accuracy: 0.08 - ETA: 4:43 - loss: 1.4890 - accuracy: 0.08 - ETA: 4:42 - loss: 1.4838 - accuracy: 0.08 - ETA: 4:41 - loss: 1.4910 - accuracy: 0.08 - ETA: 4:41 - loss: 1.4945 - accuracy: 0.08 - ETA: 4:39 - loss: 1.4913 - accuracy: 0.08 - ETA: 4:38 - loss: 1.4923 - accuracy: 0.08 - ETA: 4:37 - loss: 1.4934 - accuracy: 0.08 - ETA: 4:36 - loss: 1.4947 - accuracy: 0.08 - ETA: 4:36 - loss: 1.4963 - accuracy: 0.08 - ETA: 4:35 - loss: 1.4925 - accuracy: 0.08 - ETA: 4:34 - loss: 1.4937 - accuracy: 0.08 - ETA: 4:32 - loss: 1.4949 - accuracy: 0.08 - ETA: 4:32 - loss: 1.4940 - accuracy: 0.08 - ETA: 4:31 - loss: 1.4964 - accuracy: 0.08 - ETA: 4:30 - loss: 1.4952 - accuracy: 0.08 - ETA: 4:29 - loss: 1.4974 - accuracy: 0.08 - ETA: 4:28 - loss: 1.4962 - accuracy: 0.08 - ETA: 4:27 - loss: 1.4959 - accuracy: 0.08 - ETA: 4:27 - loss: 1.5008 - accuracy: 0.08 - ETA: 4:26 - loss: 1.5076 - accuracy: 0.08 - ETA: 4:25 - loss: 1.5139 - accuracy: 0.08 - ETA: 4:24 - loss: 1.5088 - accuracy: 0.08 - ETA: 4:23 - loss: 1.5090 - accuracy: 0.08 - ETA: 4:22 - loss: 1.5088 - accuracy: 0.08 - ETA: 4:21 - loss: 1.5090 - accuracy: 0.08 - ETA: 4:20 - loss: 1.5121 - accuracy: 0.08 - ETA: 4:19 - loss: 1.5098 - accuracy: 0.08 - ETA: 4:18 - loss: 1.5070 - accuracy: 0.08 - ETA: 4:18 - loss: 1.5102 - accuracy: 0.08 - ETA: 4:17 - loss: 1.5093 - accuracy: 0.08 - ETA: 4:16 - loss: 1.5040 - accuracy: 0.08 - ETA: 4:15 - loss: 1.5046 - accuracy: 0.08 - ETA: 4:14 - loss: 1.5036 - accuracy: 0.08 - ETA: 4:13 - loss: 1.5049 - accuracy: 0.08 - ETA: 4:12 - loss: 1.5053 - accuracy: 0.08 - ETA: 4:11 - loss: 1.5063 - accuracy: 0.08 - ETA: 4:11 - loss: 1.5058 - accuracy: 0.08 - ETA: 4:10 - loss: 1.5061 - accuracy: 0.08 - ETA: 4:09 - loss: 1.5056 - accuracy: 0.08 - ETA: 4:08 - loss: 1.5037 - accuracy: 0.08 - ETA: 4:07 - loss: 1.5035 - accuracy: 0.08 - ETA: 4:06 - loss: 1.5027 - accuracy: 0.08 - ETA: 4:05 - loss: 1.5000 - accuracy: 0.08 - ETA: 4:04 - loss: 1.4981 - accuracy: 0.08 - ETA: 4:04 - loss: 1.4979 - accuracy: 0.08 - ETA: 4:03 - loss: 1.4961 - accuracy: 0.08 - ETA: 4:02 - loss: 1.4948 - accuracy: 0.08 - ETA: 4:01 - loss: 1.4925 - accuracy: 0.08 - ETA: 4:00 - loss: 1.4932 - accuracy: 0.08 - ETA: 3:59 - loss: 1.4913 - accuracy: 0.08 - ETA: 3:58 - loss: 1.4927 - accuracy: 0.08 - ETA: 3:57 - loss: 1.4909 - accuracy: 0.08 - ETA: 3:56 - loss: 1.4887 - accuracy: 0.08 - ETA: 3:56 - loss: 1.4891 - accuracy: 0.08 - ETA: 3:55 - loss: 1.4880 - accuracy: 0.08 - ETA: 3:54 - loss: 1.4878 - accuracy: 0.08 - ETA: 3:53 - loss: 1.4859 - accuracy: 0.08 - ETA: 3:52 - loss: 1.4855 - accuracy: 0.08 - ETA: 3:51 - loss: 1.4869 - accuracy: 0.08 - ETA: 3:50 - loss: 1.4841 - accuracy: 0.08 - ETA: 3:49 - loss: 1.4827 - accuracy: 0.08 - ETA: 3:48 - loss: 1.4863 - accuracy: 0.08 - ETA: 3:47 - loss: 1.4863 - accuracy: 0.08 - ETA: 3:46 - loss: 1.4872 - accuracy: 0.08 - ETA: 3:46 - loss: 1.4887 - accuracy: 0.08 - ETA: 3:45 - loss: 1.4879 - accuracy: 0.08 - ETA: 3:44 - loss: 1.4857 - accuracy: 0.08 - ETA: 3:43 - loss: 1.4842 - accuracy: 0.08 - ETA: 3:42 - loss: 1.4829 - accuracy: 0.08 - ETA: 3:41 - loss: 1.4823 - accuracy: 0.08 - ETA: 3:40 - loss: 1.4813 - accuracy: 0.08 - ETA: 3:39 - loss: 1.4809 - accuracy: 0.08 - ETA: 3:38 - loss: 1.4809 - accuracy: 0.08 - ETA: 3:37 - loss: 1.4809 - accuracy: 0.08 - ETA: 3:37 - loss: 1.4810 - accuracy: 0.08 - ETA: 3:36 - loss: 1.4808 - accuracy: 0.08 - ETA: 3:35 - loss: 1.4767 - accuracy: 0.08 - ETA: 3:34 - loss: 1.4755 - accuracy: 0.08 - ETA: 3:33 - loss: 1.4754 - accuracy: 0.08 - ETA: 3:32 - loss: 1.4771 - accuracy: 0.08 - ETA: 3:31 - loss: 1.4769 - accuracy: 0.08 - ETA: 3:30 - loss: 1.4765 - accuracy: 0.08 - ETA: 3:29 - loss: 1.4774 - accuracy: 0.08 - ETA: 3:29 - loss: 1.4749 - accuracy: 0.08 - ETA: 3:28 - loss: 1.4773 - accuracy: 0.08 - ETA: 3:27 - loss: 1.4813 - accuracy: 0.08 - ETA: 3:26 - loss: 1.4797 - accuracy: 0.08 - ETA: 3:25 - loss: 1.4788 - accuracy: 0.08 - ETA: 3:24 - loss: 1.4793 - accuracy: 0.08 - ETA: 3:23 - loss: 1.4781 - accuracy: 0.08 - ETA: 3:22 - loss: 1.4761 - accuracy: 0.08 - ETA: 3:21 - loss: 1.4775 - accuracy: 0.08 - ETA: 3:20 - loss: 1.4761 - accuracy: 0.08 - ETA: 3:19 - loss: 1.4767 - accuracy: 0.08 - ETA: 3:18 - loss: 1.4757 - accuracy: 0.08 - ETA: 3:18 - loss: 1.4753 - accuracy: 0.08 - ETA: 3:17 - loss: 1.4774 - accuracy: 0.08 - ETA: 3:16 - loss: 1.4781 - accuracy: 0.08 - ETA: 3:15 - loss: 1.4776 - accuracy: 0.08 - ETA: 3:14 - loss: 1.4766 - accuracy: 0.08 - ETA: 3:13 - loss: 1.4764 - accuracy: 0.08 - ETA: 3:12 - loss: 1.4752 - accuracy: 0.08 - ETA: 3:11 - loss: 1.4749 - accuracy: 0.08 - ETA: 3:10 - loss: 1.4724 - accuracy: 0.08 - ETA: 3:10 - loss: 1.4739 - accuracy: 0.08 - ETA: 3:09 - loss: 1.4757 - accuracy: 0.08 - ETA: 3:08 - loss: 1.4757 - accuracy: 0.08 - ETA: 3:07 - loss: 1.4758 - accuracy: 0.08 - ETA: 3:06 - loss: 1.4749 - accuracy: 0.08 - ETA: 3:05 - loss: 1.4743 - accuracy: 0.08 - ETA: 3:04 - loss: 1.4751 - accuracy: 0.08 - ETA: 3:03 - loss: 1.4745 - accuracy: 0.08 - ETA: 3:02 - loss: 1.4737 - accuracy: 0.08 - ETA: 3:01 - loss: 1.4745 - accuracy: 0.08 - ETA: 3:01 - loss: 1.4749 - accuracy: 0.08 - ETA: 3:00 - loss: 1.4742 - accuracy: 0.08 - ETA: 2:59 - loss: 1.4740 - accuracy: 0.08 - ETA: 2:58 - loss: 1.4748 - accuracy: 0.08 - ETA: 2:57 - loss: 1.4759 - accuracy: 0.08 - ETA: 2:56 - loss: 1.4766 - accuracy: 0.08 - ETA: 2:55 - loss: 1.4765 - accuracy: 0.08 - ETA: 2:54 - loss: 1.4762 - accuracy: 0.08 - ETA: 2:54 - loss: 1.4759 - accuracy: 0.08 - ETA: 2:53 - loss: 1.4752 - accuracy: 0.08 - ETA: 2:52 - loss: 1.4753 - accuracy: 0.08 - ETA: 2:51 - loss: 1.4756 - accuracy: 0.08 - ETA: 2:50 - loss: 1.4740 - accuracy: 0.08 - ETA: 2:49 - loss: 1.4734 - accuracy: 0.08 - ETA: 2:48 - loss: 1.4745 - accuracy: 0.08 - ETA: 2:47 - loss: 1.4737 - accuracy: 0.08 - ETA: 2:46 - loss: 1.4735 - accuracy: 0.08 - ETA: 2:45 - loss: 1.4743 - accuracy: 0.08 - ETA: 2:45 - loss: 1.4739 - accuracy: 0.08 - ETA: 2:44 - loss: 1.4759 - accuracy: 0.08 - ETA: 2:43 - loss: 1.4749 - accuracy: 0.08 - ETA: 2:42 - loss: 1.4758 - accuracy: 0.08 - ETA: 2:41 - loss: 1.4745 - accuracy: 0.08 - ETA: 2:40 - loss: 1.4742 - accuracy: 0.08 - ETA: 2:39 - loss: 1.4734 - accuracy: 0.08 - ETA: 2:38 - loss: 1.4726 - accuracy: 0.08 - ETA: 2:37 - loss: 1.4714 - accuracy: 0.08 - ETA: 2:37 - loss: 1.4702 - accuracy: 0.08 - ETA: 2:36 - loss: 1.4708 - accuracy: 0.08 - ETA: 2:35 - loss: 1.4721 - accuracy: 0.08 - ETA: 2:34 - loss: 1.4717 - accuracy: 0.08 - ETA: 2:33 - loss: 1.4724 - accuracy: 0.08 - ETA: 2:32 - loss: 1.4725 - accuracy: 0.08 - ETA: 2:31 - loss: 1.4720 - accuracy: 0.08 - ETA: 2:30 - loss: 1.4727 - accuracy: 0.08 - ETA: 2:30 - loss: 1.4722 - accuracy: 0.08 - ETA: 2:29 - loss: 1.4718 - accuracy: 0.08 - ETA: 2:28 - loss: 1.4731 - accuracy: 0.08 - ETA: 2:27 - loss: 1.4722 - accuracy: 0.08 - ETA: 2:26 - loss: 1.4722 - accuracy: 0.08 - ETA: 2:25 - loss: 1.4725 - accuracy: 0.08 - ETA: 2:24 - loss: 1.4732 - accuracy: 0.08 - ETA: 2:23 - loss: 1.4730 - accuracy: 0.08 - ETA: 2:23 - loss: 1.4726 - accuracy: 0.08 - ETA: 2:22 - loss: 1.4721 - accuracy: 0.0823345/345 [==============================] - ETA: 2:21 - loss: 1.4733 - accuracy: 0.08 - ETA: 2:20 - loss: 1.4723 - accuracy: 0.08 - ETA: 2:19 - loss: 1.4732 - accuracy: 0.08 - ETA: 2:18 - loss: 1.4734 - accuracy: 0.08 - ETA: 2:17 - loss: 1.4746 - accuracy: 0.08 - ETA: 2:16 - loss: 1.4742 - accuracy: 0.08 - ETA: 2:15 - loss: 1.4740 - accuracy: 0.08 - ETA: 2:14 - loss: 1.4727 - accuracy: 0.08 - ETA: 2:14 - loss: 1.4726 - accuracy: 0.08 - ETA: 2:13 - loss: 1.4737 - accuracy: 0.08 - ETA: 2:12 - loss: 1.4725 - accuracy: 0.08 - ETA: 2:11 - loss: 1.4727 - accuracy: 0.08 - ETA: 2:10 - loss: 1.4719 - accuracy: 0.08 - ETA: 2:09 - loss: 1.4720 - accuracy: 0.08 - ETA: 2:08 - loss: 1.4716 - accuracy: 0.08 - ETA: 2:07 - loss: 1.4704 - accuracy: 0.08 - ETA: 2:06 - loss: 1.4700 - accuracy: 0.08 - ETA: 2:06 - loss: 1.4698 - accuracy: 0.08 - ETA: 2:05 - loss: 1.4697 - accuracy: 0.08 - ETA: 2:04 - loss: 1.4684 - accuracy: 0.08 - ETA: 2:03 - loss: 1.4689 - accuracy: 0.08 - ETA: 2:02 - loss: 1.4692 - accuracy: 0.08 - ETA: 2:01 - loss: 1.4687 - accuracy: 0.08 - ETA: 2:00 - loss: 1.4696 - accuracy: 0.08 - ETA: 1:59 - loss: 1.4692 - accuracy: 0.08 - ETA: 1:58 - loss: 1.4697 - accuracy: 0.08 - ETA: 1:57 - loss: 1.4696 - accuracy: 0.08 - ETA: 1:57 - loss: 1.4691 - accuracy: 0.08 - ETA: 1:56 - loss: 1.4688 - accuracy: 0.08 - ETA: 1:55 - loss: 1.4689 - accuracy: 0.08 - ETA: 1:54 - loss: 1.4698 - accuracy: 0.08 - ETA: 1:53 - loss: 1.4685 - accuracy: 0.08 - ETA: 1:52 - loss: 1.4680 - accuracy: 0.08 - ETA: 1:51 - loss: 1.4681 - accuracy: 0.08 - ETA: 1:50 - loss: 1.4680 - accuracy: 0.08 - ETA: 1:49 - loss: 1.4676 - accuracy: 0.08 - ETA: 1:49 - loss: 1.4690 - accuracy: 0.08 - ETA: 1:48 - loss: 1.4692 - accuracy: 0.08 - ETA: 1:47 - loss: 1.4692 - accuracy: 0.08 - ETA: 1:46 - loss: 1.4689 - accuracy: 0.08 - ETA: 1:45 - loss: 1.4698 - accuracy: 0.08 - ETA: 1:44 - loss: 1.4707 - accuracy: 0.08 - ETA: 1:43 - loss: 1.4704 - accuracy: 0.08 - ETA: 1:42 - loss: 1.4693 - accuracy: 0.08 - ETA: 1:41 - loss: 1.4690 - accuracy: 0.08 - ETA: 1:41 - loss: 1.4682 - accuracy: 0.08 - ETA: 1:40 - loss: 1.4675 - accuracy: 0.08 - ETA: 1:39 - loss: 1.4684 - accuracy: 0.08 - ETA: 1:38 - loss: 1.4681 - accuracy: 0.08 - ETA: 1:37 - loss: 1.4671 - accuracy: 0.08 - ETA: 1:36 - loss: 1.4680 - accuracy: 0.08 - ETA: 1:35 - loss: 1.4694 - accuracy: 0.08 - ETA: 1:34 - loss: 1.4695 - accuracy: 0.08 - ETA: 1:33 - loss: 1.4681 - accuracy: 0.08 - ETA: 1:33 - loss: 1.4679 - accuracy: 0.08 - ETA: 1:32 - loss: 1.4691 - accuracy: 0.08 - ETA: 1:31 - loss: 1.4701 - accuracy: 0.08 - ETA: 1:30 - loss: 1.4700 - accuracy: 0.08 - ETA: 1:29 - loss: 1.4694 - accuracy: 0.08 - ETA: 1:28 - loss: 1.4689 - accuracy: 0.08 - ETA: 1:27 - loss: 1.4680 - accuracy: 0.08 - ETA: 1:26 - loss: 1.4698 - accuracy: 0.08 - ETA: 1:25 - loss: 1.4702 - accuracy: 0.08 - ETA: 1:24 - loss: 1.4695 - accuracy: 0.08 - ETA: 1:24 - loss: 1.4696 - accuracy: 0.08 - ETA: 1:23 - loss: 1.4690 - accuracy: 0.08 - ETA: 1:22 - loss: 1.4693 - accuracy: 0.08 - ETA: 1:21 - loss: 1.4686 - accuracy: 0.08 - ETA: 1:20 - loss: 1.4682 - accuracy: 0.08 - ETA: 1:19 - loss: 1.4679 - accuracy: 0.08 - ETA: 1:18 - loss: 1.4671 - accuracy: 0.08 - ETA: 1:17 - loss: 1.4669 - accuracy: 0.08 - ETA: 1:16 - loss: 1.4674 - accuracy: 0.08 - ETA: 1:16 - loss: 1.4672 - accuracy: 0.08 - ETA: 1:15 - loss: 1.4664 - accuracy: 0.08 - ETA: 1:14 - loss: 1.4674 - accuracy: 0.08 - ETA: 1:13 - loss: 1.4668 - accuracy: 0.08 - ETA: 1:12 - loss: 1.4667 - accuracy: 0.08 - ETA: 1:11 - loss: 1.4672 - accuracy: 0.08 - ETA: 1:10 - loss: 1.4669 - accuracy: 0.08 - ETA: 1:09 - loss: 1.4661 - accuracy: 0.08 - ETA: 1:08 - loss: 1.4662 - accuracy: 0.08 - ETA: 1:08 - loss: 1.4654 - accuracy: 0.08 - ETA: 1:07 - loss: 1.4653 - accuracy: 0.08 - ETA: 1:06 - loss: 1.4655 - accuracy: 0.08 - ETA: 1:05 - loss: 1.4649 - accuracy: 0.08 - ETA: 1:04 - loss: 1.4656 - accuracy: 0.08 - ETA: 1:03 - loss: 1.4657 - accuracy: 0.08 - ETA: 1:02 - loss: 1.4661 - accuracy: 0.08 - ETA: 1:01 - loss: 1.4656 - accuracy: 0.08 - ETA: 1:00 - loss: 1.4650 - accuracy: 0.08 - ETA: 59s - loss: 1.4646 - accuracy: 0.0825 - ETA: 59s - loss: 1.4651 - accuracy: 0.082 - ETA: 58s - loss: 1.4652 - accuracy: 0.082 - ETA: 57s - loss: 1.4649 - accuracy: 0.082 - ETA: 56s - loss: 1.4647 - accuracy: 0.082 - ETA: 55s - loss: 1.4640 - accuracy: 0.082 - ETA: 54s - loss: 1.4634 - accuracy: 0.082 - ETA: 53s - loss: 1.4636 - accuracy: 0.082 - ETA: 52s - loss: 1.4632 - accuracy: 0.082 - ETA: 51s - loss: 1.4635 - accuracy: 0.082 - ETA: 51s - loss: 1.4636 - accuracy: 0.082 - ETA: 50s - loss: 1.4632 - accuracy: 0.082 - ETA: 49s - loss: 1.4633 - accuracy: 0.082 - ETA: 48s - loss: 1.4637 - accuracy: 0.082 - ETA: 47s - loss: 1.4642 - accuracy: 0.082 - ETA: 46s - loss: 1.4637 - accuracy: 0.082 - ETA: 45s - loss: 1.4634 - accuracy: 0.082 - ETA: 44s - loss: 1.4623 - accuracy: 0.082 - ETA: 43s - loss: 1.4621 - accuracy: 0.082 - ETA: 42s - loss: 1.4625 - accuracy: 0.082 - ETA: 42s - loss: 1.4620 - accuracy: 0.082 - ETA: 41s - loss: 1.4618 - accuracy: 0.082 - ETA: 40s - loss: 1.4625 - accuracy: 0.082 - ETA: 39s - loss: 1.4629 - accuracy: 0.082 - ETA: 38s - loss: 1.4617 - accuracy: 0.082 - ETA: 37s - loss: 1.4611 - accuracy: 0.082 - ETA: 36s - loss: 1.4621 - accuracy: 0.082 - ETA: 35s - loss: 1.4617 - accuracy: 0.082 - ETA: 34s - loss: 1.4617 - accuracy: 0.082 - ETA: 33s - loss: 1.4621 - accuracy: 0.082 - ETA: 33s - loss: 1.4618 - accuracy: 0.082 - ETA: 32s - loss: 1.4622 - accuracy: 0.082 - ETA: 31s - loss: 1.4627 - accuracy: 0.082 - ETA: 30s - loss: 1.4621 - accuracy: 0.082 - ETA: 29s - loss: 1.4618 - accuracy: 0.082 - ETA: 28s - loss: 1.4615 - accuracy: 0.082 - ETA: 27s - loss: 1.4609 - accuracy: 0.082 - ETA: 26s - loss: 1.4606 - accuracy: 0.082 - ETA: 25s - loss: 1.4598 - accuracy: 0.082 - ETA: 25s - loss: 1.4598 - accuracy: 0.082 - ETA: 24s - loss: 1.4591 - accuracy: 0.082 - ETA: 23s - loss: 1.4593 - accuracy: 0.082 - ETA: 22s - loss: 1.4594 - accuracy: 0.082 - ETA: 21s - loss: 1.4583 - accuracy: 0.082 - ETA: 20s - loss: 1.4585 - accuracy: 0.082 - ETA: 19s - loss: 1.4591 - accuracy: 0.082 - ETA: 18s - loss: 1.4604 - accuracy: 0.082 - ETA: 17s - loss: 1.4610 - accuracy: 0.082 - ETA: 17s - loss: 1.4607 - accuracy: 0.082 - ETA: 16s - loss: 1.4605 - accuracy: 0.082 - ETA: 15s - loss: 1.4610 - accuracy: 0.082 - ETA: 14s - loss: 1.4605 - accuracy: 0.082 - ETA: 13s - loss: 1.4598 - accuracy: 0.082 - ETA: 12s - loss: 1.4596 - accuracy: 0.082 - ETA: 11s - loss: 1.4599 - accuracy: 0.082 - ETA: 10s - loss: 1.4595 - accuracy: 0.082 - ETA: 9s - loss: 1.4597 - accuracy: 0.082 - ETA: 8s - loss: 1.4595 - accuracy: 0.08 - ETA: 8s - loss: 1.4587 - accuracy: 0.08 - ETA: 7s - loss: 1.4598 - accuracy: 0.08 - ETA: 6s - loss: 1.4600 - accuracy: 0.08 - ETA: 5s - loss: 1.4589 - accuracy: 0.08 - ETA: 4s - loss: 1.4579 - accuracy: 0.08 - ETA: 3s - loss: 1.4580 - accuracy: 0.08 - ETA: 2s - loss: 1.4574 - accuracy: 0.08 - ETA: 1s - loss: 1.4573 - accuracy: 0.08 - ETA: 0s - loss: 1.4565 - accuracy: 0.08 - 309s 895ms/step - loss: 1.4565 - accuracy: 0.0825\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/345 [===============>..............] - ETA: 5:05 - loss: 1.3726 - accuracy: 0.08 - ETA: 5:02 - loss: 1.5365 - accuracy: 0.09 - ETA: 5:04 - loss: 1.4576 - accuracy: 0.08 - ETA: 5:07 - loss: 1.4209 - accuracy: 0.08 - ETA: 5:10 - loss: 1.4184 - accuracy: 0.08 - ETA: 5:08 - loss: 1.4066 - accuracy: 0.08 - ETA: 5:04 - loss: 1.3807 - accuracy: 0.08 - ETA: 5:01 - loss: 1.3906 - accuracy: 0.08 - ETA: 5:03 - loss: 1.3788 - accuracy: 0.08 - ETA: 5:03 - loss: 1.3758 - accuracy: 0.08 - ETA: 5:00 - loss: 1.3407 - accuracy: 0.08 - ETA: 4:58 - loss: 1.3422 - accuracy: 0.08 - ETA: 4:57 - loss: 1.3440 - accuracy: 0.08 - ETA: 4:57 - loss: 1.3517 - accuracy: 0.08 - ETA: 4:56 - loss: 1.3568 - accuracy: 0.08 - ETA: 4:54 - loss: 1.3536 - accuracy: 0.08 - ETA: 4:53 - loss: 1.3550 - accuracy: 0.08 - ETA: 4:52 - loss: 1.3591 - accuracy: 0.08 - ETA: 4:52 - loss: 1.3665 - accuracy: 0.08 - ETA: 4:50 - loss: 1.3688 - accuracy: 0.08 - ETA: 4:50 - loss: 1.3650 - accuracy: 0.08 - ETA: 4:49 - loss: 1.3707 - accuracy: 0.08 - ETA: 4:48 - loss: 1.3742 - accuracy: 0.08 - ETA: 4:47 - loss: 1.3798 - accuracy: 0.08 - ETA: 4:46 - loss: 1.3771 - accuracy: 0.08 - ETA: 4:44 - loss: 1.3751 - accuracy: 0.08 - ETA: 4:43 - loss: 1.3743 - accuracy: 0.08 - ETA: 4:43 - loss: 1.3792 - accuracy: 0.08 - ETA: 4:42 - loss: 1.3743 - accuracy: 0.08 - ETA: 4:41 - loss: 1.3809 - accuracy: 0.08 - ETA: 4:40 - loss: 1.3734 - accuracy: 0.08 - ETA: 4:39 - loss: 1.3733 - accuracy: 0.08 - ETA: 4:38 - loss: 1.3803 - accuracy: 0.08 - ETA: 4:38 - loss: 1.3751 - accuracy: 0.08 - ETA: 4:37 - loss: 1.3724 - accuracy: 0.08 - ETA: 4:36 - loss: 1.3791 - accuracy: 0.08 - ETA: 4:34 - loss: 1.3823 - accuracy: 0.08 - ETA: 4:34 - loss: 1.3868 - accuracy: 0.08 - ETA: 4:33 - loss: 1.3907 - accuracy: 0.08 - ETA: 4:32 - loss: 1.3876 - accuracy: 0.08 - ETA: 4:31 - loss: 1.3853 - accuracy: 0.08 - ETA: 4:30 - loss: 1.3829 - accuracy: 0.08 - ETA: 4:30 - loss: 1.3830 - accuracy: 0.08 - ETA: 4:29 - loss: 1.3848 - accuracy: 0.08 - ETA: 4:28 - loss: 1.3872 - accuracy: 0.08 - ETA: 4:26 - loss: 1.3842 - accuracy: 0.08 - ETA: 4:26 - loss: 1.3818 - accuracy: 0.08 - ETA: 4:25 - loss: 1.3853 - accuracy: 0.08 - ETA: 4:24 - loss: 1.3843 - accuracy: 0.08 - ETA: 4:23 - loss: 1.3872 - accuracy: 0.08 - ETA: 4:22 - loss: 1.3921 - accuracy: 0.08 - ETA: 4:22 - loss: 1.3909 - accuracy: 0.08 - ETA: 4:21 - loss: 1.3886 - accuracy: 0.08 - ETA: 4:20 - loss: 1.3909 - accuracy: 0.08 - ETA: 4:19 - loss: 1.3886 - accuracy: 0.08 - ETA: 4:18 - loss: 1.3876 - accuracy: 0.08 - ETA: 4:17 - loss: 1.3892 - accuracy: 0.08 - ETA: 4:16 - loss: 1.3924 - accuracy: 0.08 - ETA: 4:15 - loss: 1.3936 - accuracy: 0.08 - ETA: 4:14 - loss: 1.3932 - accuracy: 0.08 - ETA: 4:13 - loss: 1.3930 - accuracy: 0.08 - ETA: 4:12 - loss: 1.3941 - accuracy: 0.08 - ETA: 4:12 - loss: 1.3948 - accuracy: 0.08 - ETA: 4:11 - loss: 1.3975 - accuracy: 0.08 - ETA: 4:10 - loss: 1.3972 - accuracy: 0.08 - ETA: 4:09 - loss: 1.4002 - accuracy: 0.08 - ETA: 4:08 - loss: 1.4016 - accuracy: 0.08 - ETA: 4:07 - loss: 1.3982 - accuracy: 0.08 - ETA: 4:07 - loss: 1.3990 - accuracy: 0.08 - ETA: 4:06 - loss: 1.3980 - accuracy: 0.08 - ETA: 4:05 - loss: 1.3983 - accuracy: 0.08 - ETA: 4:04 - loss: 1.3953 - accuracy: 0.08 - ETA: 4:03 - loss: 1.3932 - accuracy: 0.08 - ETA: 4:02 - loss: 1.3926 - accuracy: 0.08 - ETA: 4:01 - loss: 1.3950 - accuracy: 0.08 - ETA: 4:00 - loss: 1.3946 - accuracy: 0.08 - ETA: 3:59 - loss: 1.3930 - accuracy: 0.08 - ETA: 3:58 - loss: 1.3926 - accuracy: 0.08 - ETA: 3:57 - loss: 1.3912 - accuracy: 0.08 - ETA: 3:56 - loss: 1.3895 - accuracy: 0.08 - ETA: 3:55 - loss: 1.3941 - accuracy: 0.08 - ETA: 3:54 - loss: 1.3924 - accuracy: 0.08 - ETA: 3:54 - loss: 1.3918 - accuracy: 0.08 - ETA: 3:53 - loss: 1.3891 - accuracy: 0.08 - ETA: 3:52 - loss: 1.3916 - accuracy: 0.08 - ETA: 3:51 - loss: 1.3928 - accuracy: 0.08 - ETA: 3:50 - loss: 1.3955 - accuracy: 0.08 - ETA: 3:49 - loss: 1.3953 - accuracy: 0.08 - ETA: 3:48 - loss: 1.3983 - accuracy: 0.08 - ETA: 3:47 - loss: 1.3980 - accuracy: 0.08 - ETA: 3:47 - loss: 1.3990 - accuracy: 0.08 - ETA: 3:46 - loss: 1.3981 - accuracy: 0.08 - ETA: 3:45 - loss: 1.3965 - accuracy: 0.08 - ETA: 3:44 - loss: 1.3979 - accuracy: 0.08 - ETA: 3:43 - loss: 1.3995 - accuracy: 0.08 - ETA: 3:42 - loss: 1.3995 - accuracy: 0.08 - ETA: 3:41 - loss: 1.3994 - accuracy: 0.08 - ETA: 3:40 - loss: 1.4006 - accuracy: 0.08 - ETA: 3:39 - loss: 1.3999 - accuracy: 0.08 - ETA: 3:39 - loss: 1.3995 - accuracy: 0.08 - ETA: 3:38 - loss: 1.4011 - accuracy: 0.08 - ETA: 3:37 - loss: 1.3994 - accuracy: 0.08 - ETA: 3:36 - loss: 1.3984 - accuracy: 0.08 - ETA: 3:35 - loss: 1.3971 - accuracy: 0.08 - ETA: 3:34 - loss: 1.3961 - accuracy: 0.08 - ETA: 3:33 - loss: 1.3939 - accuracy: 0.08 - ETA: 3:32 - loss: 1.3940 - accuracy: 0.08 - ETA: 3:32 - loss: 1.3955 - accuracy: 0.08 - ETA: 3:31 - loss: 1.3944 - accuracy: 0.08 - ETA: 3:30 - loss: 1.3935 - accuracy: 0.08 - ETA: 3:29 - loss: 1.3916 - accuracy: 0.08 - ETA: 3:28 - loss: 1.3904 - accuracy: 0.08 - ETA: 3:27 - loss: 1.3911 - accuracy: 0.08 - ETA: 3:26 - loss: 1.3929 - accuracy: 0.08 - ETA: 3:25 - loss: 1.3915 - accuracy: 0.08 - ETA: 3:25 - loss: 1.3912 - accuracy: 0.08 - ETA: 3:24 - loss: 1.3918 - accuracy: 0.08 - ETA: 3:23 - loss: 1.3922 - accuracy: 0.08 - ETA: 3:22 - loss: 1.3923 - accuracy: 0.08 - ETA: 3:21 - loss: 1.3930 - accuracy: 0.08 - ETA: 3:20 - loss: 1.3914 - accuracy: 0.08 - ETA: 3:19 - loss: 1.3909 - accuracy: 0.08 - ETA: 3:18 - loss: 1.3919 - accuracy: 0.08 - ETA: 3:17 - loss: 1.3901 - accuracy: 0.08 - ETA: 3:17 - loss: 1.3909 - accuracy: 0.08 - ETA: 3:16 - loss: 1.3902 - accuracy: 0.08 - ETA: 3:15 - loss: 1.3902 - accuracy: 0.08 - ETA: 3:14 - loss: 1.3895 - accuracy: 0.08 - ETA: 3:13 - loss: 1.3897 - accuracy: 0.08 - ETA: 3:12 - loss: 1.3901 - accuracy: 0.08 - ETA: 3:11 - loss: 1.3897 - accuracy: 0.08 - ETA: 3:10 - loss: 1.3901 - accuracy: 0.08 - ETA: 3:09 - loss: 1.3903 - accuracy: 0.08 - ETA: 3:08 - loss: 1.3898 - accuracy: 0.08 - ETA: 3:08 - loss: 1.3899 - accuracy: 0.08 - ETA: 3:07 - loss: 1.3893 - accuracy: 0.08 - ETA: 3:06 - loss: 1.3904 - accuracy: 0.08 - ETA: 3:05 - loss: 1.3906 - accuracy: 0.08 - ETA: 3:04 - loss: 1.3908 - accuracy: 0.08 - ETA: 3:03 - loss: 1.3903 - accuracy: 0.08 - ETA: 3:02 - loss: 1.3913 - accuracy: 0.08 - ETA: 3:01 - loss: 1.3902 - accuracy: 0.08 - ETA: 3:01 - loss: 1.3898 - accuracy: 0.08 - ETA: 3:00 - loss: 1.3886 - accuracy: 0.08 - ETA: 2:59 - loss: 1.3894 - accuracy: 0.08 - ETA: 2:58 - loss: 1.3891 - accuracy: 0.08 - ETA: 2:57 - loss: 1.3886 - accuracy: 0.08 - ETA: 2:56 - loss: 1.3884 - accuracy: 0.08 - ETA: 2:55 - loss: 1.3876 - accuracy: 0.08 - ETA: 2:54 - loss: 1.3888 - accuracy: 0.08 - ETA: 2:53 - loss: 1.3884 - accuracy: 0.08 - ETA: 2:52 - loss: 1.3883 - accuracy: 0.08 - ETA: 2:52 - loss: 1.3875 - accuracy: 0.08 - ETA: 2:51 - loss: 1.3876 - accuracy: 0.08 - ETA: 2:50 - loss: 1.3866 - accuracy: 0.08 - ETA: 2:49 - loss: 1.3866 - accuracy: 0.08 - ETA: 2:48 - loss: 1.3860 - accuracy: 0.08 - ETA: 2:47 - loss: 1.3872 - accuracy: 0.08 - ETA: 2:46 - loss: 1.3868 - accuracy: 0.08 - ETA: 2:45 - loss: 1.3871 - accuracy: 0.08 - ETA: 2:44 - loss: 1.3865 - accuracy: 0.08 - ETA: 2:44 - loss: 1.3869 - accuracy: 0.08 - ETA: 2:43 - loss: 1.3872 - accuracy: 0.08 - ETA: 2:42 - loss: 1.3878 - accuracy: 0.08 - ETA: 2:41 - loss: 1.3885 - accuracy: 0.08 - ETA: 2:40 - loss: 1.3892 - accuracy: 0.08 - ETA: 2:39 - loss: 1.3894 - accuracy: 0.08 - ETA: 2:38 - loss: 1.3891 - accuracy: 0.08 - ETA: 2:37 - loss: 1.3887 - accuracy: 0.08 - ETA: 2:36 - loss: 1.3886 - accuracy: 0.08 - ETA: 2:35 - loss: 1.3877 - accuracy: 0.08 - ETA: 2:35 - loss: 1.3886 - accuracy: 0.08 - ETA: 2:34 - loss: 1.3866 - accuracy: 0.08 - ETA: 2:33 - loss: 1.3862 - accuracy: 0.08 - ETA: 2:32 - loss: 1.3858 - accuracy: 0.08 - ETA: 2:31 - loss: 1.3848 - accuracy: 0.08 - ETA: 2:30 - loss: 1.3852 - accuracy: 0.08 - ETA: 2:29 - loss: 1.3840 - accuracy: 0.08 - ETA: 2:28 - loss: 1.3836 - accuracy: 0.08 - ETA: 2:27 - loss: 1.3835 - accuracy: 0.08 - ETA: 2:26 - loss: 1.3848 - accuracy: 0.08 - ETA: 2:26 - loss: 1.3862 - accuracy: 0.08 - ETA: 2:25 - loss: 1.3863 - accuracy: 0.08 - ETA: 2:24 - loss: 1.3864 - accuracy: 0.08 - ETA: 2:23 - loss: 1.3860 - accuracy: 0.08 - ETA: 2:22 - loss: 1.3866 - accuracy: 0.0867"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - ETA: 2:21 - loss: 1.3861 - accuracy: 0.08 - ETA: 2:20 - loss: 1.3855 - accuracy: 0.08 - ETA: 2:19 - loss: 1.3850 - accuracy: 0.08 - ETA: 2:18 - loss: 1.3859 - accuracy: 0.08 - ETA: 2:17 - loss: 1.3863 - accuracy: 0.08 - ETA: 2:17 - loss: 1.3848 - accuracy: 0.08 - ETA: 2:16 - loss: 1.3856 - accuracy: 0.08 - ETA: 2:15 - loss: 1.3854 - accuracy: 0.08 - ETA: 2:14 - loss: 1.3844 - accuracy: 0.08 - ETA: 2:13 - loss: 1.3850 - accuracy: 0.08 - ETA: 2:12 - loss: 1.3845 - accuracy: 0.08 - ETA: 2:11 - loss: 1.3846 - accuracy: 0.08 - ETA: 2:10 - loss: 1.3834 - accuracy: 0.08 - ETA: 2:09 - loss: 1.3842 - accuracy: 0.08 - ETA: 2:09 - loss: 1.3831 - accuracy: 0.08 - ETA: 2:08 - loss: 1.3827 - accuracy: 0.08 - ETA: 2:07 - loss: 1.3834 - accuracy: 0.08 - ETA: 2:06 - loss: 1.3835 - accuracy: 0.08 - ETA: 2:05 - loss: 1.3837 - accuracy: 0.08 - ETA: 2:04 - loss: 1.3849 - accuracy: 0.08 - ETA: 2:03 - loss: 1.3845 - accuracy: 0.08 - ETA: 2:02 - loss: 1.3849 - accuracy: 0.08 - ETA: 2:01 - loss: 1.3859 - accuracy: 0.08 - ETA: 2:00 - loss: 1.3863 - accuracy: 0.08 - ETA: 2:00 - loss: 1.3866 - accuracy: 0.08 - ETA: 1:59 - loss: 1.3869 - accuracy: 0.08 - ETA: 1:58 - loss: 1.3862 - accuracy: 0.08 - ETA: 1:57 - loss: 1.3864 - accuracy: 0.08 - ETA: 1:56 - loss: 1.3868 - accuracy: 0.08 - ETA: 1:55 - loss: 1.3866 - accuracy: 0.08 - ETA: 1:54 - loss: 1.3864 - accuracy: 0.08 - ETA: 1:53 - loss: 1.3863 - accuracy: 0.08 - ETA: 1:52 - loss: 1.3864 - accuracy: 0.08 - ETA: 1:51 - loss: 1.3861 - accuracy: 0.08 - ETA: 1:51 - loss: 1.3863 - accuracy: 0.08 - ETA: 1:50 - loss: 1.3859 - accuracy: 0.08 - ETA: 1:49 - loss: 1.3858 - accuracy: 0.08 - ETA: 1:48 - loss: 1.3854 - accuracy: 0.08 - ETA: 1:47 - loss: 1.3857 - accuracy: 0.08 - ETA: 1:46 - loss: 1.3852 - accuracy: 0.08 - ETA: 1:45 - loss: 1.3847 - accuracy: 0.08 - ETA: 1:44 - loss: 1.3858 - accuracy: 0.08 - ETA: 1:43 - loss: 1.3861 - accuracy: 0.08 - ETA: 1:43 - loss: 1.3872 - accuracy: 0.08 - ETA: 1:42 - loss: 1.3861 - accuracy: 0.08 - ETA: 1:41 - loss: 1.3865 - accuracy: 0.08 - ETA: 1:40 - loss: 1.3871 - accuracy: 0.08 - ETA: 1:39 - loss: 1.3866 - accuracy: 0.08 - ETA: 1:38 - loss: 1.3861 - accuracy: 0.08 - ETA: 1:37 - loss: 1.3868 - accuracy: 0.08 - ETA: 1:36 - loss: 1.3871 - accuracy: 0.08 - ETA: 1:35 - loss: 1.3862 - accuracy: 0.08 - ETA: 1:34 - loss: 1.3863 - accuracy: 0.08 - ETA: 1:34 - loss: 1.3859 - accuracy: 0.08 - ETA: 1:33 - loss: 1.3857 - accuracy: 0.08 - ETA: 1:32 - loss: 1.3851 - accuracy: 0.08 - ETA: 1:31 - loss: 1.3845 - accuracy: 0.08 - ETA: 1:30 - loss: 1.3837 - accuracy: 0.08 - ETA: 1:29 - loss: 1.3828 - accuracy: 0.08 - ETA: 1:28 - loss: 1.3836 - accuracy: 0.08 - ETA: 1:27 - loss: 1.3840 - accuracy: 0.08 - ETA: 1:26 - loss: 1.3834 - accuracy: 0.08 - ETA: 1:26 - loss: 1.3845 - accuracy: 0.08 - ETA: 1:25 - loss: 1.3848 - accuracy: 0.08 - ETA: 1:24 - loss: 1.3855 - accuracy: 0.08 - ETA: 1:23 - loss: 1.3852 - accuracy: 0.08 - ETA: 1:22 - loss: 1.3858 - accuracy: 0.08 - ETA: 1:21 - loss: 1.3856 - accuracy: 0.08 - ETA: 1:20 - loss: 1.3865 - accuracy: 0.08 - ETA: 1:19 - loss: 1.3865 - accuracy: 0.08 - ETA: 1:18 - loss: 1.3869 - accuracy: 0.08 - ETA: 1:18 - loss: 1.3863 - accuracy: 0.08 - ETA: 1:17 - loss: 1.3869 - accuracy: 0.08 - ETA: 1:16 - loss: 1.3860 - accuracy: 0.08 - ETA: 1:15 - loss: 1.3865 - accuracy: 0.08 - ETA: 1:14 - loss: 1.3864 - accuracy: 0.08 - ETA: 1:13 - loss: 1.3862 - accuracy: 0.08 - ETA: 1:12 - loss: 1.3855 - accuracy: 0.08 - ETA: 1:11 - loss: 1.3855 - accuracy: 0.08 - ETA: 1:10 - loss: 1.3850 - accuracy: 0.08 - ETA: 1:09 - loss: 1.3844 - accuracy: 0.08 - ETA: 1:09 - loss: 1.3853 - accuracy: 0.08 - ETA: 1:08 - loss: 1.3846 - accuracy: 0.08 - ETA: 1:07 - loss: 1.3847 - accuracy: 0.08 - ETA: 1:06 - loss: 1.3844 - accuracy: 0.08 - ETA: 1:05 - loss: 1.3852 - accuracy: 0.08 - ETA: 1:04 - loss: 1.3858 - accuracy: 0.08 - ETA: 1:03 - loss: 1.3855 - accuracy: 0.08 - ETA: 1:02 - loss: 1.3856 - accuracy: 0.08 - ETA: 1:01 - loss: 1.3851 - accuracy: 0.08 - ETA: 1:01 - loss: 1.3855 - accuracy: 0.08 - ETA: 1:00 - loss: 1.3858 - accuracy: 0.08 - ETA: 59s - loss: 1.3858 - accuracy: 0.0864 - ETA: 58s - loss: 1.3856 - accuracy: 0.086 - ETA: 57s - loss: 1.3854 - accuracy: 0.086 - ETA: 56s - loss: 1.3853 - accuracy: 0.086 - ETA: 55s - loss: 1.3853 - accuracy: 0.086 - ETA: 54s - loss: 1.3858 - accuracy: 0.086 - ETA: 53s - loss: 1.3862 - accuracy: 0.086 - ETA: 52s - loss: 1.3864 - accuracy: 0.086 - ETA: 52s - loss: 1.3861 - accuracy: 0.086 - ETA: 51s - loss: 1.3860 - accuracy: 0.086 - ETA: 50s - loss: 1.3860 - accuracy: 0.086 - ETA: 49s - loss: 1.3861 - accuracy: 0.086 - ETA: 48s - loss: 1.3869 - accuracy: 0.086 - ETA: 47s - loss: 1.3865 - accuracy: 0.086 - ETA: 46s - loss: 1.3873 - accuracy: 0.086 - ETA: 45s - loss: 1.3864 - accuracy: 0.086 - ETA: 44s - loss: 1.3875 - accuracy: 0.086 - ETA: 43s - loss: 1.3878 - accuracy: 0.086 - ETA: 43s - loss: 1.3886 - accuracy: 0.086 - ETA: 42s - loss: 1.3882 - accuracy: 0.086 - ETA: 41s - loss: 1.3890 - accuracy: 0.086 - ETA: 40s - loss: 1.3899 - accuracy: 0.086 - ETA: 39s - loss: 1.3898 - accuracy: 0.086 - ETA: 38s - loss: 1.3893 - accuracy: 0.086 - ETA: 37s - loss: 1.3895 - accuracy: 0.086 - ETA: 36s - loss: 1.3894 - accuracy: 0.086 - ETA: 35s - loss: 1.3897 - accuracy: 0.086 - ETA: 35s - loss: 1.3903 - accuracy: 0.086 - ETA: 34s - loss: 1.3906 - accuracy: 0.086 - ETA: 33s - loss: 1.3904 - accuracy: 0.086 - ETA: 32s - loss: 1.3904 - accuracy: 0.086 - ETA: 31s - loss: 1.3900 - accuracy: 0.086 - ETA: 30s - loss: 1.3901 - accuracy: 0.086 - ETA: 29s - loss: 1.3896 - accuracy: 0.086 - ETA: 28s - loss: 1.3903 - accuracy: 0.086 - ETA: 27s - loss: 1.3903 - accuracy: 0.086 - ETA: 26s - loss: 1.3904 - accuracy: 0.086 - ETA: 26s - loss: 1.3901 - accuracy: 0.086 - ETA: 25s - loss: 1.3900 - accuracy: 0.086 - ETA: 24s - loss: 1.3901 - accuracy: 0.086 - ETA: 23s - loss: 1.3902 - accuracy: 0.086 - ETA: 22s - loss: 1.3901 - accuracy: 0.086 - ETA: 21s - loss: 1.3902 - accuracy: 0.086 - ETA: 20s - loss: 1.3910 - accuracy: 0.086 - ETA: 19s - loss: 1.3910 - accuracy: 0.086 - ETA: 18s - loss: 1.3898 - accuracy: 0.086 - ETA: 17s - loss: 1.3907 - accuracy: 0.086 - ETA: 17s - loss: 1.3908 - accuracy: 0.086 - ETA: 16s - loss: 1.3906 - accuracy: 0.086 - ETA: 15s - loss: 1.3908 - accuracy: 0.086 - ETA: 14s - loss: 1.3909 - accuracy: 0.086 - ETA: 13s - loss: 1.3914 - accuracy: 0.086 - ETA: 12s - loss: 1.3924 - accuracy: 0.086 - ETA: 11s - loss: 1.3928 - accuracy: 0.086 - ETA: 10s - loss: 1.3926 - accuracy: 0.086 - ETA: 9s - loss: 1.3931 - accuracy: 0.086 - ETA: 8s - loss: 1.3928 - accuracy: 0.08 - ETA: 8s - loss: 1.3927 - accuracy: 0.08 - ETA: 7s - loss: 1.3921 - accuracy: 0.08 - ETA: 6s - loss: 1.3921 - accuracy: 0.08 - ETA: 5s - loss: 1.3921 - accuracy: 0.08 - ETA: 4s - loss: 1.3917 - accuracy: 0.08 - ETA: 3s - loss: 1.3915 - accuracy: 0.08 - ETA: 2s - loss: 1.3922 - accuracy: 0.08 - ETA: 1s - loss: 1.3930 - accuracy: 0.08 - ETA: 0s - loss: 1.3931 - accuracy: 0.08 - 310s 899ms/step - loss: 1.3931 - accuracy: 0.0868\n",
      "Epoch 6/20\n",
      "186/345 [===============>..............] - ETA: 5:39 - loss: 1.2128 - accuracy: 0.08 - ETA: 5:17 - loss: 1.2638 - accuracy: 0.08 - ETA: 5:16 - loss: 1.3008 - accuracy: 0.08 - ETA: 5:11 - loss: 1.3757 - accuracy: 0.08 - ETA: 5:07 - loss: 1.3767 - accuracy: 0.08 - ETA: 5:10 - loss: 1.3714 - accuracy: 0.08 - ETA: 5:09 - loss: 1.3699 - accuracy: 0.09 - ETA: 5:06 - loss: 1.3727 - accuracy: 0.09 - ETA: 5:04 - loss: 1.3531 - accuracy: 0.09 - ETA: 5:03 - loss: 1.3569 - accuracy: 0.09 - ETA: 5:03 - loss: 1.3463 - accuracy: 0.09 - ETA: 5:03 - loss: 1.3293 - accuracy: 0.09 - ETA: 5:02 - loss: 1.3347 - accuracy: 0.09 - ETA: 5:00 - loss: 1.3421 - accuracy: 0.09 - ETA: 4:59 - loss: 1.3236 - accuracy: 0.08 - ETA: 5:00 - loss: 1.3237 - accuracy: 0.08 - ETA: 4:58 - loss: 1.3222 - accuracy: 0.09 - ETA: 4:57 - loss: 1.3220 - accuracy: 0.09 - ETA: 4:55 - loss: 1.3321 - accuracy: 0.08 - ETA: 4:55 - loss: 1.3262 - accuracy: 0.08 - ETA: 4:53 - loss: 1.3239 - accuracy: 0.08 - ETA: 4:53 - loss: 1.3235 - accuracy: 0.09 - ETA: 4:52 - loss: 1.3332 - accuracy: 0.09 - ETA: 4:51 - loss: 1.3431 - accuracy: 0.09 - ETA: 4:50 - loss: 1.3413 - accuracy: 0.09 - ETA: 4:49 - loss: 1.3427 - accuracy: 0.09 - ETA: 4:49 - loss: 1.3333 - accuracy: 0.09 - ETA: 4:47 - loss: 1.3365 - accuracy: 0.09 - ETA: 4:46 - loss: 1.3356 - accuracy: 0.09 - ETA: 4:44 - loss: 1.3398 - accuracy: 0.09 - ETA: 4:44 - loss: 1.3395 - accuracy: 0.09 - ETA: 4:43 - loss: 1.3413 - accuracy: 0.09 - ETA: 4:42 - loss: 1.3426 - accuracy: 0.09 - ETA: 4:41 - loss: 1.3406 - accuracy: 0.09 - ETA: 4:40 - loss: 1.3428 - accuracy: 0.09 - ETA: 4:40 - loss: 1.3406 - accuracy: 0.09 - ETA: 4:39 - loss: 1.3434 - accuracy: 0.09 - ETA: 4:37 - loss: 1.3442 - accuracy: 0.09 - ETA: 4:37 - loss: 1.3475 - accuracy: 0.09 - ETA: 4:36 - loss: 1.3431 - accuracy: 0.09 - ETA: 4:35 - loss: 1.3385 - accuracy: 0.09 - ETA: 4:34 - loss: 1.3338 - accuracy: 0.09 - ETA: 4:33 - loss: 1.3348 - accuracy: 0.09 - ETA: 4:32 - loss: 1.3375 - accuracy: 0.09 - ETA: 4:31 - loss: 1.3370 - accuracy: 0.09 - ETA: 4:31 - loss: 1.3375 - accuracy: 0.09 - ETA: 4:30 - loss: 1.3355 - accuracy: 0.09 - ETA: 4:29 - loss: 1.3346 - accuracy: 0.09 - ETA: 4:28 - loss: 1.3346 - accuracy: 0.09 - ETA: 4:26 - loss: 1.3344 - accuracy: 0.09 - ETA: 4:26 - loss: 1.3407 - accuracy: 0.09 - ETA: 4:25 - loss: 1.3329 - accuracy: 0.09 - ETA: 4:24 - loss: 1.3334 - accuracy: 0.09 - ETA: 4:23 - loss: 1.3320 - accuracy: 0.09 - ETA: 4:22 - loss: 1.3330 - accuracy: 0.09 - ETA: 4:21 - loss: 1.3350 - accuracy: 0.09 - ETA: 4:20 - loss: 1.3392 - accuracy: 0.09 - ETA: 4:19 - loss: 1.3421 - accuracy: 0.09 - ETA: 4:18 - loss: 1.3442 - accuracy: 0.09 - ETA: 4:17 - loss: 1.3397 - accuracy: 0.09 - ETA: 4:17 - loss: 1.3374 - accuracy: 0.09 - ETA: 4:16 - loss: 1.3358 - accuracy: 0.09 - ETA: 4:15 - loss: 1.3331 - accuracy: 0.09 - ETA: 4:14 - loss: 1.3313 - accuracy: 0.09 - ETA: 4:13 - loss: 1.3345 - accuracy: 0.09 - ETA: 4:12 - loss: 1.3352 - accuracy: 0.09 - ETA: 4:11 - loss: 1.3353 - accuracy: 0.09 - ETA: 4:10 - loss: 1.3370 - accuracy: 0.09 - ETA: 4:09 - loss: 1.3362 - accuracy: 0.09 - ETA: 4:08 - loss: 1.3361 - accuracy: 0.09 - ETA: 4:08 - loss: 1.3358 - accuracy: 0.09 - ETA: 4:07 - loss: 1.3356 - accuracy: 0.09 - ETA: 4:06 - loss: 1.3376 - accuracy: 0.09 - ETA: 4:05 - loss: 1.3366 - accuracy: 0.09 - ETA: 4:04 - loss: 1.3366 - accuracy: 0.09 - ETA: 4:03 - loss: 1.3365 - accuracy: 0.09 - ETA: 4:02 - loss: 1.3350 - accuracy: 0.09 - ETA: 4:01 - loss: 1.3331 - accuracy: 0.09 - ETA: 4:00 - loss: 1.3332 - accuracy: 0.09 - ETA: 3:59 - loss: 1.3359 - accuracy: 0.09 - ETA: 3:59 - loss: 1.3380 - accuracy: 0.09 - ETA: 3:58 - loss: 1.3374 - accuracy: 0.09 - ETA: 3:57 - loss: 1.3377 - accuracy: 0.09 - ETA: 3:56 - loss: 1.3359 - accuracy: 0.09 - ETA: 3:55 - loss: 1.3343 - accuracy: 0.09 - ETA: 3:54 - loss: 1.3363 - accuracy: 0.09 - ETA: 3:53 - loss: 1.3374 - accuracy: 0.09 - ETA: 3:52 - loss: 1.3362 - accuracy: 0.09 - ETA: 3:51 - loss: 1.3364 - accuracy: 0.09 - ETA: 3:51 - loss: 1.3360 - accuracy: 0.09 - ETA: 3:50 - loss: 1.3378 - accuracy: 0.09 - ETA: 3:49 - loss: 1.3382 - accuracy: 0.09 - ETA: 3:48 - loss: 1.3396 - accuracy: 0.09 - ETA: 3:47 - loss: 1.3412 - accuracy: 0.09 - ETA: 3:46 - loss: 1.3422 - accuracy: 0.09 - ETA: 3:46 - loss: 1.3412 - accuracy: 0.09 - ETA: 3:45 - loss: 1.3434 - accuracy: 0.09 - ETA: 3:44 - loss: 1.3432 - accuracy: 0.09 - ETA: 3:43 - loss: 1.3426 - accuracy: 0.09 - ETA: 3:42 - loss: 1.3425 - accuracy: 0.09 - ETA: 3:41 - loss: 1.3417 - accuracy: 0.09 - ETA: 3:40 - loss: 1.3449 - accuracy: 0.09 - ETA: 3:39 - loss: 1.3452 - accuracy: 0.09 - ETA: 3:38 - loss: 1.3434 - accuracy: 0.09 - ETA: 3:37 - loss: 1.3408 - accuracy: 0.09 - ETA: 3:36 - loss: 1.3404 - accuracy: 0.09 - ETA: 3:36 - loss: 1.3398 - accuracy: 0.09 - ETA: 3:35 - loss: 1.3387 - accuracy: 0.09 - ETA: 3:34 - loss: 1.3393 - accuracy: 0.09 - ETA: 3:33 - loss: 1.3393 - accuracy: 0.09 - ETA: 3:32 - loss: 1.3391 - accuracy: 0.09 - ETA: 3:31 - loss: 1.3376 - accuracy: 0.09 - ETA: 3:30 - loss: 1.3354 - accuracy: 0.09 - ETA: 3:29 - loss: 1.3331 - accuracy: 0.09 - ETA: 3:28 - loss: 1.3333 - accuracy: 0.09 - ETA: 3:27 - loss: 1.3319 - accuracy: 0.09 - ETA: 3:26 - loss: 1.3322 - accuracy: 0.09 - ETA: 3:25 - loss: 1.3318 - accuracy: 0.09 - ETA: 3:24 - loss: 1.3317 - accuracy: 0.09 - ETA: 3:23 - loss: 1.3308 - accuracy: 0.09 - ETA: 3:22 - loss: 1.3319 - accuracy: 0.09 - ETA: 3:22 - loss: 1.3295 - accuracy: 0.09 - ETA: 3:21 - loss: 1.3312 - accuracy: 0.09 - ETA: 3:20 - loss: 1.3310 - accuracy: 0.09 - ETA: 3:19 - loss: 1.3324 - accuracy: 0.09 - ETA: 3:18 - loss: 1.3339 - accuracy: 0.09 - ETA: 3:17 - loss: 1.3322 - accuracy: 0.09 - ETA: 3:16 - loss: 1.3321 - accuracy: 0.09 - ETA: 3:15 - loss: 1.3320 - accuracy: 0.09 - ETA: 3:14 - loss: 1.3325 - accuracy: 0.09 - ETA: 3:13 - loss: 1.3337 - accuracy: 0.09 - ETA: 3:13 - loss: 1.3348 - accuracy: 0.09 - ETA: 3:12 - loss: 1.3368 - accuracy: 0.09 - ETA: 3:11 - loss: 1.3350 - accuracy: 0.09 - ETA: 3:10 - loss: 1.3333 - accuracy: 0.09 - ETA: 3:09 - loss: 1.3327 - accuracy: 0.09 - ETA: 3:08 - loss: 1.3346 - accuracy: 0.09 - ETA: 3:07 - loss: 1.3347 - accuracy: 0.09 - ETA: 3:06 - loss: 1.3351 - accuracy: 0.09 - ETA: 3:05 - loss: 1.3359 - accuracy: 0.09 - ETA: 3:04 - loss: 1.3354 - accuracy: 0.09 - ETA: 3:03 - loss: 1.3360 - accuracy: 0.09 - ETA: 3:03 - loss: 1.3369 - accuracy: 0.09 - ETA: 3:02 - loss: 1.3364 - accuracy: 0.09 - ETA: 3:01 - loss: 1.3371 - accuracy: 0.09 - ETA: 3:00 - loss: 1.3350 - accuracy: 0.09 - ETA: 2:59 - loss: 1.3362 - accuracy: 0.09 - ETA: 2:58 - loss: 1.3360 - accuracy: 0.09 - ETA: 2:57 - loss: 1.3357 - accuracy: 0.09 - ETA: 2:56 - loss: 1.3355 - accuracy: 0.09 - ETA: 2:55 - loss: 1.3359 - accuracy: 0.09 - ETA: 2:54 - loss: 1.3361 - accuracy: 0.09 - ETA: 2:53 - loss: 1.3372 - accuracy: 0.09 - ETA: 2:53 - loss: 1.3369 - accuracy: 0.09 - ETA: 2:52 - loss: 1.3372 - accuracy: 0.09 - ETA: 2:51 - loss: 1.3355 - accuracy: 0.09 - ETA: 2:50 - loss: 1.3344 - accuracy: 0.09 - ETA: 2:49 - loss: 1.3358 - accuracy: 0.09 - ETA: 2:48 - loss: 1.3362 - accuracy: 0.09 - ETA: 2:47 - loss: 1.3358 - accuracy: 0.09 - ETA: 2:46 - loss: 1.3367 - accuracy: 0.09 - ETA: 2:45 - loss: 1.3369 - accuracy: 0.09 - ETA: 2:44 - loss: 1.3362 - accuracy: 0.09 - ETA: 2:43 - loss: 1.3346 - accuracy: 0.09 - ETA: 2:43 - loss: 1.3351 - accuracy: 0.09 - ETA: 2:42 - loss: 1.3328 - accuracy: 0.09 - ETA: 2:41 - loss: 1.3325 - accuracy: 0.09 - ETA: 2:40 - loss: 1.3316 - accuracy: 0.09 - ETA: 2:39 - loss: 1.3309 - accuracy: 0.09 - ETA: 2:38 - loss: 1.3303 - accuracy: 0.09 - ETA: 2:37 - loss: 1.3310 - accuracy: 0.09 - ETA: 2:36 - loss: 1.3303 - accuracy: 0.09 - ETA: 2:35 - loss: 1.3296 - accuracy: 0.09 - ETA: 2:34 - loss: 1.3314 - accuracy: 0.09 - ETA: 2:33 - loss: 1.3316 - accuracy: 0.09 - ETA: 2:33 - loss: 1.3323 - accuracy: 0.09 - ETA: 2:32 - loss: 1.3323 - accuracy: 0.09 - ETA: 2:31 - loss: 1.3331 - accuracy: 0.09 - ETA: 2:30 - loss: 1.3341 - accuracy: 0.09 - ETA: 2:29 - loss: 1.3343 - accuracy: 0.09 - ETA: 2:28 - loss: 1.3339 - accuracy: 0.09 - ETA: 2:27 - loss: 1.3339 - accuracy: 0.09 - ETA: 2:26 - loss: 1.3334 - accuracy: 0.09 - ETA: 2:25 - loss: 1.3336 - accuracy: 0.09 - ETA: 2:24 - loss: 1.3334 - accuracy: 0.09 - ETA: 2:23 - loss: 1.3328 - accuracy: 0.0915345/345 [==============================] - ETA: 2:23 - loss: 1.3323 - accuracy: 0.09 - ETA: 2:22 - loss: 1.3329 - accuracy: 0.09 - ETA: 2:21 - loss: 1.3336 - accuracy: 0.09 - ETA: 2:20 - loss: 1.3336 - accuracy: 0.09 - ETA: 2:19 - loss: 1.3331 - accuracy: 0.09 - ETA: 2:18 - loss: 1.3330 - accuracy: 0.09 - ETA: 2:17 - loss: 1.3336 - accuracy: 0.09 - ETA: 2:16 - loss: 1.3333 - accuracy: 0.09 - ETA: 2:15 - loss: 1.3337 - accuracy: 0.09 - ETA: 2:14 - loss: 1.3335 - accuracy: 0.09 - ETA: 2:14 - loss: 1.3334 - accuracy: 0.09 - ETA: 2:13 - loss: 1.3333 - accuracy: 0.09 - ETA: 2:12 - loss: 1.3316 - accuracy: 0.09 - ETA: 2:11 - loss: 1.3325 - accuracy: 0.09 - ETA: 2:10 - loss: 1.3316 - accuracy: 0.09 - ETA: 2:09 - loss: 1.3319 - accuracy: 0.09 - ETA: 2:08 - loss: 1.3319 - accuracy: 0.09 - ETA: 2:07 - loss: 1.3320 - accuracy: 0.09 - ETA: 2:06 - loss: 1.3319 - accuracy: 0.09 - ETA: 2:05 - loss: 1.3332 - accuracy: 0.09 - ETA: 2:04 - loss: 1.3338 - accuracy: 0.09 - ETA: 2:04 - loss: 1.3328 - accuracy: 0.09 - ETA: 2:03 - loss: 1.3332 - accuracy: 0.09 - ETA: 2:02 - loss: 1.3329 - accuracy: 0.09 - ETA: 2:01 - loss: 1.3327 - accuracy: 0.09 - ETA: 2:00 - loss: 1.3328 - accuracy: 0.09 - ETA: 1:59 - loss: 1.3344 - accuracy: 0.09 - ETA: 1:58 - loss: 1.3341 - accuracy: 0.09 - ETA: 1:57 - loss: 1.3342 - accuracy: 0.09 - ETA: 1:56 - loss: 1.3339 - accuracy: 0.09 - ETA: 1:55 - loss: 1.3336 - accuracy: 0.09 - ETA: 1:54 - loss: 1.3341 - accuracy: 0.09 - ETA: 1:54 - loss: 1.3353 - accuracy: 0.09 - ETA: 1:53 - loss: 1.3352 - accuracy: 0.09 - ETA: 1:52 - loss: 1.3342 - accuracy: 0.09 - ETA: 1:51 - loss: 1.3342 - accuracy: 0.09 - ETA: 1:50 - loss: 1.3338 - accuracy: 0.09 - ETA: 1:49 - loss: 1.3331 - accuracy: 0.09 - ETA: 1:48 - loss: 1.3325 - accuracy: 0.09 - ETA: 1:47 - loss: 1.3329 - accuracy: 0.09 - ETA: 1:46 - loss: 1.3344 - accuracy: 0.09 - ETA: 1:45 - loss: 1.3344 - accuracy: 0.09 - ETA: 1:45 - loss: 1.3344 - accuracy: 0.09 - ETA: 1:44 - loss: 1.3346 - accuracy: 0.09 - ETA: 1:43 - loss: 1.3356 - accuracy: 0.09 - ETA: 1:42 - loss: 1.3348 - accuracy: 0.09 - ETA: 1:41 - loss: 1.3348 - accuracy: 0.09 - ETA: 1:40 - loss: 1.3359 - accuracy: 0.09 - ETA: 1:39 - loss: 1.3360 - accuracy: 0.09 - ETA: 1:38 - loss: 1.3361 - accuracy: 0.09 - ETA: 1:37 - loss: 1.3361 - accuracy: 0.09 - ETA: 1:36 - loss: 1.3364 - accuracy: 0.09 - ETA: 1:35 - loss: 1.3356 - accuracy: 0.09 - ETA: 1:35 - loss: 1.3347 - accuracy: 0.09 - ETA: 1:34 - loss: 1.3338 - accuracy: 0.09 - ETA: 1:33 - loss: 1.3339 - accuracy: 0.09 - ETA: 1:32 - loss: 1.3333 - accuracy: 0.09 - ETA: 1:31 - loss: 1.3336 - accuracy: 0.09 - ETA: 1:30 - loss: 1.3348 - accuracy: 0.09 - ETA: 1:29 - loss: 1.3343 - accuracy: 0.09 - ETA: 1:28 - loss: 1.3334 - accuracy: 0.09 - ETA: 1:27 - loss: 1.3332 - accuracy: 0.09 - ETA: 1:26 - loss: 1.3315 - accuracy: 0.09 - ETA: 1:25 - loss: 1.3313 - accuracy: 0.09 - ETA: 1:25 - loss: 1.3310 - accuracy: 0.09 - ETA: 1:24 - loss: 1.3314 - accuracy: 0.09 - ETA: 1:23 - loss: 1.3311 - accuracy: 0.09 - ETA: 1:22 - loss: 1.3308 - accuracy: 0.09 - ETA: 1:21 - loss: 1.3312 - accuracy: 0.09 - ETA: 1:20 - loss: 1.3307 - accuracy: 0.09 - ETA: 1:19 - loss: 1.3302 - accuracy: 0.09 - ETA: 1:18 - loss: 1.3305 - accuracy: 0.09 - ETA: 1:17 - loss: 1.3307 - accuracy: 0.09 - ETA: 1:16 - loss: 1.3306 - accuracy: 0.09 - ETA: 1:16 - loss: 1.3315 - accuracy: 0.09 - ETA: 1:15 - loss: 1.3315 - accuracy: 0.09 - ETA: 1:14 - loss: 1.3315 - accuracy: 0.09 - ETA: 1:13 - loss: 1.3326 - accuracy: 0.09 - ETA: 1:12 - loss: 1.3332 - accuracy: 0.09 - ETA: 1:11 - loss: 1.3333 - accuracy: 0.09 - ETA: 1:10 - loss: 1.3332 - accuracy: 0.09 - ETA: 1:09 - loss: 1.3335 - accuracy: 0.09 - ETA: 1:08 - loss: 1.3337 - accuracy: 0.09 - ETA: 1:07 - loss: 1.3336 - accuracy: 0.09 - ETA: 1:06 - loss: 1.3336 - accuracy: 0.09 - ETA: 1:06 - loss: 1.3332 - accuracy: 0.09 - ETA: 1:05 - loss: 1.3332 - accuracy: 0.09 - ETA: 1:04 - loss: 1.3335 - accuracy: 0.09 - ETA: 1:03 - loss: 1.3331 - accuracy: 0.09 - ETA: 1:02 - loss: 1.3340 - accuracy: 0.09 - ETA: 1:01 - loss: 1.3343 - accuracy: 0.09 - ETA: 1:00 - loss: 1.3342 - accuracy: 0.09 - ETA: 59s - loss: 1.3340 - accuracy: 0.0915 - ETA: 58s - loss: 1.3337 - accuracy: 0.091 - ETA: 57s - loss: 1.3326 - accuracy: 0.091 - ETA: 57s - loss: 1.3323 - accuracy: 0.091 - ETA: 56s - loss: 1.3320 - accuracy: 0.091 - ETA: 55s - loss: 1.3318 - accuracy: 0.091 - ETA: 54s - loss: 1.3320 - accuracy: 0.091 - ETA: 53s - loss: 1.3325 - accuracy: 0.091 - ETA: 52s - loss: 1.3324 - accuracy: 0.091 - ETA: 51s - loss: 1.3322 - accuracy: 0.091 - ETA: 50s - loss: 1.3325 - accuracy: 0.091 - ETA: 49s - loss: 1.3326 - accuracy: 0.091 - ETA: 48s - loss: 1.3320 - accuracy: 0.091 - ETA: 47s - loss: 1.3319 - accuracy: 0.091 - ETA: 47s - loss: 1.3320 - accuracy: 0.091 - ETA: 46s - loss: 1.3317 - accuracy: 0.091 - ETA: 45s - loss: 1.3319 - accuracy: 0.091 - ETA: 44s - loss: 1.3322 - accuracy: 0.091 - ETA: 43s - loss: 1.3319 - accuracy: 0.091 - ETA: 42s - loss: 1.3323 - accuracy: 0.091 - ETA: 41s - loss: 1.3321 - accuracy: 0.091 - ETA: 40s - loss: 1.3329 - accuracy: 0.091 - ETA: 39s - loss: 1.3338 - accuracy: 0.091 - ETA: 38s - loss: 1.3337 - accuracy: 0.091 - ETA: 38s - loss: 1.3337 - accuracy: 0.091 - ETA: 37s - loss: 1.3334 - accuracy: 0.091 - ETA: 36s - loss: 1.3340 - accuracy: 0.091 - ETA: 35s - loss: 1.3330 - accuracy: 0.091 - ETA: 34s - loss: 1.3330 - accuracy: 0.091 - ETA: 33s - loss: 1.3327 - accuracy: 0.091 - ETA: 32s - loss: 1.3326 - accuracy: 0.091 - ETA: 31s - loss: 1.3326 - accuracy: 0.091 - ETA: 30s - loss: 1.3324 - accuracy: 0.091 - ETA: 29s - loss: 1.3324 - accuracy: 0.091 - ETA: 28s - loss: 1.3339 - accuracy: 0.091 - ETA: 28s - loss: 1.3345 - accuracy: 0.091 - ETA: 27s - loss: 1.3340 - accuracy: 0.091 - ETA: 26s - loss: 1.3336 - accuracy: 0.091 - ETA: 25s - loss: 1.3345 - accuracy: 0.091 - ETA: 24s - loss: 1.3343 - accuracy: 0.091 - ETA: 23s - loss: 1.3343 - accuracy: 0.091 - ETA: 22s - loss: 1.3342 - accuracy: 0.091 - ETA: 21s - loss: 1.3336 - accuracy: 0.091 - ETA: 20s - loss: 1.3329 - accuracy: 0.091 - ETA: 19s - loss: 1.3322 - accuracy: 0.091 - ETA: 19s - loss: 1.3326 - accuracy: 0.091 - ETA: 18s - loss: 1.3332 - accuracy: 0.091 - ETA: 17s - loss: 1.3332 - accuracy: 0.091 - ETA: 16s - loss: 1.3328 - accuracy: 0.091 - ETA: 15s - loss: 1.3328 - accuracy: 0.091 - ETA: 14s - loss: 1.3331 - accuracy: 0.091 - ETA: 13s - loss: 1.3333 - accuracy: 0.091 - ETA: 12s - loss: 1.3339 - accuracy: 0.091 - ETA: 11s - loss: 1.3341 - accuracy: 0.091 - ETA: 10s - loss: 1.3342 - accuracy: 0.091 - ETA: 9s - loss: 1.3341 - accuracy: 0.091 - ETA: 9s - loss: 1.3343 - accuracy: 0.09 - ETA: 8s - loss: 1.3343 - accuracy: 0.09 - ETA: 7s - loss: 1.3341 - accuracy: 0.09 - ETA: 6s - loss: 1.3347 - accuracy: 0.09 - ETA: 5s - loss: 1.3346 - accuracy: 0.09 - ETA: 4s - loss: 1.3349 - accuracy: 0.09 - ETA: 3s - loss: 1.3357 - accuracy: 0.09 - ETA: 2s - loss: 1.3360 - accuracy: 0.09 - ETA: 1s - loss: 1.3357 - accuracy: 0.09 - ETA: 0s - loss: 1.3361 - accuracy: 0.09 - 312s 905ms/step - loss: 1.3361 - accuracy: 0.0916\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/345 [===============>..............] - ETA: 5:20 - loss: 1.1631 - accuracy: 0.09 - ETA: 5:14 - loss: 1.2498 - accuracy: 0.09 - ETA: 5:14 - loss: 1.2301 - accuracy: 0.09 - ETA: 5:16 - loss: 1.2321 - accuracy: 0.09 - ETA: 5:19 - loss: 1.2326 - accuracy: 0.09 - ETA: 5:19 - loss: 1.2348 - accuracy: 0.09 - ETA: 5:20 - loss: 1.2597 - accuracy: 0.09 - ETA: 5:17 - loss: 1.2803 - accuracy: 0.09 - ETA: 5:14 - loss: 1.2902 - accuracy: 0.09 - ETA: 5:13 - loss: 1.2901 - accuracy: 0.09 - ETA: 5:14 - loss: 1.2696 - accuracy: 0.09 - ETA: 5:12 - loss: 1.2662 - accuracy: 0.09 - ETA: 5:11 - loss: 1.2608 - accuracy: 0.09 - ETA: 5:09 - loss: 1.2519 - accuracy: 0.09 - ETA: 5:10 - loss: 1.2550 - accuracy: 0.09 - ETA: 5:09 - loss: 1.2621 - accuracy: 0.09 - ETA: 5:07 - loss: 1.2643 - accuracy: 0.09 - ETA: 5:06 - loss: 1.2701 - accuracy: 0.09 - ETA: 5:05 - loss: 1.2725 - accuracy: 0.09 - ETA: 5:06 - loss: 1.2715 - accuracy: 0.09 - ETA: 5:05 - loss: 1.2750 - accuracy: 0.09 - ETA: 5:03 - loss: 1.2746 - accuracy: 0.09 - ETA: 5:02 - loss: 1.2676 - accuracy: 0.09 - ETA: 5:00 - loss: 1.2590 - accuracy: 0.09 - ETA: 5:00 - loss: 1.2699 - accuracy: 0.09 - ETA: 4:58 - loss: 1.2743 - accuracy: 0.09 - ETA: 4:56 - loss: 1.2739 - accuracy: 0.09 - ETA: 4:55 - loss: 1.2690 - accuracy: 0.09 - ETA: 4:55 - loss: 1.2804 - accuracy: 0.09 - ETA: 4:53 - loss: 1.2870 - accuracy: 0.09 - ETA: 4:52 - loss: 1.2853 - accuracy: 0.09 - ETA: 4:51 - loss: 1.2819 - accuracy: 0.09 - ETA: 4:50 - loss: 1.2814 - accuracy: 0.09 - ETA: 4:49 - loss: 1.2799 - accuracy: 0.09 - ETA: 4:48 - loss: 1.2771 - accuracy: 0.09 - ETA: 4:47 - loss: 1.2780 - accuracy: 0.09 - ETA: 4:45 - loss: 1.2746 - accuracy: 0.09 - ETA: 4:45 - loss: 1.2773 - accuracy: 0.09 - ETA: 4:44 - loss: 1.2801 - accuracy: 0.09 - ETA: 4:42 - loss: 1.2787 - accuracy: 0.09 - ETA: 4:41 - loss: 1.2812 - accuracy: 0.09 - ETA: 4:40 - loss: 1.2783 - accuracy: 0.09 - ETA: 4:39 - loss: 1.2805 - accuracy: 0.09 - ETA: 4:38 - loss: 1.2804 - accuracy: 0.09 - ETA: 4:37 - loss: 1.2785 - accuracy: 0.09 - ETA: 4:36 - loss: 1.2773 - accuracy: 0.09 - ETA: 4:34 - loss: 1.2809 - accuracy: 0.09 - ETA: 4:34 - loss: 1.2857 - accuracy: 0.09 - ETA: 4:33 - loss: 1.2813 - accuracy: 0.09 - ETA: 4:32 - loss: 1.2792 - accuracy: 0.09 - ETA: 4:31 - loss: 1.2795 - accuracy: 0.09 - ETA: 4:30 - loss: 1.2783 - accuracy: 0.09 - ETA: 4:29 - loss: 1.2762 - accuracy: 0.09 - ETA: 4:28 - loss: 1.2753 - accuracy: 0.09 - ETA: 4:27 - loss: 1.2763 - accuracy: 0.09 - ETA: 4:26 - loss: 1.2701 - accuracy: 0.09 - ETA: 4:25 - loss: 1.2707 - accuracy: 0.09 - ETA: 4:24 - loss: 1.2720 - accuracy: 0.09 - ETA: 4:23 - loss: 1.2703 - accuracy: 0.09 - ETA: 4:22 - loss: 1.2710 - accuracy: 0.09 - ETA: 4:21 - loss: 1.2710 - accuracy: 0.09 - ETA: 4:20 - loss: 1.2704 - accuracy: 0.09 - ETA: 4:19 - loss: 1.2651 - accuracy: 0.09 - ETA: 4:18 - loss: 1.2665 - accuracy: 0.09 - ETA: 4:17 - loss: 1.2672 - accuracy: 0.09 - ETA: 4:16 - loss: 1.2667 - accuracy: 0.09 - ETA: 4:15 - loss: 1.2692 - accuracy: 0.09 - ETA: 4:14 - loss: 1.2699 - accuracy: 0.09 - ETA: 4:13 - loss: 1.2698 - accuracy: 0.09 - ETA: 4:12 - loss: 1.2711 - accuracy: 0.09 - ETA: 4:11 - loss: 1.2712 - accuracy: 0.09 - ETA: 4:10 - loss: 1.2691 - accuracy: 0.09 - ETA: 4:09 - loss: 1.2694 - accuracy: 0.09 - ETA: 4:08 - loss: 1.2684 - accuracy: 0.09 - ETA: 4:07 - loss: 1.2664 - accuracy: 0.09 - ETA: 4:06 - loss: 1.2653 - accuracy: 0.09 - ETA: 4:05 - loss: 1.2634 - accuracy: 0.09 - ETA: 4:05 - loss: 1.2639 - accuracy: 0.09 - ETA: 4:03 - loss: 1.2637 - accuracy: 0.09 - ETA: 4:03 - loss: 1.2632 - accuracy: 0.09 - ETA: 4:01 - loss: 1.2632 - accuracy: 0.09 - ETA: 4:01 - loss: 1.2655 - accuracy: 0.09 - ETA: 4:00 - loss: 1.2685 - accuracy: 0.09 - ETA: 3:59 - loss: 1.2687 - accuracy: 0.09 - ETA: 3:58 - loss: 1.2695 - accuracy: 0.09 - ETA: 3:57 - loss: 1.2683 - accuracy: 0.09 - ETA: 3:56 - loss: 1.2684 - accuracy: 0.09 - ETA: 3:55 - loss: 1.2687 - accuracy: 0.09 - ETA: 3:54 - loss: 1.2688 - accuracy: 0.09 - ETA: 3:53 - loss: 1.2687 - accuracy: 0.09 - ETA: 3:52 - loss: 1.2703 - accuracy: 0.09 - ETA: 3:51 - loss: 1.2722 - accuracy: 0.09 - ETA: 3:50 - loss: 1.2736 - accuracy: 0.09 - ETA: 3:49 - loss: 1.2741 - accuracy: 0.09 - ETA: 3:48 - loss: 1.2752 - accuracy: 0.09 - ETA: 3:47 - loss: 1.2753 - accuracy: 0.09 - ETA: 3:46 - loss: 1.2753 - accuracy: 0.09 - ETA: 3:45 - loss: 1.2767 - accuracy: 0.09 - ETA: 3:44 - loss: 1.2754 - accuracy: 0.09 - ETA: 3:43 - loss: 1.2762 - accuracy: 0.09 - ETA: 3:42 - loss: 1.2752 - accuracy: 0.09 - ETA: 3:42 - loss: 1.2768 - accuracy: 0.09 - ETA: 3:41 - loss: 1.2784 - accuracy: 0.09 - ETA: 3:40 - loss: 1.2784 - accuracy: 0.09 - ETA: 3:39 - loss: 1.2791 - accuracy: 0.09 - ETA: 3:38 - loss: 1.2793 - accuracy: 0.09 - ETA: 3:37 - loss: 1.2780 - accuracy: 0.09 - ETA: 3:36 - loss: 1.2767 - accuracy: 0.09 - ETA: 3:35 - loss: 1.2752 - accuracy: 0.09 - ETA: 3:34 - loss: 1.2754 - accuracy: 0.09 - ETA: 3:33 - loss: 1.2751 - accuracy: 0.09 - ETA: 3:32 - loss: 1.2767 - accuracy: 0.09 - ETA: 3:31 - loss: 1.2780 - accuracy: 0.09 - ETA: 3:30 - loss: 1.2768 - accuracy: 0.09 - ETA: 3:29 - loss: 1.2761 - accuracy: 0.09 - ETA: 3:29 - loss: 1.2776 - accuracy: 0.09 - ETA: 3:28 - loss: 1.2768 - accuracy: 0.09 - ETA: 3:27 - loss: 1.2789 - accuracy: 0.09 - ETA: 3:26 - loss: 1.2793 - accuracy: 0.09 - ETA: 3:25 - loss: 1.2807 - accuracy: 0.09 - ETA: 3:24 - loss: 1.2798 - accuracy: 0.09 - ETA: 3:23 - loss: 1.2802 - accuracy: 0.09 - ETA: 3:22 - loss: 1.2792 - accuracy: 0.09 - ETA: 3:21 - loss: 1.2781 - accuracy: 0.09 - ETA: 3:20 - loss: 1.2773 - accuracy: 0.09 - ETA: 3:19 - loss: 1.2786 - accuracy: 0.09 - ETA: 3:18 - loss: 1.2761 - accuracy: 0.09 - ETA: 3:17 - loss: 1.2765 - accuracy: 0.09 - ETA: 3:16 - loss: 1.2760 - accuracy: 0.09 - ETA: 3:16 - loss: 1.2757 - accuracy: 0.09 - ETA: 3:15 - loss: 1.2758 - accuracy: 0.09 - ETA: 3:14 - loss: 1.2752 - accuracy: 0.09 - ETA: 3:13 - loss: 1.2746 - accuracy: 0.09 - ETA: 3:12 - loss: 1.2754 - accuracy: 0.09 - ETA: 3:11 - loss: 1.2750 - accuracy: 0.09 - ETA: 3:10 - loss: 1.2750 - accuracy: 0.09 - ETA: 3:09 - loss: 1.2755 - accuracy: 0.09 - ETA: 3:08 - loss: 1.2758 - accuracy: 0.09 - ETA: 3:07 - loss: 1.2765 - accuracy: 0.09 - ETA: 3:06 - loss: 1.2791 - accuracy: 0.09 - ETA: 3:05 - loss: 1.2775 - accuracy: 0.09 - ETA: 3:04 - loss: 1.2776 - accuracy: 0.09 - ETA: 3:04 - loss: 1.2766 - accuracy: 0.09 - ETA: 3:03 - loss: 1.2758 - accuracy: 0.09 - ETA: 3:02 - loss: 1.2765 - accuracy: 0.09 - ETA: 3:01 - loss: 1.2758 - accuracy: 0.09 - ETA: 3:00 - loss: 1.2761 - accuracy: 0.09 - ETA: 2:59 - loss: 1.2761 - accuracy: 0.09 - ETA: 2:58 - loss: 1.2762 - accuracy: 0.09 - ETA: 2:57 - loss: 1.2747 - accuracy: 0.09 - ETA: 2:56 - loss: 1.2737 - accuracy: 0.09 - ETA: 2:55 - loss: 1.2746 - accuracy: 0.09 - ETA: 2:54 - loss: 1.2736 - accuracy: 0.09 - ETA: 2:53 - loss: 1.2709 - accuracy: 0.09 - ETA: 2:53 - loss: 1.2706 - accuracy: 0.09 - ETA: 2:52 - loss: 1.2707 - accuracy: 0.09 - ETA: 2:51 - loss: 1.2702 - accuracy: 0.09 - ETA: 2:50 - loss: 1.2702 - accuracy: 0.09 - ETA: 2:49 - loss: 1.2701 - accuracy: 0.09 - ETA: 2:48 - loss: 1.2699 - accuracy: 0.09 - ETA: 2:47 - loss: 1.2699 - accuracy: 0.09 - ETA: 2:46 - loss: 1.2702 - accuracy: 0.09 - ETA: 2:45 - loss: 1.2712 - accuracy: 0.09 - ETA: 2:44 - loss: 1.2710 - accuracy: 0.09 - ETA: 2:43 - loss: 1.2713 - accuracy: 0.09 - ETA: 2:42 - loss: 1.2714 - accuracy: 0.09 - ETA: 2:42 - loss: 1.2708 - accuracy: 0.09 - ETA: 2:41 - loss: 1.2696 - accuracy: 0.09 - ETA: 2:40 - loss: 1.2707 - accuracy: 0.09 - ETA: 2:39 - loss: 1.2704 - accuracy: 0.09 - ETA: 2:38 - loss: 1.2706 - accuracy: 0.09 - ETA: 2:37 - loss: 1.2706 - accuracy: 0.09 - ETA: 2:36 - loss: 1.2703 - accuracy: 0.09 - ETA: 2:35 - loss: 1.2705 - accuracy: 0.09 - ETA: 2:34 - loss: 1.2707 - accuracy: 0.09 - ETA: 2:33 - loss: 1.2711 - accuracy: 0.09 - ETA: 2:32 - loss: 1.2695 - accuracy: 0.09 - ETA: 2:31 - loss: 1.2690 - accuracy: 0.09 - ETA: 2:30 - loss: 1.2694 - accuracy: 0.09 - ETA: 2:30 - loss: 1.2689 - accuracy: 0.09 - ETA: 2:29 - loss: 1.2691 - accuracy: 0.09 - ETA: 2:28 - loss: 1.2697 - accuracy: 0.09 - ETA: 2:27 - loss: 1.2689 - accuracy: 0.09 - ETA: 2:26 - loss: 1.2697 - accuracy: 0.09 - ETA: 2:25 - loss: 1.2690 - accuracy: 0.09 - ETA: 2:24 - loss: 1.2679 - accuracy: 0.0969"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - ETA: 2:23 - loss: 1.2694 - accuracy: 0.09 - ETA: 2:22 - loss: 1.2698 - accuracy: 0.09 - ETA: 2:21 - loss: 1.2697 - accuracy: 0.09 - ETA: 2:21 - loss: 1.2689 - accuracy: 0.09 - ETA: 2:20 - loss: 1.2689 - accuracy: 0.09 - ETA: 2:19 - loss: 1.2687 - accuracy: 0.09 - ETA: 2:18 - loss: 1.2687 - accuracy: 0.09 - ETA: 2:17 - loss: 1.2682 - accuracy: 0.09 - ETA: 2:16 - loss: 1.2688 - accuracy: 0.09 - ETA: 2:15 - loss: 1.2684 - accuracy: 0.09 - ETA: 2:14 - loss: 1.2690 - accuracy: 0.09 - ETA: 2:13 - loss: 1.2685 - accuracy: 0.09 - ETA: 2:12 - loss: 1.2687 - accuracy: 0.09 - ETA: 2:11 - loss: 1.2697 - accuracy: 0.09 - ETA: 2:11 - loss: 1.2697 - accuracy: 0.09 - ETA: 2:10 - loss: 1.2694 - accuracy: 0.09 - ETA: 2:09 - loss: 1.2688 - accuracy: 0.09 - ETA: 2:08 - loss: 1.2679 - accuracy: 0.09 - ETA: 2:07 - loss: 1.2674 - accuracy: 0.09 - ETA: 2:06 - loss: 1.2675 - accuracy: 0.09 - ETA: 2:05 - loss: 1.2678 - accuracy: 0.09 - ETA: 2:04 - loss: 1.2677 - accuracy: 0.09 - ETA: 2:03 - loss: 1.2682 - accuracy: 0.09 - ETA: 2:02 - loss: 1.2692 - accuracy: 0.09 - ETA: 2:01 - loss: 1.2692 - accuracy: 0.09 - ETA: 2:00 - loss: 1.2689 - accuracy: 0.09 - ETA: 2:00 - loss: 1.2690 - accuracy: 0.09 - ETA: 1:59 - loss: 1.2689 - accuracy: 0.09 - ETA: 1:58 - loss: 1.2693 - accuracy: 0.09 - ETA: 1:57 - loss: 1.2690 - accuracy: 0.09 - ETA: 1:56 - loss: 1.2710 - accuracy: 0.09 - ETA: 1:55 - loss: 1.2711 - accuracy: 0.09 - ETA: 1:54 - loss: 1.2705 - accuracy: 0.09 - ETA: 1:53 - loss: 1.2711 - accuracy: 0.09 - ETA: 1:52 - loss: 1.2717 - accuracy: 0.09 - ETA: 1:51 - loss: 1.2710 - accuracy: 0.09 - ETA: 1:50 - loss: 1.2709 - accuracy: 0.09 - ETA: 1:49 - loss: 1.2710 - accuracy: 0.09 - ETA: 1:49 - loss: 1.2711 - accuracy: 0.09 - ETA: 1:48 - loss: 1.2709 - accuracy: 0.09 - ETA: 1:47 - loss: 1.2708 - accuracy: 0.09 - ETA: 1:46 - loss: 1.2708 - accuracy: 0.09 - ETA: 1:45 - loss: 1.2707 - accuracy: 0.09 - ETA: 1:44 - loss: 1.2711 - accuracy: 0.09 - ETA: 1:43 - loss: 1.2719 - accuracy: 0.09 - ETA: 1:42 - loss: 1.2719 - accuracy: 0.09 - ETA: 1:41 - loss: 1.2715 - accuracy: 0.09 - ETA: 1:40 - loss: 1.2723 - accuracy: 0.09 - ETA: 1:39 - loss: 1.2720 - accuracy: 0.09 - ETA: 1:39 - loss: 1.2717 - accuracy: 0.09 - ETA: 1:38 - loss: 1.2723 - accuracy: 0.09 - ETA: 1:37 - loss: 1.2738 - accuracy: 0.09 - ETA: 1:36 - loss: 1.2731 - accuracy: 0.09 - ETA: 1:35 - loss: 1.2724 - accuracy: 0.09 - ETA: 1:34 - loss: 1.2725 - accuracy: 0.09 - ETA: 1:33 - loss: 1.2733 - accuracy: 0.09 - ETA: 1:32 - loss: 1.2728 - accuracy: 0.09 - ETA: 1:31 - loss: 1.2729 - accuracy: 0.09 - ETA: 1:30 - loss: 1.2734 - accuracy: 0.09 - ETA: 1:29 - loss: 1.2735 - accuracy: 0.09 - ETA: 1:29 - loss: 1.2730 - accuracy: 0.09 - ETA: 1:28 - loss: 1.2736 - accuracy: 0.09 - ETA: 1:27 - loss: 1.2734 - accuracy: 0.09 - ETA: 1:26 - loss: 1.2727 - accuracy: 0.09 - ETA: 1:25 - loss: 1.2731 - accuracy: 0.09 - ETA: 1:24 - loss: 1.2737 - accuracy: 0.09 - ETA: 1:23 - loss: 1.2737 - accuracy: 0.09 - ETA: 1:22 - loss: 1.2737 - accuracy: 0.09 - ETA: 1:21 - loss: 1.2733 - accuracy: 0.09 - ETA: 1:20 - loss: 1.2736 - accuracy: 0.09 - ETA: 1:19 - loss: 1.2737 - accuracy: 0.09 - ETA: 1:18 - loss: 1.2735 - accuracy: 0.09 - ETA: 1:18 - loss: 1.2732 - accuracy: 0.09 - ETA: 1:17 - loss: 1.2735 - accuracy: 0.09 - ETA: 1:16 - loss: 1.2741 - accuracy: 0.09 - ETA: 1:15 - loss: 1.2739 - accuracy: 0.09 - ETA: 1:14 - loss: 1.2745 - accuracy: 0.09 - ETA: 1:13 - loss: 1.2748 - accuracy: 0.09 - ETA: 1:12 - loss: 1.2753 - accuracy: 0.09 - ETA: 1:11 - loss: 1.2749 - accuracy: 0.09 - ETA: 1:10 - loss: 1.2745 - accuracy: 0.09 - ETA: 1:09 - loss: 1.2744 - accuracy: 0.09 - ETA: 1:08 - loss: 1.2744 - accuracy: 0.09 - ETA: 1:08 - loss: 1.2748 - accuracy: 0.09 - ETA: 1:07 - loss: 1.2750 - accuracy: 0.09 - ETA: 1:06 - loss: 1.2747 - accuracy: 0.09 - ETA: 1:05 - loss: 1.2748 - accuracy: 0.09 - ETA: 1:04 - loss: 1.2743 - accuracy: 0.09 - ETA: 1:03 - loss: 1.2751 - accuracy: 0.09 - ETA: 1:02 - loss: 1.2748 - accuracy: 0.09 - ETA: 1:01 - loss: 1.2753 - accuracy: 0.09 - ETA: 1:00 - loss: 1.2750 - accuracy: 0.09 - ETA: 59s - loss: 1.2743 - accuracy: 0.0966 - ETA: 58s - loss: 1.2743 - accuracy: 0.096 - ETA: 58s - loss: 1.2744 - accuracy: 0.096 - ETA: 57s - loss: 1.2745 - accuracy: 0.096 - ETA: 56s - loss: 1.2740 - accuracy: 0.096 - ETA: 55s - loss: 1.2737 - accuracy: 0.096 - ETA: 54s - loss: 1.2730 - accuracy: 0.096 - ETA: 53s - loss: 1.2733 - accuracy: 0.096 - ETA: 52s - loss: 1.2734 - accuracy: 0.096 - ETA: 51s - loss: 1.2734 - accuracy: 0.096 - ETA: 50s - loss: 1.2739 - accuracy: 0.096 - ETA: 49s - loss: 1.2737 - accuracy: 0.096 - ETA: 49s - loss: 1.2737 - accuracy: 0.096 - ETA: 48s - loss: 1.2743 - accuracy: 0.096 - ETA: 47s - loss: 1.2747 - accuracy: 0.096 - ETA: 46s - loss: 1.2748 - accuracy: 0.096 - ETA: 45s - loss: 1.2749 - accuracy: 0.096 - ETA: 44s - loss: 1.2751 - accuracy: 0.096 - ETA: 43s - loss: 1.2751 - accuracy: 0.096 - ETA: 42s - loss: 1.2756 - accuracy: 0.096 - ETA: 41s - loss: 1.2766 - accuracy: 0.096 - ETA: 40s - loss: 1.2768 - accuracy: 0.096 - ETA: 39s - loss: 1.2761 - accuracy: 0.096 - ETA: 39s - loss: 1.2755 - accuracy: 0.096 - ETA: 38s - loss: 1.2752 - accuracy: 0.096 - ETA: 37s - loss: 1.2750 - accuracy: 0.096 - ETA: 36s - loss: 1.2750 - accuracy: 0.096 - ETA: 35s - loss: 1.2750 - accuracy: 0.096 - ETA: 34s - loss: 1.2749 - accuracy: 0.096 - ETA: 33s - loss: 1.2747 - accuracy: 0.096 - ETA: 32s - loss: 1.2752 - accuracy: 0.096 - ETA: 31s - loss: 1.2755 - accuracy: 0.096 - ETA: 30s - loss: 1.2757 - accuracy: 0.096 - ETA: 29s - loss: 1.2757 - accuracy: 0.096 - ETA: 29s - loss: 1.2762 - accuracy: 0.096 - ETA: 28s - loss: 1.2761 - accuracy: 0.096 - ETA: 27s - loss: 1.2759 - accuracy: 0.096 - ETA: 26s - loss: 1.2758 - accuracy: 0.096 - ETA: 25s - loss: 1.2754 - accuracy: 0.096 - ETA: 24s - loss: 1.2763 - accuracy: 0.096 - ETA: 23s - loss: 1.2762 - accuracy: 0.096 - ETA: 22s - loss: 1.2765 - accuracy: 0.096 - ETA: 21s - loss: 1.2769 - accuracy: 0.096 - ETA: 20s - loss: 1.2776 - accuracy: 0.096 - ETA: 19s - loss: 1.2772 - accuracy: 0.096 - ETA: 19s - loss: 1.2769 - accuracy: 0.096 - ETA: 18s - loss: 1.2766 - accuracy: 0.096 - ETA: 17s - loss: 1.2776 - accuracy: 0.096 - ETA: 16s - loss: 1.2780 - accuracy: 0.096 - ETA: 15s - loss: 1.2775 - accuracy: 0.096 - ETA: 14s - loss: 1.2778 - accuracy: 0.096 - ETA: 13s - loss: 1.2778 - accuracy: 0.096 - ETA: 12s - loss: 1.2781 - accuracy: 0.096 - ETA: 11s - loss: 1.2785 - accuracy: 0.096 - ETA: 10s - loss: 1.2787 - accuracy: 0.096 - ETA: 9s - loss: 1.2792 - accuracy: 0.096 - ETA: 9s - loss: 1.2798 - accuracy: 0.09 - ETA: 8s - loss: 1.2797 - accuracy: 0.09 - ETA: 7s - loss: 1.2801 - accuracy: 0.09 - ETA: 6s - loss: 1.2801 - accuracy: 0.09 - ETA: 5s - loss: 1.2807 - accuracy: 0.09 - ETA: 4s - loss: 1.2801 - accuracy: 0.09 - ETA: 3s - loss: 1.2805 - accuracy: 0.09 - ETA: 2s - loss: 1.2801 - accuracy: 0.09 - ETA: 1s - loss: 1.2801 - accuracy: 0.09 - ETA: 0s - loss: 1.2808 - accuracy: 0.09 - 313s 907ms/step - loss: 1.2808 - accuracy: 0.0964\n",
      "Epoch 8/20\n",
      "186/345 [===============>..............] - ETA: 5:38 - loss: 0.9709 - accuracy: 0.09 - ETA: 5:17 - loss: 0.9821 - accuracy: 0.09 - ETA: 5:11 - loss: 1.0193 - accuracy: 0.09 - ETA: 5:10 - loss: 1.0973 - accuracy: 0.10 - ETA: 5:14 - loss: 1.1184 - accuracy: 0.10 - ETA: 5:10 - loss: 1.1271 - accuracy: 0.10 - ETA: 5:07 - loss: 1.1218 - accuracy: 0.10 - ETA: 5:05 - loss: 1.1172 - accuracy: 0.10 - ETA: 5:05 - loss: 1.1331 - accuracy: 0.10 - ETA: 5:04 - loss: 1.1533 - accuracy: 0.10 - ETA: 5:03 - loss: 1.1671 - accuracy: 0.10 - ETA: 5:02 - loss: 1.1764 - accuracy: 0.10 - ETA: 5:00 - loss: 1.1825 - accuracy: 0.10 - ETA: 4:58 - loss: 1.1839 - accuracy: 0.10 - ETA: 4:58 - loss: 1.1843 - accuracy: 0.10 - ETA: 4:57 - loss: 1.1814 - accuracy: 0.10 - ETA: 4:56 - loss: 1.1863 - accuracy: 0.10 - ETA: 4:55 - loss: 1.1810 - accuracy: 0.10 - ETA: 4:54 - loss: 1.1818 - accuracy: 0.10 - ETA: 4:55 - loss: 1.1815 - accuracy: 0.10 - ETA: 4:53 - loss: 1.1807 - accuracy: 0.10 - ETA: 4:52 - loss: 1.1796 - accuracy: 0.10 - ETA: 4:51 - loss: 1.1821 - accuracy: 0.10 - ETA: 4:51 - loss: 1.1792 - accuracy: 0.10 - ETA: 4:50 - loss: 1.1801 - accuracy: 0.10 - ETA: 4:49 - loss: 1.1863 - accuracy: 0.10 - ETA: 4:47 - loss: 1.1846 - accuracy: 0.10 - ETA: 4:47 - loss: 1.1845 - accuracy: 0.10 - ETA: 4:46 - loss: 1.1864 - accuracy: 0.10 - ETA: 4:45 - loss: 1.1855 - accuracy: 0.10 - ETA: 4:44 - loss: 1.1858 - accuracy: 0.10 - ETA: 4:43 - loss: 1.1871 - accuracy: 0.10 - ETA: 4:42 - loss: 1.1896 - accuracy: 0.10 - ETA: 4:41 - loss: 1.1917 - accuracy: 0.10 - ETA: 4:40 - loss: 1.1922 - accuracy: 0.10 - ETA: 4:39 - loss: 1.1935 - accuracy: 0.10 - ETA: 4:38 - loss: 1.1905 - accuracy: 0.10 - ETA: 4:38 - loss: 1.1924 - accuracy: 0.10 - ETA: 4:37 - loss: 1.1924 - accuracy: 0.10 - ETA: 4:36 - loss: 1.1857 - accuracy: 0.10 - ETA: 4:35 - loss: 1.1876 - accuracy: 0.10 - ETA: 4:34 - loss: 1.1875 - accuracy: 0.10 - ETA: 4:33 - loss: 1.1882 - accuracy: 0.10 - ETA: 4:32 - loss: 1.1854 - accuracy: 0.10 - ETA: 4:31 - loss: 1.1893 - accuracy: 0.10 - ETA: 4:30 - loss: 1.1924 - accuracy: 0.10 - ETA: 4:29 - loss: 1.1944 - accuracy: 0.10 - ETA: 4:29 - loss: 1.1959 - accuracy: 0.10 - ETA: 4:28 - loss: 1.1981 - accuracy: 0.10 - ETA: 4:27 - loss: 1.2031 - accuracy: 0.10 - ETA: 4:26 - loss: 1.2032 - accuracy: 0.10 - ETA: 4:25 - loss: 1.2027 - accuracy: 0.10 - ETA: 4:24 - loss: 1.1998 - accuracy: 0.10 - ETA: 4:23 - loss: 1.2050 - accuracy: 0.10 - ETA: 4:22 - loss: 1.2052 - accuracy: 0.10 - ETA: 4:21 - loss: 1.2046 - accuracy: 0.10 - ETA: 4:20 - loss: 1.2055 - accuracy: 0.10 - ETA: 4:19 - loss: 1.2104 - accuracy: 0.10 - ETA: 4:19 - loss: 1.2075 - accuracy: 0.10 - ETA: 4:18 - loss: 1.2070 - accuracy: 0.10 - ETA: 4:17 - loss: 1.2083 - accuracy: 0.10 - ETA: 4:16 - loss: 1.2062 - accuracy: 0.10 - ETA: 4:15 - loss: 1.2057 - accuracy: 0.10 - ETA: 4:14 - loss: 1.2042 - accuracy: 0.10 - ETA: 4:13 - loss: 1.2051 - accuracy: 0.10 - ETA: 4:12 - loss: 1.2039 - accuracy: 0.10 - ETA: 4:11 - loss: 1.2005 - accuracy: 0.10 - ETA: 4:11 - loss: 1.2001 - accuracy: 0.10 - ETA: 4:10 - loss: 1.2014 - accuracy: 0.10 - ETA: 4:09 - loss: 1.1981 - accuracy: 0.10 - ETA: 4:08 - loss: 1.1979 - accuracy: 0.10 - ETA: 4:07 - loss: 1.1979 - accuracy: 0.10 - ETA: 4:06 - loss: 1.1977 - accuracy: 0.10 - ETA: 4:05 - loss: 1.1986 - accuracy: 0.10 - ETA: 4:04 - loss: 1.2012 - accuracy: 0.10 - ETA: 4:03 - loss: 1.2005 - accuracy: 0.10 - ETA: 4:02 - loss: 1.1983 - accuracy: 0.10 - ETA: 4:01 - loss: 1.1969 - accuracy: 0.10 - ETA: 4:01 - loss: 1.1961 - accuracy: 0.10 - ETA: 4:00 - loss: 1.1980 - accuracy: 0.10 - ETA: 3:59 - loss: 1.1959 - accuracy: 0.10 - ETA: 3:58 - loss: 1.1928 - accuracy: 0.10 - ETA: 3:57 - loss: 1.1952 - accuracy: 0.10 - ETA: 3:56 - loss: 1.1964 - accuracy: 0.10 - ETA: 3:55 - loss: 1.1970 - accuracy: 0.10 - ETA: 3:54 - loss: 1.1973 - accuracy: 0.10 - ETA: 3:53 - loss: 1.1996 - accuracy: 0.10 - ETA: 3:52 - loss: 1.1977 - accuracy: 0.10 - ETA: 3:51 - loss: 1.1994 - accuracy: 0.10 - ETA: 3:51 - loss: 1.2003 - accuracy: 0.10 - ETA: 3:50 - loss: 1.2016 - accuracy: 0.10 - ETA: 3:49 - loss: 1.1985 - accuracy: 0.10 - ETA: 3:48 - loss: 1.1972 - accuracy: 0.10 - ETA: 3:47 - loss: 1.1958 - accuracy: 0.10 - ETA: 3:46 - loss: 1.2000 - accuracy: 0.10 - ETA: 3:45 - loss: 1.1998 - accuracy: 0.10 - ETA: 3:44 - loss: 1.1998 - accuracy: 0.10 - ETA: 3:43 - loss: 1.1985 - accuracy: 0.10 - ETA: 3:42 - loss: 1.1979 - accuracy: 0.10 - ETA: 3:41 - loss: 1.1986 - accuracy: 0.10 - ETA: 3:40 - loss: 1.1968 - accuracy: 0.10 - ETA: 3:39 - loss: 1.1964 - accuracy: 0.10 - ETA: 3:39 - loss: 1.1957 - accuracy: 0.10 - ETA: 3:38 - loss: 1.1955 - accuracy: 0.10 - ETA: 3:37 - loss: 1.1969 - accuracy: 0.10 - ETA: 3:36 - loss: 1.1960 - accuracy: 0.10 - ETA: 3:35 - loss: 1.1962 - accuracy: 0.10 - ETA: 3:34 - loss: 1.1959 - accuracy: 0.10 - ETA: 3:33 - loss: 1.1966 - accuracy: 0.10 - ETA: 3:32 - loss: 1.1939 - accuracy: 0.10 - ETA: 3:31 - loss: 1.1930 - accuracy: 0.10 - ETA: 3:30 - loss: 1.1934 - accuracy: 0.10 - ETA: 3:30 - loss: 1.1916 - accuracy: 0.10 - ETA: 3:29 - loss: 1.1897 - accuracy: 0.10 - ETA: 3:28 - loss: 1.1904 - accuracy: 0.10 - ETA: 3:27 - loss: 1.1894 - accuracy: 0.10 - ETA: 3:26 - loss: 1.1902 - accuracy: 0.10 - ETA: 3:25 - loss: 1.1903 - accuracy: 0.10 - ETA: 3:24 - loss: 1.1898 - accuracy: 0.10 - ETA: 3:23 - loss: 1.1889 - accuracy: 0.10 - ETA: 3:22 - loss: 1.1892 - accuracy: 0.10 - ETA: 3:21 - loss: 1.1902 - accuracy: 0.10 - ETA: 3:21 - loss: 1.1890 - accuracy: 0.10 - ETA: 3:20 - loss: 1.1887 - accuracy: 0.10 - ETA: 3:19 - loss: 1.1897 - accuracy: 0.10 - ETA: 3:18 - loss: 1.1894 - accuracy: 0.10 - ETA: 3:17 - loss: 1.1901 - accuracy: 0.10 - ETA: 3:16 - loss: 1.1892 - accuracy: 0.10 - ETA: 3:15 - loss: 1.1908 - accuracy: 0.10 - ETA: 3:14 - loss: 1.1912 - accuracy: 0.10 - ETA: 3:13 - loss: 1.1919 - accuracy: 0.10 - ETA: 3:12 - loss: 1.1923 - accuracy: 0.10 - ETA: 3:11 - loss: 1.1941 - accuracy: 0.10 - ETA: 3:11 - loss: 1.1936 - accuracy: 0.10 - ETA: 3:10 - loss: 1.1944 - accuracy: 0.10 - ETA: 3:09 - loss: 1.1939 - accuracy: 0.10 - ETA: 3:08 - loss: 1.1922 - accuracy: 0.10 - ETA: 3:07 - loss: 1.1916 - accuracy: 0.10 - ETA: 3:06 - loss: 1.1906 - accuracy: 0.10 - ETA: 3:05 - loss: 1.1909 - accuracy: 0.10 - ETA: 3:04 - loss: 1.1906 - accuracy: 0.10 - ETA: 3:03 - loss: 1.1921 - accuracy: 0.10 - ETA: 3:02 - loss: 1.1920 - accuracy: 0.10 - ETA: 3:01 - loss: 1.1923 - accuracy: 0.10 - ETA: 3:00 - loss: 1.1925 - accuracy: 0.10 - ETA: 3:00 - loss: 1.1936 - accuracy: 0.10 - ETA: 2:59 - loss: 1.1950 - accuracy: 0.10 - ETA: 2:58 - loss: 1.1955 - accuracy: 0.10 - ETA: 2:57 - loss: 1.1951 - accuracy: 0.10 - ETA: 2:56 - loss: 1.1967 - accuracy: 0.10 - ETA: 2:55 - loss: 1.1970 - accuracy: 0.10 - ETA: 2:54 - loss: 1.1978 - accuracy: 0.10 - ETA: 2:53 - loss: 1.1965 - accuracy: 0.10 - ETA: 2:52 - loss: 1.1966 - accuracy: 0.10 - ETA: 2:51 - loss: 1.1958 - accuracy: 0.10 - ETA: 2:50 - loss: 1.1964 - accuracy: 0.10 - ETA: 2:50 - loss: 1.1977 - accuracy: 0.10 - ETA: 2:49 - loss: 1.1980 - accuracy: 0.10 - ETA: 2:48 - loss: 1.1984 - accuracy: 0.10 - ETA: 2:47 - loss: 1.1985 - accuracy: 0.10 - ETA: 2:46 - loss: 1.1989 - accuracy: 0.10 - ETA: 2:45 - loss: 1.1999 - accuracy: 0.10 - ETA: 2:44 - loss: 1.2005 - accuracy: 0.10 - ETA: 2:43 - loss: 1.2014 - accuracy: 0.10 - ETA: 2:42 - loss: 1.2008 - accuracy: 0.10 - ETA: 2:41 - loss: 1.2009 - accuracy: 0.10 - ETA: 2:41 - loss: 1.2008 - accuracy: 0.10 - ETA: 2:40 - loss: 1.2001 - accuracy: 0.10 - ETA: 2:39 - loss: 1.2004 - accuracy: 0.10 - ETA: 2:38 - loss: 1.1999 - accuracy: 0.10 - ETA: 2:37 - loss: 1.2001 - accuracy: 0.10 - ETA: 2:36 - loss: 1.2012 - accuracy: 0.10 - ETA: 2:35 - loss: 1.2012 - accuracy: 0.10 - ETA: 2:34 - loss: 1.2018 - accuracy: 0.10 - ETA: 2:33 - loss: 1.2030 - accuracy: 0.10 - ETA: 2:32 - loss: 1.2038 - accuracy: 0.10 - ETA: 2:31 - loss: 1.2033 - accuracy: 0.10 - ETA: 2:31 - loss: 1.2032 - accuracy: 0.10 - ETA: 2:30 - loss: 1.2024 - accuracy: 0.10 - ETA: 2:29 - loss: 1.2024 - accuracy: 0.10 - ETA: 2:28 - loss: 1.2028 - accuracy: 0.10 - ETA: 2:27 - loss: 1.2027 - accuracy: 0.10 - ETA: 2:26 - loss: 1.2042 - accuracy: 0.10 - ETA: 2:25 - loss: 1.2037 - accuracy: 0.10 - ETA: 2:24 - loss: 1.2046 - accuracy: 0.10 - ETA: 2:23 - loss: 1.2032 - accuracy: 0.1037254/345 [=====================>........] - ETA: 2:22 - loss: 1.2029 - accuracy: 0.10 - ETA: 2:22 - loss: 1.2029 - accuracy: 0.10 - ETA: 2:21 - loss: 1.2030 - accuracy: 0.10 - ETA: 2:20 - loss: 1.2027 - accuracy: 0.10 - ETA: 2:19 - loss: 1.2040 - accuracy: 0.10 - ETA: 2:18 - loss: 1.2050 - accuracy: 0.10 - ETA: 2:17 - loss: 1.2053 - accuracy: 0.10 - ETA: 2:16 - loss: 1.2054 - accuracy: 0.10 - ETA: 2:15 - loss: 1.2053 - accuracy: 0.10 - ETA: 2:14 - loss: 1.2045 - accuracy: 0.10 - ETA: 2:13 - loss: 1.2042 - accuracy: 0.10 - ETA: 2:12 - loss: 1.2044 - accuracy: 0.10 - ETA: 2:12 - loss: 1.2045 - accuracy: 0.10 - ETA: 2:11 - loss: 1.2033 - accuracy: 0.10 - ETA: 2:10 - loss: 1.2033 - accuracy: 0.10 - ETA: 2:09 - loss: 1.2042 - accuracy: 0.10 - ETA: 2:08 - loss: 1.2046 - accuracy: 0.10 - ETA: 2:07 - loss: 1.2043 - accuracy: 0.10 - ETA: 2:06 - loss: 1.2045 - accuracy: 0.10 - ETA: 2:05 - loss: 1.2043 - accuracy: 0.10 - ETA: 2:04 - loss: 1.2037 - accuracy: 0.10 - ETA: 2:04 - loss: 1.2052 - accuracy: 0.10 - ETA: 2:03 - loss: 1.2062 - accuracy: 0.10 - ETA: 2:02 - loss: 1.2072 - accuracy: 0.10 - ETA: 2:01 - loss: 1.2064 - accuracy: 0.10 - ETA: 2:00 - loss: 1.2058 - accuracy: 0.10 - ETA: 1:59 - loss: 1.2046 - accuracy: 0.10 - ETA: 1:59 - loss: 1.2048 - accuracy: 0.10 - ETA: 1:58 - loss: 1.2058 - accuracy: 0.10 - ETA: 1:57 - loss: 1.2071 - accuracy: 0.10 - ETA: 1:56 - loss: 1.2064 - accuracy: 0.10 - ETA: 1:55 - loss: 1.2072 - accuracy: 0.10 - ETA: 1:54 - loss: 1.2066 - accuracy: 0.10 - ETA: 1:53 - loss: 1.2068 - accuracy: 0.10 - ETA: 1:52 - loss: 1.2065 - accuracy: 0.10 - ETA: 1:52 - loss: 1.2065 - accuracy: 0.10 - ETA: 1:51 - loss: 1.2060 - accuracy: 0.10 - ETA: 1:50 - loss: 1.2071 - accuracy: 0.10 - ETA: 1:49 - loss: 1.2076 - accuracy: 0.10 - ETA: 1:48 - loss: 1.2073 - accuracy: 0.10 - ETA: 1:47 - loss: 1.2075 - accuracy: 0.10 - ETA: 1:46 - loss: 1.2074 - accuracy: 0.10 - ETA: 1:45 - loss: 1.2077 - accuracy: 0.10 - ETA: 1:44 - loss: 1.2083 - accuracy: 0.10 - ETA: 1:44 - loss: 1.2089 - accuracy: 0.10 - ETA: 1:43 - loss: 1.2091 - accuracy: 0.10 - ETA: 1:42 - loss: 1.2093 - accuracy: 0.10 - ETA: 1:41 - loss: 1.2095 - accuracy: 0.10 - ETA: 1:40 - loss: 1.2094 - accuracy: 0.10 - ETA: 1:39 - loss: 1.2104 - accuracy: 0.10 - ETA: 1:38 - loss: 1.2104 - accuracy: 0.10 - ETA: 1:37 - loss: 1.2102 - accuracy: 0.10 - ETA: 1:36 - loss: 1.2109 - accuracy: 0.10 - ETA: 1:36 - loss: 1.2109 - accuracy: 0.10 - ETA: 1:35 - loss: 1.2117 - accuracy: 0.10 - ETA: 1:34 - loss: 1.2120 - accuracy: 0.10 - ETA: 1:33 - loss: 1.2121 - accuracy: 0.10 - ETA: 1:32 - loss: 1.2122 - accuracy: 0.10 - ETA: 1:31 - loss: 1.2127 - accuracy: 0.10 - ETA: 1:30 - loss: 1.2137 - accuracy: 0.10 - ETA: 1:29 - loss: 1.2132 - accuracy: 0.10 - ETA: 1:28 - loss: 1.2134 - accuracy: 0.10 - ETA: 1:27 - loss: 1.2134 - accuracy: 0.10 - ETA: 1:26 - loss: 1.2131 - accuracy: 0.10 - ETA: 1:26 - loss: 1.2136 - accuracy: 0.10 - ETA: 1:25 - loss: 1.2136 - accuracy: 0.10 - ETA: 1:24 - loss: 1.2128 - accuracy: 0.10 - ETA: 1:23 - loss: 1.2134 - accuracy: 0.1031"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and predict\n",
    "\n",
    "The following steps are used for evaluation:\n",
    "\n",
    "* Apply the same preprocessing method we used to create our dataset for the input sentence.\n",
    "* Tokenize the input sentence and add `START_TOKEN` and `END_TOKEN`. \n",
    "* Calculate the padding masks and the look ahead masks.\n",
    "* The decoder then outputs the predictions by looking at the encoder output and its own output.\n",
    "* Select the last word and calculate the argmax of that.\n",
    "* Concatentate the predicted word to the decoder input as pass it to the decoder.\n",
    "* In this approach, the decoder predicts the next word based on the previous words it predicted.\n",
    "\n",
    "Note: The model used here has less capacity and trained on a subset of the full dataset, hence its performance can be further improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  for i in range(MAX_LENGTH):\n",
    "    predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # concatenated the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0)\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "  prediction = evaluate(sentence)\n",
    "\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict('Where have you been?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict(\"It's a trap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the model with its previous output\n",
    "sentence = 'I am not crazy, my mother had me tested.'\n",
    "for _ in range(5):\n",
    "  sentence = predict(sentence)\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here we are, we have implemented a Transformer in TensorFlow 2.0 in around 500 lines of code.\n",
    "\n",
    "In this tutorial, we focus on the two different approaches to implement complex models with Functional API and Model subclassing, and how to incorporate them.\n",
    "\n",
    "Try using a different dataset or hyper-parameters to train the Transformer! Thanks for reading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
