{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM을 이용한 Chatbot 만들기\n",
    "\n",
    "## 참고\n",
    "cornell movie dialog와 데이터 정제\n",
    "\n",
    "https://github.com/PacktPublishing/TensorFlow-Deep-Learning-Projects/tree/master/Chapter07\n",
    "\n",
    "keras를 사용한 lstm chatbot\n",
    "\n",
    "https://medium.com/predict/creating-a-chatbot-from-scratch-using-keras-and-tensorflow-59e8fc76be79\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_utils.py와 corpoa_tools.py 소스코드 이용\n",
    "\n",
    "https://github.com/PacktPublishing/TensorFlow-Deep-Learning-Projects/tree/master/Chapter07\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습할 자료 다운로드\n",
    "\n",
    "* 자료 1\n",
    "\n",
    "conrell movie corpus\n",
    "\n",
    "* 자료 2\n",
    "\n",
    "chatterbot/english on Kaggle.com by kausr25. It contains pairs of questions and answers based on a number of subjects like food, history, AI etc.\n",
    "\n",
    "### chatbot_train.py 로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Corpora length (i.e. number of sentences)\n",
      "222382\n",
      "Directory  ./dictionary  already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import requests, zipfile, io\n",
    "from corpora_tools import *\n",
    "\n",
    "def cornell_download(url):\n",
    "    if not os.path.exists(\"cornell movie-dialogs corpus\"):\n",
    "        r = requests.get(url) \n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall()\n",
    "    return\n",
    "\n",
    "def kaggle_download(url):\n",
    "    if not os.path.exists(\"chatbot_nlp/data\"):\n",
    "        r = requests.get(url) \n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall()\n",
    "    return\n",
    "\n",
    "# movie_conversations 텍스트에서 발화(utterance) 리스트를 얻음, re.sub의 패턴에 raw string 명시하도록 r 추가\n",
    "def read_conversations():\n",
    "    filename = \"cornell movie-dialogs corpus/movie_conversations.txt\"\n",
    "    with open(filename, \"r\", encoding=\"ISO-8859-1\") as fh:\n",
    "        conversations_chunks = [line.split(\" +++$+++ \") for line in fh]\n",
    "    return [re.sub(r'[\\[\\]\\']', '', el[3].strip()).split(\", \") for el in conversations_chunks]\n",
    "\n",
    "# 라인 번호와 그 라인의 대화를 반환\n",
    "def read_lines():\n",
    "    filename = \"cornell movie-dialogs corpus/movie_lines.txt\"\n",
    "    with open(filename, \"r\", encoding=\"ISO-8859-1\") as fh:\n",
    "        line_chunks = [line.split(\" +++$+++ \") for line in fh]\n",
    "    return {line[0]: line[-1].strip() for line in line_chunks}\n",
    "\n",
    "# 토큰화 정렬과정, 두 개의 발화로 구성된 튜플을 포함한 생성기(generators)를 반환, 발화는 space 기준으로 토큰화\n",
    "def get_tokenized_sequencial_sentences(list_of_lines, line_text):\n",
    "    for line in list_of_lines:\n",
    "        for i in range(len(line) - 1):\n",
    "            yield (line_text[line[i]].split(\" \"), line_text[line[i+1]].split(\" \"))\n",
    "\n",
    "def kaggle_tokenized():\n",
    "    dir_path = 'chatbot_nlp/data'\n",
    "    files_list = os.listdir(dir_path + os.sep)\n",
    "\n",
    "    questions = list()\n",
    "    answers = list()\n",
    "\n",
    "    ## 답변이 2개 이상일 경우 쪼개기\n",
    "    for filepath in files_list:\n",
    "        stream = open( dir_path + os.sep + filepath , 'rb')\n",
    "        docs = yaml.safe_load(stream)\n",
    "        conversations = docs['conversations']\n",
    "        for con in conversations:\n",
    "            if len( con ) > 2 :\n",
    "                replies = con[ 1 : ]\n",
    "                for rep in replies:\n",
    "                    question_tokens = str(con[0]).split(\" \")\n",
    "                    questions.append(question_tokens)\n",
    "                    rep_tokens = str(rep).split(\" \")\n",
    "                    answers.append( rep_tokens )\n",
    "            elif len( con )> 1:\n",
    "                question_tokens = str(con[0]).split(\" \")\n",
    "                questions.append(question_tokens)\n",
    "                answer_tokens = str(con[1]).split(\" \")\n",
    "                answers.append( answer_tokens )\n",
    "\n",
    "    return questions, answers\n",
    "\n",
    "def retrieve_corpora_from_zip():\n",
    "    cornell_download(\"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\")\n",
    "    conversations = read_conversations()\n",
    "    lines = read_lines()\n",
    "    questions, answers = zip(*list(get_tokenized_sequencial_sentences(conversations, lines)))\n",
    "\n",
    "    # dataset 추가\n",
    "    kaggle_download('https://github.com/shubham0204/Dataset_Archives/blob/master/chatbot_nlp.zip?raw=true')\n",
    "    sen1, sen2 = kaggle_tokenized()\n",
    "    questions = questions + tuple(sen1)\n",
    "    answers = answers + tuple(sen2)\n",
    "\n",
    "    return questions, answers\n",
    "\n",
    "sen_l1, sen_l2 = retrieve_corpora_from_zip()\n",
    "\n",
    "print(\"# Corpora length (i.e. number of sentences)\")\n",
    "print(len(sen_l1))\n",
    "assert len(sen_l1) == len(sen_l2)\n",
    "\n",
    "\n",
    "clean_sen_l1 = [clean_sentence(s) for s in sen_l1]\n",
    "clean_sen_l2 = [clean_sentence(s) for s in sen_l2]\n",
    "\n",
    "dirName = \"./dictionary\"\n",
    "if not os.path.exists(dirName):\n",
    "    os.mkdir(dirName)\n",
    "    print(\"Directory \" , dirName ,  \" Created \")\n",
    "else:    \n",
    "    print(\"Directory \" , dirName ,  \" already exists\")\n",
    "\n",
    "dict_l1 = create_indexed_dictionary(clean_sen_l1, dict_size=50000, storage_path=\"dictionary/l1_dict.p\")\n",
    "dict_l2 = create_indexed_dictionary(clean_sen_l2, dict_size=50000, storage_path=\"dictionary/l2_dict.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
